diff --git a/pylsp/hookspecs.py b/pylsp/hookspecs.py
index 41508be..e7f3f1f 100644
--- a/pylsp/hookspecs.py
+++ b/pylsp/hookspecs.py
@@ -2,134 +2,3 @@
 # Copyright 2021- Python Language Server Contributors.
 
 from pylsp import hookspec
-
-
-@hookspec
-def pylsp_code_actions(config, workspace, document, range, context):
-    pass
-
-
-@hookspec
-def pylsp_code_lens(config, workspace, document) -> None:
-    pass
-
-
-@hookspec
-def pylsp_commands(config, workspace) -> None:
-    """The list of command strings supported by the server.
-
-    Returns:
-        List[str]: The supported commands.
-    """
-
-
-@hookspec
-def pylsp_completions(config, workspace, document, position, ignored_names) -> None:
-    pass
-
-
-@hookspec(firstresult=True)
-def pylsp_completion_item_resolve(config, workspace, document, completion_item) -> None:
-    pass
-
-
-@hookspec
-def pylsp_definitions(config, workspace, document, position) -> None:
-    pass
-
-
-@hookspec
-def pylsp_dispatchers(config, workspace) -> None:
-    pass
-
-
-@hookspec
-def pylsp_document_did_open(config, workspace, document) -> None:
-    pass
-
-
-@hookspec
-def pylsp_document_did_save(config, workspace, document) -> None:
-    pass
-
-
-@hookspec
-def pylsp_document_highlight(config, workspace, document, position) -> None:
-    pass
-
-
-@hookspec
-def pylsp_document_symbols(config, workspace, document) -> None:
-    pass
-
-
-@hookspec(firstresult=True)
-def pylsp_execute_command(config, workspace, command, arguments) -> None:
-    pass
-
-
-@hookspec
-def pylsp_experimental_capabilities(config, workspace) -> None:
-    pass
-
-
-@hookspec
-def pylsp_folding_range(config, workspace, document) -> None:
-    pass
-
-
-@hookspec(firstresult=True)
-def pylsp_format_document(config, workspace, document, options) -> None:
-    pass
-
-
-@hookspec(firstresult=True)
-def pylsp_format_range(config, workspace, document, range, options) -> None:
-    pass
-
-
-@hookspec(firstresult=True)
-def pylsp_hover(config, workspace, document, position) -> None:
-    pass
-
-
-@hookspec
-def pylsp_initialize(config, workspace) -> None:
-    pass
-
-
-@hookspec
-def pylsp_initialized() -> None:
-    pass
-
-
-@hookspec
-def pylsp_lint(config, workspace, document, is_saved) -> None:
-    pass
-
-
-@hookspec
-def pylsp_references(
-    config, workspace, document, position, exclude_declaration
-) -> None:
-    pass
-
-
-@hookspec(firstresult=True)
-def pylsp_rename(config, workspace, document, position, new_name) -> None:
-    pass
-
-
-@hookspec
-def pylsp_settings(config) -> None:
-    pass
-
-
-@hookspec(firstresult=True)
-def pylsp_signature_help(config, workspace, document, position) -> None:
-    pass
-
-
-@hookspec
-def pylsp_workspace_configuration_changed(config, workspace) -> None:
-    pass
diff --git a/pylsp/plugins/autopep8_format.py b/pylsp/plugins/autopep8_format.py
index 2b3491d..69aba5c 100644
--- a/pylsp/plugins/autopep8_format.py
+++ b/pylsp/plugins/autopep8_format.py
@@ -1,6 +1,8 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import log
+from pylsp.pylsp_shared import _format
 import logging
 
 import pycodestyle
@@ -9,87 +11,3 @@ from autopep8 import fix_code
 
 from pylsp import hookimpl
 from pylsp._utils import get_eol_chars
-
-log = logging.getLogger(__name__)
-
-
-@hookimpl(tryfirst=True)  # Prefer autopep8 over YAPF
-def pylsp_format_document(config, workspace, document, options):
-    with workspace.report_progress("format: autopep8"):
-        log.info("Formatting document %s with autopep8", document)
-        return _format(config, document)
-
-
-@hookimpl(tryfirst=True)  # Prefer autopep8 over YAPF
-def pylsp_format_range(config, workspace, document, range, options):
-    log.info("Formatting document %s in range %s with autopep8", document, range)
-
-    # First we 'round' the range up/down to full lines only
-    range["start"]["character"] = 0
-    range["end"]["line"] += 1
-    range["end"]["character"] = 0
-
-    # Add 1 for 1-indexing vs LSP's 0-indexing
-    line_range = (range["start"]["line"] + 1, range["end"]["line"])
-    return _format(config, document, line_range=line_range)
-
-
-def _format(config, document, line_range=None):
-    options = _autopep8_config(config, document)
-    if line_range:
-        options["line_range"] = list(line_range)
-
-    # Temporarily re-monkey-patch the continued_indentation checker - #771
-    del pycodestyle._checks["logical_line"][pycodestyle.continued_indentation]
-    pycodestyle.register_check(autopep8_c_i)
-
-    # Autopep8 doesn't work with CR line endings, so we replace them by '\n'
-    # and restore them below.
-    replace_cr = False
-    source = document.source
-    eol_chars = get_eol_chars(source)
-    if eol_chars == "\r":
-        replace_cr = True
-        source = source.replace("\r", "\n")
-
-    new_source = fix_code(source, options=options)
-
-    # Switch it back
-    del pycodestyle._checks["logical_line"][autopep8_c_i]
-    pycodestyle.register_check(pycodestyle.continued_indentation)
-
-    if new_source == source:
-        return []
-
-    if replace_cr:
-        new_source = new_source.replace("\n", "\r")
-
-    # I'm too lazy at the moment to parse diffs into TextEdit items
-    # So let's just return the entire file...
-    return [
-        {
-            "range": {
-                "start": {"line": 0, "character": 0},
-                # End char 0 of the line after our document
-                "end": {"line": len(document.lines), "character": 0},
-            },
-            "newText": new_source,
-        }
-    ]
-
-
-def _autopep8_config(config, document=None):
-    # We user pycodestyle settings to avoid redefining things
-    path = document.path if document is not None else None
-    settings = config.plugin_settings("pycodestyle", document_path=path)
-    options = {
-        "exclude": settings.get("exclude"),
-        "hang_closing": settings.get("hangClosing"),
-        "ignore": settings.get("ignore"),
-        "max_line_length": settings.get("maxLineLength"),
-        "select": settings.get("select"),
-        "aggressive": settings.get("aggressive"),
-    }
-
-    # Filter out null options
-    return {k: v for k, v in options.items() if v}
diff --git a/pylsp/plugins/definition.py b/pylsp/plugins/definition.py
index 67abfb7..266dbe0 100644
--- a/pylsp/plugins/definition.py
+++ b/pylsp/plugins/definition.py
@@ -2,6 +2,8 @@
 # Copyright 2021- Python Language Server Contributors.
 from __future__ import annotations
 
+from pylsp.pylsp_shared import _resolve_definition
+from pylsp.pylsp_shared import _not_internal_definition
 import logging
 from typing import TYPE_CHECKING, Any, Dict, List
 
@@ -17,68 +19,3 @@ if TYPE_CHECKING:
     from pylsp.workspace import Document
 
 log = logging.getLogger(__name__)
-
-
-MAX_JEDI_GOTO_HOPS = 100
-
-
-def _resolve_definition(
-    maybe_defn: Name, script: Script, settings: Dict[str, Any]
-) -> Name:
-    for _ in range(MAX_JEDI_GOTO_HOPS):
-        if maybe_defn.is_definition() or maybe_defn.module_path != script.path:
-            break
-        defns = script.goto(
-            follow_imports=settings.get("follow_imports", True),
-            follow_builtin_imports=settings.get("follow_builtin_imports", True),
-            line=maybe_defn.line,
-            column=maybe_defn.column,
-        )
-        if len(defns) == 1:
-            maybe_defn = defns[0]
-        else:
-            break
-    return maybe_defn
-
-
-@hookimpl
-def pylsp_definitions(
-    config: Config, document: Document, position: Dict[str, int]
-) -> List[Dict[str, Any]]:
-    settings = config.plugin_settings("jedi_definition")
-    code_position = _utils.position_to_jedi_linecolumn(document, position)
-    script = document.jedi_script(use_document_path=True)
-    auto_import_modules = jedi.settings.auto_import_modules
-
-    try:
-        jedi.settings.auto_import_modules = []
-        definitions = script.goto(
-            follow_imports=settings.get("follow_imports", True),
-            follow_builtin_imports=settings.get("follow_builtin_imports", True),
-            **code_position,
-        )
-        definitions = [_resolve_definition(d, script, settings) for d in definitions]
-    finally:
-        jedi.settings.auto_import_modules = auto_import_modules
-
-    follow_builtin_defns = settings.get("follow_builtin_definitions", True)
-    return [
-        {
-            "uri": uris.uri_with(document.uri, path=str(d.module_path)),
-            "range": {
-                "start": {"line": d.line - 1, "character": d.column},
-                "end": {"line": d.line - 1, "character": d.column + len(d.name)},
-            },
-        }
-        for d in definitions
-        if d.is_definition() and (follow_builtin_defns or _not_internal_definition(d))
-    ]
-
-
-def _not_internal_definition(definition: Name) -> bool:
-    return (
-        definition.line is not None
-        and definition.column is not None
-        and definition.module_path is not None
-        and not definition.in_builtin_module()
-    )
diff --git a/pylsp/plugins/flake8_lint.py b/pylsp/plugins/flake8_lint.py
index 74e2664..c118d10 100644
--- a/pylsp/plugins/flake8_lint.py
+++ b/pylsp/plugins/flake8_lint.py
@@ -3,6 +3,11 @@
 
 """Linter pluging for flake8"""
 
+from pylsp.pylsp_shared import log
+from pylsp.pylsp_shared import run_flake8
+from pylsp.pylsp_shared import build_args
+from pylsp.pylsp_shared import parse_stdout
+from pylsp.pylsp_shared import PYFLAKES_ERROR_MESSAGES
 import logging
 import os.path
 import re
@@ -13,231 +18,3 @@ from subprocess import PIPE, Popen
 from flake8.plugins.pyflakes import FLAKE8_PYFLAKES_CODES
 
 from pylsp import hookimpl, lsp
-from pylsp.plugins.pyflakes_lint import PYFLAKES_ERROR_MESSAGES
-
-log = logging.getLogger(__name__)
-
-FIX_IGNORES_RE = re.compile(r"([^a-zA-Z0-9_,]*;.*(\W+||$))")
-UNNECESSITY_CODES = {
-    "F401",  # `module` imported but unused
-    "F504",  # % format unused named arguments
-    "F522",  # .format(...) unused named arguments
-    "F523",  # .format(...) unused positional arguments
-    "F841",  # local variable `name` is assigned to but never used
-}
-# NOTE: If the user sets the flake8 executable with workspace configuration, the
-# error codes in this set may be inaccurate.
-ERROR_CODES = (
-    # Errors from the pyflakes plugin of flake8
-    {FLAKE8_PYFLAKES_CODES.get(m.__name__, "E999") for m in PYFLAKES_ERROR_MESSAGES}
-    # Syntax error from flake8 itself
-    | {"E999"}
-)
-
-
-@hookimpl
-def pylsp_settings():
-    # Default flake8 to disabled
-    return {"plugins": {"flake8": {"enabled": False}}}
-
-
-@hookimpl
-def pylsp_lint(workspace, document):
-    with workspace.report_progress("lint: flake8"):
-        config = workspace._config
-        settings = config.plugin_settings("flake8", document_path=document.path)
-        log.debug("Got flake8 settings: %s", settings)
-
-        ignores = settings.get("ignore", [])
-        per_file_ignores = settings.get("perFileIgnores")
-
-        if per_file_ignores:
-            prev_file_pat = None
-            for path in per_file_ignores:
-                try:
-                    file_pat, errors = path.split(":")
-                    prev_file_pat = file_pat
-                except ValueError:
-                    # It's legal to just specify another error type for the same
-                    # file pattern:
-                    if prev_file_pat is None:
-                        log.warning("skipping a Per-file-ignore with no file pattern")
-                        continue
-                    file_pat = prev_file_pat
-                    errors = path
-                if PurePath(document.path).match(file_pat):
-                    ignores.extend(errors.split(","))
-
-        opts = {
-            "config": settings.get("config"),
-            "exclude": settings.get("exclude"),
-            "extend-ignore": settings.get("extendIgnore"),
-            "extend-select": settings.get("extendSelect"),
-            "filename": settings.get("filename"),
-            "hang-closing": settings.get("hangClosing"),
-            "ignore": ignores or None,
-            "max-complexity": settings.get("maxComplexity"),
-            "max-line-length": settings.get("maxLineLength"),
-            "indent-size": settings.get("indentSize"),
-            "select": settings.get("select"),
-        }
-
-        # flake takes only absolute path to the config. So we should check and
-        # convert if necessary
-        if opts.get("config") and not os.path.isabs(opts.get("config")):
-            opts["config"] = os.path.abspath(
-                os.path.expanduser(os.path.expandvars(opts.get("config")))
-            )
-            log.debug("using flake8 with config: %s", opts["config"])
-
-        # Call the flake8 utility then parse diagnostics from stdout
-        flake8_executable = settings.get("executable", "flake8")
-
-        args = build_args(opts)
-
-        # ensure the same source is used for flake8 execution and result parsing;
-        # single source access improves performance as it is only one disk access
-        source = document.source
-        output = run_flake8(flake8_executable, args, document, source)
-        return parse_stdout(source, output)
-
-
-def run_flake8(flake8_executable, args, document, source):
-    """Run flake8 with the provided arguments, logs errors
-    from stderr if any.
-    """
-    # a quick temporary fix to deal with Atom
-    args = [
-        (i if not i.startswith("--ignore=") else FIX_IGNORES_RE.sub("", i))
-        for i in args
-        if i is not None
-    ]
-
-    if document.path and document.path.startswith(document._workspace.root_path):
-        args.extend(
-            [
-                "--stdin-display-name",
-                os.path.relpath(document.path, document._workspace.root_path),
-            ]
-        )
-
-    # if executable looks like a path resolve it
-    if not os.path.isfile(flake8_executable) and os.sep in flake8_executable:
-        flake8_executable = os.path.abspath(
-            os.path.expanduser(os.path.expandvars(flake8_executable))
-        )
-
-    log.debug("Calling %s with args: '%s'", flake8_executable, args)
-    popen_kwargs = {}
-    if cwd := document._workspace.root_path:
-        popen_kwargs["cwd"] = cwd
-    try:
-        cmd = [flake8_executable]
-        cmd.extend(args)
-        p = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE, **popen_kwargs)
-    except IOError:
-        log.debug(
-            "Can't execute %s. Trying with '%s -m flake8'",
-            flake8_executable,
-            sys.executable,
-        )
-        cmd = [sys.executable, "-m", "flake8"]
-        cmd.extend(args)
-        p = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE, **popen_kwargs)
-    (stdout, stderr) = p.communicate(source.encode())
-    if stderr:
-        log.error("Error while running flake8 '%s'", stderr.decode())
-    return stdout.decode()
-
-
-def build_args(options):
-    """Build arguments for calling flake8.
-
-    Args:
-        options: dictionary of argument names and their values.
-    """
-    args = ["-"]  # use stdin
-    for arg_name, arg_val in options.items():
-        if arg_val is None:
-            continue
-        arg = None
-        if isinstance(arg_val, list):
-            arg = "--{}={}".format(arg_name, ",".join(arg_val))
-        elif isinstance(arg_val, bool):
-            if arg_val:
-                arg = "--{}".format(arg_name)
-        else:
-            arg = "--{}={}".format(arg_name, arg_val)
-        args.append(arg)
-    return args
-
-
-def parse_stdout(source, stdout):
-    """
-    Build a diagnostics from flake8's output, it should extract every result and format
-    it into a dict that looks like this:
-        {
-            'source': 'flake8',
-            'code': code, # 'E501'
-            'range': {
-                'start': {
-                    'line': start_line,
-                    'character': start_column,
-                },
-                'end': {
-                    'line': end_line,
-                    'character': end_column,
-                },
-            },
-            'message': msg,
-            'severity': lsp.DiagnosticSeverity.*,
-        }
-
-    Args:
-        document: The document to be linted.
-        stdout: output from flake8
-    Returns:
-        A list of dictionaries.
-    """
-
-    document_lines = source.splitlines(True)
-    diagnostics = []
-    lines = stdout.splitlines()
-    for raw_line in lines:
-        parsed_line = re.match(r"(.*):(\d*):(\d*): (\w*) (.*)", raw_line)
-        if not parsed_line:
-            log.debug("Flake8 output parser can't parse line '%s'", raw_line)
-            continue
-
-        parsed_line = parsed_line.groups()
-        if len(parsed_line) != 5:
-            log.debug("Flake8 output parser can't parse line '%s'", raw_line)
-            continue
-
-        _, line, character, code, msg = parsed_line
-        line = int(line) - 1
-        character = int(character) - 1
-        # show also the code in message
-        msg = code + " " + msg
-        severity = lsp.DiagnosticSeverity.Warning
-        if code in ERROR_CODES:
-            severity = lsp.DiagnosticSeverity.Error
-        diagnostic = {
-            "source": "flake8",
-            "code": code,
-            "range": {
-                "start": {"line": line, "character": character},
-                "end": {
-                    "line": line,
-                    # no way to determine the column
-                    "character": len(document_lines[line]),
-                },
-            },
-            "message": msg,
-            "severity": severity,
-        }
-        if code in UNNECESSITY_CODES:
-            diagnostic["tags"] = [lsp.DiagnosticTag.Unnecessary]
-        diagnostics.append(diagnostic)
-
-    return diagnostics
diff --git a/pylsp/plugins/folding.py b/pylsp/plugins/folding.py
index 123ba4a..71ddcef 100644
--- a/pylsp/plugins/folding.py
+++ b/pylsp/plugins/folding.py
@@ -1,210 +1,10 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import __compute_folding_ranges
 import re
 
 import parso
 import parso.python.tree as tree_nodes
 
 from pylsp import hookimpl
-
-SKIP_NODES = (tree_nodes.Module, tree_nodes.IfStmt, tree_nodes.TryStmt)
-IDENTATION_REGEX = re.compile(r"(\s+).+")
-
-
-@hookimpl
-def pylsp_folding_range(document):
-    program = document.source + "\n"
-    lines = program.splitlines()
-    tree = parso.parse(program)
-    ranges = __compute_folding_ranges(tree, lines)
-
-    results = []
-    for start_line, end_line in ranges:
-        start_line -= 1
-        end_line -= 1
-        # If start/end character is not defined, then it defaults to the
-        # corresponding line last character
-        results.append(
-            {
-                "startLine": start_line,
-                "endLine": end_line,
-            }
-        )
-    return results
-
-
-def __merge_folding_ranges(left, right):
-    for start in list(left.keys()):
-        right_start = right.pop(start, None)
-        if right_start is not None:
-            left[start] = max(right_start, start)
-    left.update(right)
-    return left
-
-
-def __empty_identation_stack(
-    identation_stack, level_limits, current_line, folding_ranges
-):
-    while identation_stack != []:
-        upper_level = identation_stack.pop(0)
-        level_start = level_limits.pop(upper_level)
-        folding_ranges.append((level_start, current_line))
-    return folding_ranges
-
-
-def __match_identation_stack(
-    identation_stack, level, level_limits, folding_ranges, current_line
-):
-    upper_level = identation_stack.pop(0)
-    while upper_level >= level:
-        level_start = level_limits.pop(upper_level)
-        folding_ranges.append((level_start, current_line))
-        upper_level = identation_stack.pop(0)
-    identation_stack.insert(0, upper_level)
-    return identation_stack, folding_ranges
-
-
-def __compute_folding_ranges_identation(text):
-    lines = text.splitlines()
-    folding_ranges = []
-    identation_stack = []
-    level_limits = {}
-    current_level = 0
-    current_line = 0
-    while lines[current_line] == "":
-        current_line += 1
-    for i, line in enumerate(lines):
-        if i < current_line:
-            continue
-        i += 1
-        identation_match = IDENTATION_REGEX.match(line)
-        if identation_match is not None:
-            whitespace = identation_match.group(1)
-            level = len(whitespace)
-            if level > current_level:
-                level_limits[current_level] = current_line
-                identation_stack.insert(0, current_level)
-                current_level = level
-            elif level < current_level:
-                identation_stack, folding_ranges = __match_identation_stack(
-                    identation_stack, level, level_limits, folding_ranges, current_line
-                )
-                current_level = level
-        else:
-            folding_ranges = __empty_identation_stack(
-                identation_stack, level_limits, current_line, folding_ranges
-            )
-            current_level = 0
-        if line.strip() != "":
-            current_line = i
-    folding_ranges = __empty_identation_stack(
-        identation_stack, level_limits, current_line, folding_ranges
-    )
-    return dict(folding_ranges)
-
-
-def __check_if_node_is_valid(node):
-    valid = True
-    if isinstance(node, tree_nodes.PythonNode):
-        kind = node.type
-        valid = kind not in {
-            "decorated",
-            "parameters",
-            "dictorsetmaker",
-            "testlist_comp",
-        }
-        if kind == "suite":
-            if isinstance(node.parent, tree_nodes.Function):
-                valid = False
-    return valid
-
-
-def __handle_skip(stack, skip):
-    body = stack[skip]
-    children = [body]
-    if hasattr(body, "children"):
-        children = body.children
-    stack = stack[:skip] + children + stack[skip + 1 :]
-    node = body
-    end_line, _ = body.end_pos
-    return node, end_line
-
-
-def __handle_flow_nodes(node, end_line, stack):
-    from_keyword = False
-    if isinstance(node, tree_nodes.Keyword):
-        from_keyword = True
-        if node.value in {"if", "elif", "with", "while"}:
-            node, end_line = __handle_skip(stack, 2)
-        elif node.value in {"except"}:
-            first_node = stack[0]
-            if isinstance(first_node, tree_nodes.Operator):
-                node, end_line = __handle_skip(stack, 1)
-            else:
-                node, end_line = __handle_skip(stack, 2)
-        elif node.value in {"for"}:
-            node, end_line = __handle_skip(stack, 4)
-        elif node.value in {"else"}:
-            node, end_line = __handle_skip(stack, 1)
-    return end_line, from_keyword, node, stack
-
-
-def __compute_start_end_lines(node, stack):
-    start_line, _ = node.start_pos
-    end_line, _ = node.end_pos
-    modified = False
-    end_line, from_keyword, node, stack = __handle_flow_nodes(node, end_line, stack)
-
-    last_leaf = node.get_last_leaf()
-    last_newline = isinstance(last_leaf, tree_nodes.Newline)
-    last_operator = isinstance(last_leaf, tree_nodes.Operator)
-    node_is_operator = isinstance(node, tree_nodes.Operator)
-    last_operator = last_operator or not node_is_operator
-
-    end_line -= 1
-
-    if isinstance(node.parent, tree_nodes.PythonNode) and not from_keyword:
-        kind = node.type
-        if kind in {"suite", "atom", "atom_expr", "arglist"}:
-            if len(stack) > 0:
-                next_node = stack[0]
-                next_line, _ = next_node.start_pos
-                if next_line > end_line:
-                    end_line += 1
-                    modified = True
-    if not last_newline and not modified and not last_operator:
-        end_line += 1
-    return start_line, end_line, stack
-
-
-def __compute_folding_ranges(tree, lines):
-    folding_ranges = {}
-    stack = [tree]
-
-    while len(stack) > 0:
-        node = stack.pop(0)
-        if isinstance(node, tree_nodes.Newline):
-            # Skip newline nodes
-            continue
-        if isinstance(node, tree_nodes.PythonErrorNode):
-            # Fallback to indentation-based (best-effort) folding
-            start_line, _ = node.start_pos
-            start_line -= 1
-            padding = [""] * start_line
-            text = "\n".join(padding + lines[start_line:]) + "\n"
-            identation_ranges = __compute_folding_ranges_identation(text)
-            folding_ranges = __merge_folding_ranges(folding_ranges, identation_ranges)
-            break
-        if not isinstance(node, SKIP_NODES):
-            valid = __check_if_node_is_valid(node)
-            if valid:
-                start_line, end_line, stack = __compute_start_end_lines(node, stack)
-                if end_line > start_line:
-                    current_end = folding_ranges.get(start_line, -1)
-                    folding_ranges[start_line] = max(current_end, end_line)
-        if hasattr(node, "children"):
-            stack = node.children + stack
-
-    folding_ranges = sorted(folding_ranges.items())
-    return folding_ranges
diff --git a/pylsp/plugins/highlight.py b/pylsp/plugins/highlight.py
index c4c1240..4bdbe18 100644
--- a/pylsp/plugins/highlight.py
+++ b/pylsp/plugins/highlight.py
@@ -6,31 +6,3 @@ import logging
 from pylsp import _utils, hookimpl, lsp
 
 log = logging.getLogger(__name__)
-
-
-@hookimpl
-def pylsp_document_highlight(document, position):
-    code_position = _utils.position_to_jedi_linecolumn(document, position)
-    usages = document.jedi_script().get_references(**code_position)
-
-    def is_valid(definition):
-        return definition.line is not None and definition.column is not None
-
-    def local_to_document(definition):
-        return (
-            not definition.module_path or str(definition.module_path) == document.path
-        )
-
-    return [
-        {
-            "range": {
-                "start": {"line": d.line - 1, "character": d.column},
-                "end": {"line": d.line - 1, "character": d.column + len(d.name)},
-            },
-            "kind": lsp.DocumentHighlightKind.Write
-            if d.is_definition()
-            else lsp.DocumentHighlightKind.Read,
-        }
-        for d in usages
-        if is_valid(d) and local_to_document(d)
-    ]
diff --git a/pylsp/plugins/hover.py b/pylsp/plugins/hover.py
index ca69d1b..fe8d9c5 100644
--- a/pylsp/plugins/hover.py
+++ b/pylsp/plugins/hover.py
@@ -6,45 +6,3 @@ import logging
 from pylsp import _utils, hookimpl
 
 log = logging.getLogger(__name__)
-
-
-@hookimpl
-def pylsp_hover(config, document, position):
-    code_position = _utils.position_to_jedi_linecolumn(document, position)
-    definitions = document.jedi_script(use_document_path=True).infer(**code_position)
-    word = document.word_at_position(position)
-
-    # Find first exact matching definition
-    definition = next((x for x in definitions if x.name == word), None)
-
-    # Ensure a definition is used if only one is available
-    # even if the word doesn't match. An example of this case is 'np'
-    # where 'numpy' doesn't match with 'np'. Same for NumPy ufuncs
-    if len(definitions) == 1:
-        definition = definitions[0]
-
-    if not definition:
-        return {"contents": ""}
-
-    hover_capabilities = config.capabilities.get("textDocument", {}).get("hover", {})
-    supported_markup_kinds = hover_capabilities.get("contentFormat", ["markdown"])
-    preferred_markup_kind = _utils.choose_markup_kind(supported_markup_kinds)
-
-    # Find first exact matching signature
-    signature = next(
-        (
-            x.to_string()
-            for x in definition.get_signatures()
-            if (x.name == word and x.type not in ["module"])
-        ),
-        "",
-    )
-
-    return {
-        "contents": _utils.format_docstring(
-            # raw docstring returns only doc, without signature
-            definition.docstring(raw=True),
-            preferred_markup_kind,
-            signatures=[signature] if signature else None,
-        )
-    }
diff --git a/pylsp/plugins/jedi_completion.py b/pylsp/plugins/jedi_completion.py
index 2796a09..e2fc847 100644
--- a/pylsp/plugins/jedi_completion.py
+++ b/pylsp/plugins/jedi_completion.py
@@ -1,6 +1,9 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import use_snippets
+from pylsp.pylsp_shared import _resolve_completion
+from pylsp.pylsp_shared import _format_completion
 import logging
 import os
 
@@ -10,290 +13,3 @@ from pylsp import _utils, hookimpl, lsp
 from pylsp.plugins._resolvers import LABEL_RESOLVER, SNIPPET_RESOLVER
 
 log = logging.getLogger(__name__)
-
-# Map to the LSP type
-# > Valid values for type are ``module``, `` class ``, ``instance``, ``function``,
-# > ``param``, ``path``, ``keyword``, ``property`` and ``statement``.
-# see: https://jedi.readthedocs.io/en/latest/docs/api-classes.html#jedi.api.classes.BaseName.type
-_TYPE_MAP = {
-    "module": lsp.CompletionItemKind.Module,
-    "namespace": lsp.CompletionItemKind.Module,  # to be added in Jedi 0.18+
-    "class": lsp.CompletionItemKind.Class,
-    "instance": lsp.CompletionItemKind.Reference,
-    "function": lsp.CompletionItemKind.Function,
-    "param": lsp.CompletionItemKind.Variable,
-    "path": lsp.CompletionItemKind.File,
-    "keyword": lsp.CompletionItemKind.Keyword,
-    "property": lsp.CompletionItemKind.Property,  # added in Jedi 0.18
-    "statement": lsp.CompletionItemKind.Variable,
-}
-
-# Types of parso nodes for which snippet is not included in the completion
-_IMPORTS = ("import_name", "import_from")
-
-# Types of parso node for errors
-_ERRORS = ("error_node",)
-
-
-@hookimpl
-def pylsp_completions(config, document, position):
-    """Get formatted completions for current code position"""
-    settings = config.plugin_settings("jedi_completion", document_path=document.path)
-    resolve_eagerly = settings.get("eager", False)
-    code_position = _utils.position_to_jedi_linecolumn(document, position)
-
-    code_position["fuzzy"] = settings.get("fuzzy", False)
-    completions = document.jedi_script(use_document_path=True).complete(**code_position)
-
-    if not completions:
-        return None
-
-    completion_capabilities = config.capabilities.get("textDocument", {}).get(
-        "completion", {}
-    )
-    item_capabilities = completion_capabilities.get("completionItem", {})
-    snippet_support = item_capabilities.get("snippetSupport")
-    supported_markup_kinds = item_capabilities.get("documentationFormat", ["markdown"])
-    preferred_markup_kind = _utils.choose_markup_kind(supported_markup_kinds)
-
-    should_include_params = settings.get("include_params")
-    should_include_class_objects = settings.get("include_class_objects", False)
-    should_include_function_objects = settings.get("include_function_objects", False)
-
-    max_to_resolve = settings.get("resolve_at_most", 25)
-    modules_to_cache_for = settings.get("cache_for", None)
-    if modules_to_cache_for is not None:
-        LABEL_RESOLVER.cached_modules = modules_to_cache_for
-        SNIPPET_RESOLVER.cached_modules = modules_to_cache_for
-
-    include_params = (
-        snippet_support and should_include_params and use_snippets(document, position)
-    )
-    include_class_objects = (
-        snippet_support
-        and should_include_class_objects
-        and use_snippets(document, position)
-    )
-    include_function_objects = (
-        snippet_support
-        and should_include_function_objects
-        and use_snippets(document, position)
-    )
-
-    ready_completions = [
-        _format_completion(
-            c,
-            markup_kind=preferred_markup_kind,
-            include_params=include_params if c.type in ["class", "function"] else False,
-            resolve=resolve_eagerly,
-            resolve_label_or_snippet=(i < max_to_resolve),
-            snippet_support=snippet_support,
-        )
-        for i, c in enumerate(completions)
-    ]
-
-    # TODO split up once other improvements are merged
-    if include_class_objects:
-        for i, c in enumerate(completions):
-            if c.type == "class":
-                completion_dict = _format_completion(
-                    c,
-                    markup_kind=preferred_markup_kind,
-                    include_params=False,
-                    resolve=resolve_eagerly,
-                    resolve_label_or_snippet=(i < max_to_resolve),
-                    snippet_support=snippet_support,
-                )
-                completion_dict["kind"] = lsp.CompletionItemKind.TypeParameter
-                completion_dict["label"] += " object"
-                ready_completions.append(completion_dict)
-
-    if include_function_objects:
-        for i, c in enumerate(completions):
-            if c.type == "function":
-                completion_dict = _format_completion(
-                    c,
-                    markup_kind=preferred_markup_kind,
-                    include_params=False,
-                    resolve=resolve_eagerly,
-                    resolve_label_or_snippet=(i < max_to_resolve),
-                    snippet_support=snippet_support,
-                )
-                completion_dict["kind"] = lsp.CompletionItemKind.TypeParameter
-                completion_dict["label"] += " object"
-                ready_completions.append(completion_dict)
-
-    for completion_dict in ready_completions:
-        completion_dict["data"] = {"doc_uri": document.uri}
-
-    # most recently retrieved completion items, used for resolution
-    document.shared_data["LAST_JEDI_COMPLETIONS"] = {
-        # label is the only required property; here it is assumed to be unique
-        completion["label"]: (completion, data)
-        for completion, data in zip(ready_completions, completions)
-    }
-
-    return ready_completions or None
-
-
-@hookimpl
-def pylsp_completion_item_resolve(config, completion_item, document):
-    """Resolve formatted completion for given non-resolved completion"""
-    shared_data = document.shared_data["LAST_JEDI_COMPLETIONS"].get(
-        completion_item["label"]
-    )
-
-    completion_capabilities = config.capabilities.get("textDocument", {}).get(
-        "completion", {}
-    )
-    item_capabilities = completion_capabilities.get("completionItem", {})
-    supported_markup_kinds = item_capabilities.get("documentationFormat", ["markdown"])
-    preferred_markup_kind = _utils.choose_markup_kind(supported_markup_kinds)
-
-    if shared_data:
-        completion, data = shared_data
-        return _resolve_completion(completion, data, markup_kind=preferred_markup_kind)
-    return completion_item
-
-
-def is_exception_class(name):
-    """
-    Determine if a class name is an instance of an Exception.
-
-    This returns `False` if the name given corresponds with a instance of
-    the 'Exception' class, `True` otherwise
-    """
-    try:
-        return name in [cls.__name__ for cls in Exception.__subclasses__()]
-    except AttributeError:
-        # Needed in case a class don't uses new-style
-        # class definition in Python 2
-        return False
-
-
-def use_snippets(document, position):
-    """
-    Determine if it's necessary to return snippets in code completions.
-
-    This returns `False` if a completion is being requested on an import
-    statement, `True` otherwise.
-    """
-    line = position["line"]
-    lines = document.source.split("\n", line)
-    act_lines = [lines[line][: position["character"]]]
-    line -= 1
-    last_character = ""
-    while line > -1:
-        act_line = lines[line]
-        if (
-            act_line.rstrip().endswith("\\")
-            or act_line.rstrip().endswith("(")
-            or act_line.rstrip().endswith(",")
-        ):
-            act_lines.insert(0, act_line)
-            line -= 1
-            if act_line.rstrip().endswith("("):
-                # Needs to be added to the end of the code before parsing
-                # to make it valid, otherwise the node type could end
-                # being an 'error_node' for multi-line imports that use '('
-                last_character = ")"
-        else:
-            break
-    if "(" in act_lines[-1].strip():
-        last_character = ")"
-    code = "\n".join(act_lines).rsplit(";", maxsplit=1)[-1].strip() + last_character
-    tokens = parso.parse(code)
-    expr_type = tokens.children[0].type
-    return expr_type not in _IMPORTS and not (expr_type in _ERRORS and "import" in code)
-
-
-def _resolve_completion(completion, d, markup_kind: str):
-    completion["detail"] = _detail(d)
-    try:
-        docs = _utils.format_docstring(
-            d.docstring(raw=True),
-            signatures=[signature.to_string() for signature in d.get_signatures()],
-            markup_kind=markup_kind,
-        )
-    except Exception:
-        docs = ""
-    completion["documentation"] = docs
-    return completion
-
-
-def _format_completion(
-    d,
-    markup_kind: str,
-    include_params=True,
-    resolve=False,
-    resolve_label_or_snippet=False,
-    snippet_support=False,
-):
-    completion = {
-        "label": _label(d, resolve_label_or_snippet),
-        "kind": _TYPE_MAP.get(d.type),
-        "sortText": _sort_text(d),
-        "insertText": d.name,
-    }
-
-    if resolve:
-        completion = _resolve_completion(completion, d, markup_kind)
-
-    # Adjustments for file completions
-    if d.type == "path":
-        path = os.path.normpath(d.name)
-
-        # If the completion ends with os.sep, it means it's a directory. So we add os.sep at the end
-        # to ease additional file completions.
-        if d.name.endswith(os.sep):
-            if os.name == "nt":
-                path = path + "\\"
-            else:
-                path = path + "/"
-
-        # Escape to prevent conflicts with the code snippets grammer
-        # See also https://github.com/python-lsp/python-lsp-server/issues/373
-        if snippet_support:
-            path = path.replace("\\", "\\\\")
-            path = path.replace("/", "\\/")
-
-        completion["insertText"] = path
-
-    if include_params and not is_exception_class(d.name):
-        snippet = _snippet(d, resolve_label_or_snippet)
-        completion.update(snippet)
-
-    return completion
-
-
-def _label(definition, resolve=False):
-    if not resolve:
-        return definition.name
-    sig = LABEL_RESOLVER.get_or_create(definition)
-    if sig:
-        return sig
-    return definition.name
-
-
-def _snippet(definition, resolve=False):
-    if not resolve:
-        return {}
-    snippet = SNIPPET_RESOLVER.get_or_create(definition)
-    return snippet
-
-
-def _detail(definition):
-    try:
-        return definition.parent().full_name or ""
-    except AttributeError:
-        return definition.full_name or ""
-
-
-def _sort_text(definition):
-    """Ensure builtins appear at the bottom.
-    Description is of format <type>: <module>.<item>
-    """
-
-    # If its 'hidden', put it next last
-    prefix = "z{}" if definition.name.startswith("_") else "a{}"
-    return prefix.format(definition.name)
diff --git a/pylsp/plugins/jedi_rename.py b/pylsp/plugins/jedi_rename.py
index b35e321..7f34f19 100644
--- a/pylsp/plugins/jedi_rename.py
+++ b/pylsp/plugins/jedi_rename.py
@@ -1,56 +1,8 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import log
+from pylsp.pylsp_shared import _num_lines
 import logging
 
 from pylsp import _utils, hookimpl, uris
-
-log = logging.getLogger(__name__)
-
-
-@hookimpl
-def pylsp_rename(config, workspace, document, position, new_name):
-    log.debug(
-        "Executing rename of %s to %s", document.word_at_position(position), new_name
-    )
-    kwargs = _utils.position_to_jedi_linecolumn(document, position)
-    kwargs["new_name"] = new_name
-    try:
-        refactoring = document.jedi_script().rename(**kwargs)
-    except NotImplementedError as exc:
-        raise Exception(
-            "No support for renaming in Python 2/3.5 with Jedi. "
-            "Consider using the pylsp-rope plugin instead"
-        ) from exc
-    log.debug("Finished rename: %s", refactoring.get_diff())
-    changes = []
-
-    changed_files = refactoring.get_changed_files()
-    for file_path, changed_file in changed_files.items():
-        uri = uris.from_fs_path(str(file_path))
-        doc = workspace.get_maybe_document(uri)
-        changes.append(
-            {
-                "textDocument": {"uri": uri, "version": doc.version if doc else None},
-                "edits": [
-                    {
-                        "range": {
-                            "start": {"line": 0, "character": 0},
-                            "end": {
-                                "line": _num_lines(changed_file.get_new_code()),
-                                "character": 0,
-                            },
-                        },
-                        "newText": changed_file.get_new_code(),
-                    }
-                ],
-            }
-        )
-    return {"documentChanges": changes}
-
-
-def _num_lines(file_contents):
-    "Count the number of lines in the given string."
-    if _utils.get_eol_chars(file_contents):
-        return len(file_contents.splitlines())
-    return 0
diff --git a/pylsp/plugins/mccabe_lint.py b/pylsp/plugins/mccabe_lint.py
index 0e2cba2..d0df911 100644
--- a/pylsp/plugins/mccabe_lint.py
+++ b/pylsp/plugins/mccabe_lint.py
@@ -1,56 +1,12 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import log
+from pylsp.pylsp_shared import THRESHOLD
+from pylsp.pylsp_shared import DEFAULT_THRESHOLD
 import ast
 import logging
 
 import mccabe
 
 from pylsp import hookimpl, lsp
-
-log = logging.getLogger(__name__)
-
-THRESHOLD = "threshold"
-DEFAULT_THRESHOLD = 15
-
-
-@hookimpl
-def pylsp_lint(config, workspace, document):
-    with workspace.report_progress("lint: mccabe"):
-        threshold = config.plugin_settings("mccabe", document_path=document.path).get(
-            THRESHOLD, DEFAULT_THRESHOLD
-        )
-        log.debug("Running mccabe lint with threshold: %s", threshold)
-
-        try:
-            tree = compile(document.source, document.path, "exec", ast.PyCF_ONLY_AST)
-        except SyntaxError:
-            # We'll let the other linters point this one out
-            return None
-
-        visitor = mccabe.PathGraphingAstVisitor()
-        visitor.preorder(tree, visitor)
-
-        diags = []
-        for graph in visitor.graphs.values():
-            if graph.complexity() >= threshold:
-                diags.append(
-                    {
-                        "source": "mccabe",
-                        "range": {
-                            "start": {
-                                "line": graph.lineno - 1,
-                                "character": graph.column,
-                            },
-                            "end": {
-                                "line": graph.lineno - 1,
-                                "character": len(document.lines[graph.lineno]),
-                            },
-                        },
-                        "message": "Cyclomatic complexity too high: %s (threshold %s)"
-                        % (graph.complexity(), threshold),
-                        "severity": lsp.DiagnosticSeverity.Warning,
-                    }
-                )
-
-        return diags
diff --git a/pylsp/plugins/preload_imports.py b/pylsp/plugins/preload_imports.py
index ebcd9ad..6601e71 100644
--- a/pylsp/plugins/preload_imports.py
+++ b/pylsp/plugins/preload_imports.py
@@ -1,79 +1,8 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import MODULES
+from pylsp.pylsp_shared import log
 import logging
 
 from pylsp import hookimpl
-
-log = logging.getLogger(__name__)
-
-MODULES = [
-    "OpenGL",
-    "PIL",
-    "array",
-    "audioop",
-    "binascii",
-    "cPickle",
-    "cStringIO",
-    "cmath",
-    "collections",
-    "datetime",
-    "errno",
-    "exceptions",
-    "gc",
-    "imageop",
-    "imp",
-    "itertools",
-    "marshal",
-    "math",
-    "matplotlib",
-    "mmap",
-    "mpmath",
-    "msvcrt",
-    "networkx",
-    "nose",
-    "nt",
-    "numpy",
-    "operator",
-    "os",
-    "os.path",
-    "pandas",
-    "parser",
-    "rgbimg",
-    "scipy",
-    "signal",
-    "skimage",
-    "sklearn",
-    "statsmodels",
-    "strop",
-    "sympy",
-    "sys",
-    "thread",
-    "time",
-    "wx",
-    "xxsubtype",
-    "zipimport",
-    "zlib",
-]
-
-
-@hookimpl
-def pylsp_settings():
-    # Setup default modules to preload, and rope extension modules
-    return {
-        "plugins": {"preload": {"modules": MODULES}},
-        "rope": {"extensionModules": MODULES},
-    }
-
-
-@hookimpl
-def pylsp_initialize(config) -> None:
-    for mod_name in config.plugin_settings("preload").get("modules", []):
-        try:
-            __import__(mod_name)
-            log.debug("Preloaded module %s", mod_name)
-        except Exception:
-            # Catch any exception since not only ImportError can be raised here
-            # For example, old versions of NumPy can cause a ValueError.
-            # See spyder-ide/spyder#13985
-            pass
diff --git a/pylsp/plugins/pycodestyle_lint.py b/pylsp/plugins/pycodestyle_lint.py
index 7a514ad..9d06708 100644
--- a/pylsp/plugins/pycodestyle_lint.py
+++ b/pylsp/plugins/pycodestyle_lint.py
@@ -1,6 +1,9 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import log
+from pylsp.pylsp_shared import _get_severity
+from pylsp.pylsp_shared import PyCodeStyleDiagnosticReport
 import logging
 
 import pycodestyle
@@ -19,95 +22,3 @@ else:
     if autopep8_c_i in pycodestyle._checks["logical_line"]:
         del pycodestyle._checks["logical_line"][autopep8_c_i]
         pycodestyle.register_check(pycodestyle.continued_indentation)
-
-log = logging.getLogger(__name__)
-
-
-@hookimpl
-def pylsp_lint(workspace, document):
-    with workspace.report_progress("lint: pycodestyle"):
-        config = workspace._config
-        settings = config.plugin_settings("pycodestyle", document_path=document.path)
-        log.debug("Got pycodestyle settings: %s", settings)
-
-        opts = {
-            "exclude": settings.get("exclude"),
-            "filename": settings.get("filename"),
-            "hang_closing": settings.get("hangClosing"),
-            "ignore": settings.get("ignore"),
-            "max_line_length": settings.get("maxLineLength"),
-            "indent_size": settings.get("indentSize"),
-            "select": settings.get("select"),
-        }
-        kwargs = {k: v for k, v in opts.items() if v}
-        styleguide = pycodestyle.StyleGuide(kwargs)
-
-        # Use LF to lint file because other line endings can give false positives.
-        # See spyder-ide/spyder#19565 for context.
-        source = document.source
-        eol_chars = get_eol_chars(source)
-        if eol_chars in ["\r", "\r\n"]:
-            source = source.replace(eol_chars, "\n")
-            lines = source.splitlines(keepends=True)
-        else:
-            lines = document.lines
-
-        c = pycodestyle.Checker(
-            filename=document.path,
-            lines=lines,
-            options=styleguide.options,
-            report=PyCodeStyleDiagnosticReport(styleguide.options),
-        )
-        c.check_all()
-        diagnostics = c.report.diagnostics
-
-        return diagnostics
-
-
-class PyCodeStyleDiagnosticReport(pycodestyle.BaseReport):
-    def __init__(self, options) -> None:
-        self.diagnostics = []
-        super().__init__(options=options)
-
-    def error(self, line_number, offset, text, check):
-        code = text[:4]
-        if self._ignore_code(code):
-            return
-
-        # Don't care about expected errors or warnings
-        if code in self.expected:
-            return
-
-        # PyCodeStyle will sometimes give you an error the line after the end of the file
-        #   e.g. no newline at end of file
-        # In that case, the end offset should just be some number ~100
-        # (because why not? There's nothing to underline anyways)
-        err_range = {
-            "start": {"line": line_number - 1, "character": offset},
-            "end": {
-                # FIXME: It's a little naiive to mark until the end of the line, can we not easily do better?
-                "line": line_number - 1,
-                "character": 100
-                if line_number > len(self.lines)
-                else len(self.lines[line_number - 1]),
-            },
-        }
-        diagnostic = {
-            "source": "pycodestyle",
-            "range": err_range,
-            "message": text,
-            "code": code,
-            # Are style errors really ever errors?
-            "severity": _get_severity(code),
-        }
-        if code.startswith("W6"):
-            diagnostic["tags"] = [lsp.DiagnosticTag.Deprecated]
-        self.diagnostics.append(diagnostic)
-
-
-def _get_severity(code):
-    # Are style errors ever really errors?
-    if code[0] == "E" or code[0] == "W":
-        return lsp.DiagnosticSeverity.Warning
-    # If no severity is specified, why wouldn't this be informational only?
-    return lsp.DiagnosticSeverity.Information
diff --git a/pylsp/plugins/pydocstyle_lint.py b/pylsp/plugins/pydocstyle_lint.py
index a310ac8..e000d4a 100644
--- a/pylsp/plugins/pydocstyle_lint.py
+++ b/pylsp/plugins/pydocstyle_lint.py
@@ -1,6 +1,11 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import log
+from pylsp.pylsp_shared import DEFAULT_MATCH_RE
+from pylsp.pylsp_shared import DEFAULT_MATCH_DIR_RE
+from pylsp.pylsp_shared import _parse_diagnostic
+from pylsp.pylsp_shared import _patch_sys_argv
 import contextlib
 import logging
 import os
@@ -11,117 +16,6 @@ import pydocstyle
 
 from pylsp import hookimpl, lsp
 
-log = logging.getLogger(__name__)
-
 # PyDocstyle is a little verbose in debug message
 pydocstyle_logger = logging.getLogger(pydocstyle.utils.__name__)
 pydocstyle_logger.setLevel(logging.INFO)
-
-DEFAULT_MATCH_RE = pydocstyle.config.ConfigurationParser.DEFAULT_MATCH_RE
-DEFAULT_MATCH_DIR_RE = pydocstyle.config.ConfigurationParser.DEFAULT_MATCH_DIR_RE
-
-
-@hookimpl
-def pylsp_settings():
-    # Default pydocstyle to disabled
-    return {"plugins": {"pydocstyle": {"enabled": False}}}
-
-
-@hookimpl
-def pylsp_lint(config, workspace, document):
-    with workspace.report_progress("lint: pydocstyle"):
-        settings = config.plugin_settings("pydocstyle", document_path=document.path)
-        log.debug("Got pydocstyle settings: %s", settings)
-
-        # Explicitly passing a path to pydocstyle means it doesn't respect the --match flag, so do it ourselves
-        filename_match_re = re.compile(settings.get("match", DEFAULT_MATCH_RE) + "$")
-        if not filename_match_re.match(os.path.basename(document.path)):
-            return []
-
-        # Likewise with --match-dir
-        dir_match_re = re.compile(settings.get("matchDir", DEFAULT_MATCH_DIR_RE) + "$")
-        if not dir_match_re.match(os.path.basename(os.path.dirname(document.path))):
-            return []
-
-        args = [document.path]
-
-        if settings.get("convention"):
-            args.append("--convention=" + settings["convention"])
-
-            if settings.get("addSelect"):
-                args.append("--add-select=" + ",".join(settings["addSelect"]))
-            if settings.get("addIgnore"):
-                args.append("--add-ignore=" + ",".join(settings["addIgnore"]))
-
-        elif settings.get("select"):
-            args.append("--select=" + ",".join(settings["select"]))
-        elif settings.get("ignore"):
-            args.append("--ignore=" + ",".join(settings["ignore"]))
-
-        log.info("Using pydocstyle args: %s", args)
-
-        conf = pydocstyle.config.ConfigurationParser()
-        with _patch_sys_argv(args):
-            # TODO(gatesn): We can add more pydocstyle args here from our pylsp config
-            conf.parse()
-
-        # Will only yield a single filename, the document path
-        diags = []
-        for (
-            filename,
-            checked_codes,
-            ignore_decorators,
-            property_decorators,
-            ignore_self_only_init,
-        ) in conf.get_files_to_check():
-            errors = pydocstyle.checker.ConventionChecker().check_source(
-                document.source,
-                filename,
-                ignore_decorators=ignore_decorators,
-                property_decorators=property_decorators,
-                ignore_self_only_init=ignore_self_only_init,
-            )
-
-            try:
-                for error in errors:
-                    if error.code not in checked_codes:
-                        continue
-                    diags.append(_parse_diagnostic(document, error))
-            except pydocstyle.parser.ParseError:
-                # In the case we cannot parse the Python file, just continue
-                pass
-
-        log.debug("Got pydocstyle errors: %s", diags)
-        return diags
-
-
-def _parse_diagnostic(document, error):
-    lineno = error.definition.start - 1
-    line = document.lines[0] if document.lines else ""
-
-    start_character = len(line) - len(line.lstrip())
-    end_character = len(line)
-
-    return {
-        "source": "pydocstyle",
-        "code": error.code,
-        "message": error.message,
-        "severity": lsp.DiagnosticSeverity.Warning,
-        "range": {
-            "start": {"line": lineno, "character": start_character},
-            "end": {"line": lineno, "character": end_character},
-        },
-    }
-
-
-@contextlib.contextmanager
-def _patch_sys_argv(arguments) -> None:
-    old_args = sys.argv
-
-    # Preserve argv[0] since it's the executable
-    sys.argv = old_args[0:1] + arguments
-
-    try:
-        yield
-    finally:
-        sys.argv = old_args
diff --git a/pylsp/plugins/pyflakes_lint.py b/pylsp/plugins/pyflakes_lint.py
index 8a04276..f0332ba 100644
--- a/pylsp/plugins/pyflakes_lint.py
+++ b/pylsp/plugins/pyflakes_lint.py
@@ -1,97 +1,9 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import PYFLAKES_ERROR_MESSAGES
+from pylsp.pylsp_shared import PyflakesDiagnosticReport
 from pyflakes import api as pyflakes_api
 from pyflakes import messages
 
 from pylsp import hookimpl, lsp
-
-# Pyflakes messages that should be reported as Errors instead of Warns
-PYFLAKES_ERROR_MESSAGES = (
-    messages.UndefinedName,
-    messages.UndefinedExport,
-    messages.UndefinedLocal,
-    messages.DuplicateArgument,
-    messages.FutureFeatureNotDefined,
-    messages.ReturnOutsideFunction,
-    messages.YieldOutsideFunction,
-    messages.ContinueOutsideLoop,
-    messages.BreakOutsideLoop,
-    messages.TwoStarredExpressions,
-)
-
-
-@hookimpl
-def pylsp_lint(workspace, document):
-    with workspace.report_progress("lint: pyflakes"):
-        reporter = PyflakesDiagnosticReport(document.lines)
-        pyflakes_api.check(
-            document.source.encode("utf-8"), document.path, reporter=reporter
-        )
-        return reporter.diagnostics
-
-
-class PyflakesDiagnosticReport:
-    def __init__(self, lines) -> None:
-        self.lines = lines
-        self.diagnostics = []
-
-    def unexpectedError(self, _filename, msg) -> None:  # pragma: no cover
-        err_range = {
-            "start": {"line": 0, "character": 0},
-            "end": {"line": 0, "character": 0},
-        }
-        self.diagnostics.append(
-            {
-                "source": "pyflakes",
-                "range": err_range,
-                "message": msg,
-                "severity": lsp.DiagnosticSeverity.Error,
-            }
-        )
-
-    def syntaxError(self, _filename, msg, lineno, offset, text) -> None:
-        # We've seen that lineno and offset can sometimes be None
-        lineno = lineno or 1
-        offset = offset or 0
-        # could be None if the error is due to an invalid encoding
-        # see e.g. https://github.com/python-lsp/python-lsp-server/issues/429
-        text = text or ""
-
-        err_range = {
-            "start": {"line": lineno - 1, "character": offset},
-            "end": {"line": lineno - 1, "character": offset + len(text)},
-        }
-        self.diagnostics.append(
-            {
-                "source": "pyflakes",
-                "range": err_range,
-                "message": msg,
-                "severity": lsp.DiagnosticSeverity.Error,
-            }
-        )
-
-    def flake(self, message) -> None:
-        """Get message like <filename>:<lineno>: <msg>"""
-        err_range = {
-            "start": {"line": message.lineno - 1, "character": message.col},
-            "end": {
-                "line": message.lineno - 1,
-                "character": len(self.lines[message.lineno - 1]),
-            },
-        }
-
-        severity = lsp.DiagnosticSeverity.Warning
-        for message_type in PYFLAKES_ERROR_MESSAGES:
-            if isinstance(message, message_type):
-                severity = lsp.DiagnosticSeverity.Error
-                break
-
-        self.diagnostics.append(
-            {
-                "source": "pyflakes",
-                "range": err_range,
-                "message": message.message % message.message_args,
-                "severity": severity,
-            }
-        )
diff --git a/pylsp/plugins/pylint_lint.py b/pylsp/plugins/pylint_lint.py
index beffe6f..5ff622a 100644
--- a/pylsp/plugins/pylint_lint.py
+++ b/pylsp/plugins/pylint_lint.py
@@ -4,6 +4,13 @@
 
 """Linter plugin for pylint."""
 
+from pylsp.pylsp_shared import log
+from pylsp.pylsp_shared import DEPRECATION_CODES
+from pylsp.pylsp_shared import UNNECESSITY_CODES
+from pylsp.pylsp_shared import PylintLinter
+from pylsp.pylsp_shared import _build_pylint_flags
+from pylsp.pylsp_shared import build_args_stdio
+from pylsp.pylsp_shared import pylint_lint_stdin
 import collections
 import logging
 import os
@@ -19,8 +26,6 @@ try:
 except Exception:
     import json
 
-log = logging.getLogger(__name__)
-
 # Pylint fails to suppress STDOUT when importing whitelisted C
 # extensions, mangling their output into the expected JSON which breaks the
 # parser. The most prominent example (and maybe the only one out there) is
@@ -29,326 +34,3 @@ log = logging.getLogger(__name__)
 # fix for a very specific upstream issue.
 # Related: https://github.com/PyCQA/pylint/issues/3518
 os.environ["PYGAME_HIDE_SUPPORT_PROMPT"] = "hide"
-DEPRECATION_CODES = {
-    "W0402",  # Uses of a deprecated module %r
-    "W1505",  # Using deprecated method %s()
-    "W1511",  # Using deprecated argument %s of method %s()
-    "W1512",  # Using deprecated class %s of module %s
-    "W1513",  # Using deprecated decorator %s()
-}
-UNNECESSITY_CODES = {
-    "W0611",  # Unused import %s
-    "W0612",  # Unused variable %r
-    "W0613",  # Unused argument %r
-    "W0614",  # Unused import %s from wildcard import
-    "W1304",  # Unused-format-string-argument
-}
-
-
-class PylintLinter:
-    last_diags = collections.defaultdict(list)
-
-    @classmethod
-    def lint(cls, document, is_saved, flags=""):
-        """Plugin interface to pylsp linter.
-
-        Args:
-            document: The document to be linted.
-            is_saved: Whether or not the file has been saved to disk.
-            flags: Additional flags to pass to pylint. Not exposed to
-                pylsp_lint, but used for testing.
-
-        Returns:
-            A list of dicts with the following format:
-
-                {
-                    'source': 'pylint',
-                    'range': {
-                        'start': {
-                            'line': start_line,
-                            'character': start_column,
-                        },
-                        'end': {
-                            'line': end_line,
-                            'character': end_column,
-                        },
-                    }
-                    'message': msg,
-                    'severity': lsp.DiagnosticSeverity.*,
-                }
-        """
-        if not is_saved:
-            # Pylint can only be run on files that have been saved to disk.
-            # Rather than return nothing, return the previous list of
-            # diagnostics. If we return an empty list, any diagnostics we'd
-            # previously shown will be cleared until the next save. Instead,
-            # continue showing (possibly stale) diagnostics until the next
-            # save.
-            return cls.last_diags[document.path]
-
-        cmd = [
-            sys.executable,
-            "-c",
-            "import sys; from pylint.lint import Run; Run(sys.argv[1:])",
-            "-f",
-            "json",
-            document.path,
-        ] + (shlex.split(str(flags)) if flags else [])
-        log.debug("Calling pylint with '%s'", " ".join(cmd))
-
-        cwd = document._workspace.root_path
-        if not cwd:
-            cwd = os.path.dirname(__file__)
-
-        with Popen(
-            cmd, stdout=PIPE, stderr=PIPE, cwd=cwd, universal_newlines=True
-        ) as process:
-            json_out, err = process.communicate()
-
-        if err != "":
-            log.error("Error calling pylint: '%s'", err)
-
-        # pylint prints nothing rather than [] when there are no diagnostics.
-        # json.loads will not parse an empty string, so just return.
-        if not json_out.strip():
-            cls.last_diags[document.path] = []
-            return []
-
-        # Pylint's JSON output is a list of objects with the following format.
-        #
-        #     {
-        #         "obj": "main",
-        #         "path": "foo.py",
-        #         "message": "Missing function docstring",
-        #         "message-id": "C0111",
-        #         "symbol": "missing-docstring",
-        #         "column": 0,
-        #         "type": "convention",
-        #         "line": 5,
-        #         "module": "foo"
-        #     }
-        #
-        # The type can be any of:
-        #
-        #  * convention
-        #  * information
-        #  * error
-        #  * fatal
-        #  * refactor
-        #  * warning
-        diagnostics = []
-        for diag in json.loads(json_out):
-            # pylint lines index from 1, pylsp lines index from 0
-            line = diag["line"] - 1
-
-            err_range = {
-                "start": {
-                    "line": line,
-                    # Index columns start from 0
-                    "character": diag["column"],
-                },
-                "end": {
-                    "line": line,
-                    # It's possible that we're linting an empty file. Even an empty
-                    # file might fail linting if it isn't named properly.
-                    "character": len(document.lines[line]) if document.lines else 0,
-                },
-            }
-
-            if diag["type"] == "convention":
-                severity = lsp.DiagnosticSeverity.Information
-            elif diag["type"] == "information":
-                severity = lsp.DiagnosticSeverity.Information
-            elif diag["type"] == "error":
-                severity = lsp.DiagnosticSeverity.Error
-            elif diag["type"] == "fatal":
-                severity = lsp.DiagnosticSeverity.Error
-            elif diag["type"] == "refactor":
-                severity = lsp.DiagnosticSeverity.Hint
-            elif diag["type"] == "warning":
-                severity = lsp.DiagnosticSeverity.Warning
-
-            code = diag["message-id"]
-
-            diagnostic = {
-                "source": "pylint",
-                "range": err_range,
-                "message": "[{}] {}".format(diag["symbol"], diag["message"]),
-                "severity": severity,
-                "code": code,
-            }
-
-            if code in UNNECESSITY_CODES:
-                diagnostic["tags"] = [lsp.DiagnosticTag.Unnecessary]
-            if code in DEPRECATION_CODES:
-                diagnostic["tags"] = [lsp.DiagnosticTag.Deprecated]
-
-            diagnostics.append(diagnostic)
-        cls.last_diags[document.path] = diagnostics
-        return diagnostics
-
-
-def _build_pylint_flags(settings):
-    """Build arguments for calling pylint."""
-    pylint_args = settings.get("args")
-    if pylint_args is None:
-        return ""
-    return " ".join(pylint_args)
-
-
-@hookimpl
-def pylsp_settings():
-    # Default pylint to disabled because it requires a config
-    # file to be useful.
-    return {
-        "plugins": {
-            "pylint": {
-                "enabled": False,
-                "args": [],
-                # disabled by default as it can slow down the workflow
-                "executable": None,
-            }
-        }
-    }
-
-
-@hookimpl
-def pylsp_lint(config, workspace, document, is_saved):
-    """Run pylint linter."""
-    with workspace.report_progress("lint: pylint"):
-        settings = config.plugin_settings("pylint")
-        log.debug("Got pylint settings: %s", settings)
-        # pylint >= 2.5.0 is required for working through stdin and only
-        # available with python3
-        if settings.get("executable") and sys.version_info[0] >= 3:
-            flags = build_args_stdio(settings)
-            pylint_executable = settings.get("executable", "pylint")
-            return pylint_lint_stdin(pylint_executable, document, flags)
-        flags = _build_pylint_flags(settings)
-        return PylintLinter.lint(document, is_saved, flags=flags)
-
-
-def build_args_stdio(settings):
-    """Build arguments for calling pylint.
-
-    :param settings: client settings
-    :type settings: dict
-
-    :return: arguments to path to pylint
-    :rtype: list
-    """
-    pylint_args = settings.get("args")
-    if pylint_args is None:
-        return []
-    return pylint_args
-
-
-def pylint_lint_stdin(pylint_executable, document, flags):
-    """Run pylint linter from stdin.
-
-    This runs pylint in a subprocess with popen.
-    This allows passing the file from stdin and as a result
-    run pylint on unsaved files. Can slowdown the workflow.
-
-    :param pylint_executable: path to pylint executable
-    :type pylint_executable: string
-    :param document: document to run pylint on
-    :type document: pylsp.workspace.Document
-    :param flags: arguments to path to pylint
-    :type flags: list
-
-    :return: linting diagnostics
-    :rtype: list
-    """
-    pylint_result = _run_pylint_stdio(pylint_executable, document, flags)
-    return _parse_pylint_stdio_result(document, pylint_result)
-
-
-def _run_pylint_stdio(pylint_executable, document, flags):
-    """Run pylint in popen.
-
-    :param pylint_executable: path to pylint executable
-    :type pylint_executable: string
-    :param document: document to run pylint on
-    :type document: pylsp.workspace.Document
-    :param flags: arguments to path to pylint
-    :type flags: list
-
-    :return: result of calling pylint
-    :rtype: string
-    """
-    log.debug("Calling %s with args: '%s'", pylint_executable, flags)
-    try:
-        cmd = [pylint_executable]
-        cmd.extend(flags)
-        cmd.extend(["--from-stdin", document.path])
-        p = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE)
-    except IOError:
-        log.debug("Can't execute %s. Trying with 'python -m pylint'", pylint_executable)
-        cmd = [sys.executable, "-m", "pylint"]
-        cmd.extend(flags)
-        cmd.extend(["--from-stdin", document.path])
-        p = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE)
-    (stdout, stderr) = p.communicate(document.source.encode())
-    if stderr:
-        log.error("Error while running pylint '%s'", stderr.decode())
-    return stdout.decode()
-
-
-def _parse_pylint_stdio_result(document, stdout):
-    """Parse pylint results.
-
-    :param document: document to run pylint on
-    :type document: pylsp.workspace.Document
-    :param stdout: pylint results to parse
-    :type stdout: string
-
-    :return: linting diagnostics
-    :rtype: list
-    """
-    diagnostics = []
-    lines = stdout.splitlines()
-    for raw_line in lines:
-        parsed_line = re.match(r"(.*):(\d*):(\d*): (\w*): (.*)", raw_line)
-        if not parsed_line:
-            log.debug("Pylint output parser can't parse line '%s'", raw_line)
-            continue
-
-        parsed_line = parsed_line.groups()
-        if len(parsed_line) != 5:
-            log.debug("Pylint output parser can't parse line '%s'", raw_line)
-            continue
-
-        _, line, character, code, msg = parsed_line
-        line = int(line) - 1
-        character = int(character)
-        severity_map = {
-            "C": lsp.DiagnosticSeverity.Information,
-            "E": lsp.DiagnosticSeverity.Error,
-            "F": lsp.DiagnosticSeverity.Error,
-            "I": lsp.DiagnosticSeverity.Information,
-            "R": lsp.DiagnosticSeverity.Hint,
-            "W": lsp.DiagnosticSeverity.Warning,
-        }
-        severity = severity_map[code[0]]
-        diagnostic = {
-            "source": "pylint",
-            "code": code,
-            "range": {
-                "start": {"line": line, "character": character},
-                "end": {
-                    "line": line,
-                    # no way to determine the column
-                    "character": len(document.lines[line]) - 1,
-                },
-            },
-            "message": msg,
-            "severity": severity,
-        }
-        if code in UNNECESSITY_CODES:
-            diagnostic["tags"] = [lsp.DiagnosticTag.Unnecessary]
-        if code in DEPRECATION_CODES:
-            diagnostic["tags"] = [lsp.DiagnosticTag.Deprecated]
-        diagnostics.append(diagnostic)
-
-    return diagnostics
diff --git a/pylsp/plugins/references.py b/pylsp/plugins/references.py
index a4c61b5..514f60e 100644
--- a/pylsp/plugins/references.py
+++ b/pylsp/plugins/references.py
@@ -6,28 +6,3 @@ import logging
 from pylsp import _utils, hookimpl, uris
 
 log = logging.getLogger(__name__)
-
-
-@hookimpl
-def pylsp_references(document, position, exclude_declaration):
-    code_position = _utils.position_to_jedi_linecolumn(document, position)
-    usages = document.jedi_script().get_references(**code_position)
-
-    if exclude_declaration:
-        # Filter out if the usage is the actual declaration of the thing
-        usages = [d for d in usages if not d.is_definition()]
-
-    # Filter out builtin modules
-    return [
-        {
-            "uri": uris.uri_with(document.uri, path=str(d.module_path))
-            if d.module_path
-            else document.uri,
-            "range": {
-                "start": {"line": d.line - 1, "character": d.column},
-                "end": {"line": d.line - 1, "character": d.column + len(d.name)},
-            },
-        }
-        for d in usages
-        if not d.in_builtin_module()
-    ]
diff --git a/pylsp/plugins/rope_autoimport.py b/pylsp/plugins/rope_autoimport.py
index 12f5d80..5447f5b 100644
--- a/pylsp/plugins/rope_autoimport.py
+++ b/pylsp/plugins/rope_autoimport.py
@@ -1,5 +1,16 @@
 # Copyright 2022- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import log
+from pylsp.pylsp_shared import MAX_RESULTS_COMPLETIONS
+from pylsp.pylsp_shared import _should_insert
+from pylsp.pylsp_shared import _score_pow
+from pylsp.pylsp_shared import _score_max
+from pylsp.pylsp_shared import _process_statements
+from pylsp.pylsp_shared import get_names
+from pylsp.pylsp_shared import AutoimportCache
+from pylsp.pylsp_shared import cache
+from pylsp.pylsp_shared import MAX_RESULTS_CODE_ACTIONS
+from pylsp.pylsp_shared import get_name_or_module
 import logging
 import threading
 from typing import Any, Dict, Generator, List, Optional, Set, Union
@@ -17,391 +28,3 @@ from pylsp.config.config import Config
 from pylsp.workspace import Document, Workspace
 
 from ._rope_task_handle import PylspTaskHandle
-
-log = logging.getLogger(__name__)
-
-_score_pow = 5
-_score_max = 10**_score_pow
-MAX_RESULTS_COMPLETIONS = 1000
-MAX_RESULTS_CODE_ACTIONS = 5
-
-
-class AutoimportCache:
-    """Handles the cache creation."""
-
-    def __init__(self) -> None:
-        self.thread = None
-
-    def reload_cache(
-        self,
-        config: Config,
-        workspace: Workspace,
-        files: Optional[List[Document]] = None,
-        single_thread: Optional[bool] = True,
-    ):
-        if self.is_blocked():
-            return
-
-        memory: bool = config.plugin_settings("rope_autoimport").get("memory", False)
-        rope_config = config.settings().get("rope", {})
-        autoimport = workspace._rope_autoimport(rope_config, memory)
-        resources: Optional[List[Resource]] = (
-            None
-            if files is None
-            else [document._rope_resource(rope_config) for document in files]
-        )
-
-        if single_thread:
-            self._reload_cache(workspace, autoimport, resources)
-        else:
-            # Creating the cache may take 10-20s for a environment with 5k python modules. That's
-            # why we decided to move cache creation into its own thread.
-            self.thread = threading.Thread(
-                target=self._reload_cache, args=(workspace, autoimport, resources)
-            )
-            self.thread.start()
-
-    def _reload_cache(
-        self,
-        workspace: Workspace,
-        autoimport: AutoImport,
-        resources: Optional[List[Resource]] = None,
-    ) -> None:
-        task_handle = PylspTaskHandle(workspace)
-        autoimport.generate_cache(task_handle=task_handle, resources=resources)
-        autoimport.generate_modules_cache(task_handle=task_handle)
-
-    def is_blocked(self):
-        return self.thread and self.thread.is_alive()
-
-
-@hookimpl
-def pylsp_settings() -> Dict[str, Dict[str, Dict[str, Any]]]:
-    # Default rope_completion to disabled
-    return {
-        "plugins": {
-            "rope_autoimport": {
-                "enabled": False,
-                "memory": False,
-                "completions": {
-                    "enabled": True,
-                },
-                "code_actions": {
-                    "enabled": True,
-                },
-            }
-        }
-    }
-
-
-def _should_insert(expr: tree.BaseNode, word_node: tree.Leaf) -> bool:
-    """
-    Check if we should insert the word_node on the given expr.
-
-    Works for both correct and incorrect code. This is because the
-    user is often working on the code as they write it.
-    """
-    if not word_node:
-        return False
-    if len(expr.children) == 0:
-        return True
-    first_child = expr.children[0]
-    if isinstance(first_child, tree.EndMarker):
-        if "#" in first_child.prefix:
-            return False  # Check for single line comment
-    if first_child == word_node:
-        return True  # If the word is the first word then its fine
-    if len(expr.children) > 1:
-        if any(
-            node.type == "operator" and "." in node.value or node.type == "trailer"
-            for node in expr.children
-        ):
-            return False  # Check if we're on a method of a function
-    if isinstance(first_child, (tree.PythonErrorNode, tree.PythonNode)):
-        # The tree will often include error nodes like this to indicate errors
-        # we want to ignore errors since the code is being written
-        return _should_insert(first_child, word_node)
-    return _handle_first_child(first_child, expr, word_node)
-
-
-def _handle_first_child(
-    first_child: NodeOrLeaf, expr: tree.BaseNode, word_node: tree.Leaf
-) -> bool:
-    """Check if we suggest imports given the following first child."""
-    if isinstance(first_child, tree.Import):
-        return False
-    if isinstance(first_child, (tree.PythonLeaf, tree.PythonErrorLeaf)):
-        # Check if the first item is a from or import statement even when incomplete
-        if first_child.value in ("import", "from"):
-            return False
-    if isinstance(first_child, tree.Keyword):
-        if first_child.value == "def":
-            return _should_import_function(word_node, expr)
-        if first_child.value == "class":
-            return _should_import_class(word_node, expr)
-    return True
-
-
-def _should_import_class(word_node: tree.Leaf, expr: tree.BaseNode) -> bool:
-    prev_node = None
-    for node in expr.children:
-        if isinstance(node, tree.Name):
-            if isinstance(prev_node, tree.Operator):
-                if node == word_node and prev_node.value == "(":
-                    return True
-        prev_node = node
-
-    return False
-
-
-def _should_import_function(word_node: tree.Leaf, expr: tree.BaseNode) -> bool:
-    prev_node = None
-    for node in expr.children:
-        if _handle_argument(node, word_node):
-            return True
-        if isinstance(prev_node, tree.Operator):
-            if prev_node.value == "->":
-                if node == word_node:
-                    return True
-        prev_node = node
-    return False
-
-
-def _handle_argument(node: NodeOrLeaf, word_node: tree.Leaf):
-    if isinstance(node, tree.PythonNode):
-        if node.type == "tfpdef":
-            if node.children[2] == word_node:
-                return True
-        if node.type == "parameters":
-            for parameter in node.children:
-                if _handle_argument(parameter, word_node):
-                    return True
-    return False
-
-
-def _process_statements(
-    suggestions: List[SearchResult],
-    doc_uri: str,
-    word: str,
-    autoimport: AutoImport,
-    document: Document,
-    feature: str = "completions",
-) -> Generator[Dict[str, Any], None, None]:
-    for suggestion in suggestions:
-        insert_line = autoimport.find_insertion_line(document.source) - 1
-        start = {"line": insert_line, "character": 0}
-        edit_range = {"start": start, "end": start}
-        edit = {"range": edit_range, "newText": suggestion.import_statement + "\n"}
-        score = _get_score(
-            suggestion.source, suggestion.import_statement, suggestion.name, word
-        )
-        if score > _score_max:
-            continue
-        # TODO make this markdown
-        if feature == "completions":
-            yield {
-                "label": suggestion.name,
-                "kind": suggestion.itemkind,
-                "sortText": _sort_import(score),
-                "data": {"doc_uri": doc_uri},
-                "detail": _document(suggestion.import_statement),
-                "additionalTextEdits": [edit],
-            }
-        elif feature == "code_actions":
-            yield {
-                "title": suggestion.import_statement,
-                "kind": "quickfix",
-                "edit": {"changes": {doc_uri: [edit]}},
-                # data is a supported field for codeAction responses
-                # See https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocument_codeAction
-                "data": {"sortText": _sort_import(score)},
-            }
-        else:
-            raise ValueError(f"Unknown feature: {feature}")
-
-
-def get_names(script: Script) -> Set[str]:
-    """Get all names to ignore from the current file."""
-    raw_names = script.get_names(definitions=True)
-    log.debug(raw_names)
-    return {name.name for name in raw_names}
-
-
-@hookimpl
-def pylsp_completions(
-    config: Config,
-    workspace: Workspace,
-    document: Document,
-    position,
-    ignored_names: Union[Set[str], None],
-):
-    """Get autoimport suggestions."""
-    if (
-        not config.plugin_settings("rope_autoimport")
-        .get("completions", {})
-        .get("enabled", True)
-    ) or cache.is_blocked():
-        return []
-
-    line = document.lines[position["line"]]
-    expr = parso.parse(line)
-    word_node = expr.get_leaf_for_position((1, position["character"]))
-    if not _should_insert(expr, word_node):
-        return []
-    word = word_node.value
-    log.debug(f"autoimport: searching for word: {word}")
-    rope_config = config.settings(document_path=document.path).get("rope", {})
-    ignored_names: Set[str] = ignored_names or get_names(
-        document.jedi_script(use_document_path=True)
-    )
-    autoimport = workspace._rope_autoimport(rope_config)
-    suggestions = list(autoimport.search_full(word, ignored_names=ignored_names))
-    results = sorted(
-        _process_statements(
-            suggestions, document.uri, word, autoimport, document, "completions"
-        ),
-        key=lambda statement: statement["sortText"],
-    )
-    if len(results) > MAX_RESULTS_COMPLETIONS:
-        results = results[:MAX_RESULTS_COMPLETIONS]
-    return results
-
-
-def _document(import_statement: str) -> str:
-    return """# Auto-Import\n""" + import_statement
-
-
-def _get_score(
-    source: int, full_statement: str, suggested_name: str, desired_name
-) -> int:
-    import_length = len("import")
-    full_statement_score = len(full_statement) - import_length
-    suggested_name_score = (len(suggested_name) - len(desired_name)) ** 2
-    source_score = 20 * source
-    return suggested_name_score + full_statement_score + source_score
-
-
-def _sort_import(score: int) -> str:
-    score = max(min(score, (_score_max) - 1), 0)
-    # Since we are using ints, we need to pad them.
-    # We also want to prioritize autoimport behind everything since its the last priority.
-    # The minimum is to prevent score from overflowing the pad
-    return "[z" + str(score).rjust(_score_pow, "0")
-
-
-def get_name_or_module(document, diagnostic) -> str:
-    start = diagnostic["range"]["start"]
-    return (
-        parso.parse(document.lines[start["line"]])
-        .get_leaf_for_position((1, start["character"] + 1))
-        .value
-    )
-
-
-@hookimpl
-def pylsp_code_actions(
-    config: Config,
-    workspace: Workspace,
-    document: Document,
-    range: Dict,
-    context: Dict,
-) -> List[Dict]:
-    """
-    Provide code actions through rope.
-
-    Parameters
-    ----------
-    config : pylsp.config.config.Config
-        Current config.
-    workspace : pylsp.workspace.Workspace
-        Current workspace.
-    document : pylsp.workspace.Document
-        Document to apply code actions on.
-    range : Dict
-        Range argument given by pylsp. Not used here.
-    context : Dict
-        CodeActionContext given as dict.
-
-    Returns
-    -------
-      List of dicts containing the code actions.
-    """
-    if (
-        not config.plugin_settings("rope_autoimport")
-        .get("code_actions", {})
-        .get("enabled", True)
-    ) or cache.is_blocked():
-        return []
-
-    log.debug(f"textDocument/codeAction: {document} {range} {context}")
-    code_actions = []
-    for diagnostic in context.get("diagnostics", []):
-        if "undefined name" not in diagnostic.get("message", "").lower():
-            continue
-
-        word = get_name_or_module(document, diagnostic)
-        log.debug(f"autoimport: searching for word: {word}")
-        rope_config = config.settings(document_path=document.path).get("rope", {})
-        autoimport = workspace._rope_autoimport(rope_config)
-        suggestions = list(autoimport.search_full(word))
-        log.debug("autoimport: suggestions: %s", suggestions)
-        results = sorted(
-            _process_statements(
-                suggestions,
-                document.uri,
-                word,
-                autoimport,
-                document,
-                "code_actions",
-            ),
-            key=lambda statement: statement["data"]["sortText"],
-        )
-
-        if len(results) > MAX_RESULTS_CODE_ACTIONS:
-            results = results[:MAX_RESULTS_CODE_ACTIONS]
-        code_actions.extend(results)
-
-    return code_actions
-
-
-@hookimpl
-def pylsp_initialize(config: Config, workspace: Workspace) -> None:
-    """Initialize AutoImport.
-
-    Generates the cache for local and global items.
-    """
-    cache.reload_cache(config, workspace)
-
-
-@hookimpl
-def pylsp_document_did_open(config: Config, workspace: Workspace) -> None:
-    """Initialize AutoImport.
-
-    Generates the cache for local and global items.
-    """
-    cache.reload_cache(config, workspace)
-
-
-@hookimpl
-def pylsp_document_did_save(
-    config: Config, workspace: Workspace, document: Document
-) -> None:
-    """Update the names associated with this document."""
-    cache.reload_cache(config, workspace, [document])
-
-
-@hookimpl
-def pylsp_workspace_configuration_changed(config: Config, workspace: Workspace) -> None:
-    """
-    Initialize autoimport if it has been enabled through a
-    workspace/didChangeConfiguration message from the frontend.
-
-    Generates the cache for local and global items.
-    """
-    if config.plugin_settings("rope_autoimport").get("enabled", False):
-        cache.reload_cache(config, workspace)
-    else:
-        log.debug("autoimport: Skipping cache reload.")
-
-
-cache: AutoimportCache = AutoimportCache()
diff --git a/pylsp/plugins/rope_completion.py b/pylsp/plugins/rope_completion.py
index b3a1f06..7317474 100644
--- a/pylsp/plugins/rope_completion.py
+++ b/pylsp/plugins/rope_completion.py
@@ -1,161 +1,12 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import log
+from pylsp.pylsp_shared import _resolve_completion
+from pylsp.pylsp_shared import _sort_text
+from pylsp.pylsp_shared import _kind
 import logging
 
 from rope.contrib.codeassist import code_assist, sorted_proposals
 
 from pylsp import _utils, hookimpl, lsp
-
-log = logging.getLogger(__name__)
-
-
-@hookimpl
-def pylsp_settings():
-    # Default rope_completion to disabled
-    return {"plugins": {"rope_completion": {"enabled": False, "eager": False}}}
-
-
-def _resolve_completion(completion, data, markup_kind):
-    try:
-        doc = _utils.format_docstring(data.get_doc(), markup_kind=markup_kind)
-    except Exception as e:
-        log.debug("Failed to resolve Rope completion: %s", e)
-        doc = ""
-    completion["detail"] = "{0} {1}".format(data.scope or "", data.name)
-    completion["documentation"] = doc
-    return completion
-
-
-@hookimpl
-def pylsp_completions(config, workspace, document, position):
-    settings = config.plugin_settings("rope_completion", document_path=document.path)
-    resolve_eagerly = settings.get("eager", False)
-
-    # Rope is a bit rubbish at completing module imports, so we'll return None
-    word = document.word_at_position(
-        {
-            # The -1 should really be trying to look at the previous word, but that might be quite expensive
-            # So we only skip import completions when the cursor is one space after `import`
-            "line": position["line"],
-            "character": max(position["character"] - 1, 0),
-        }
-    )
-    if word == "import":
-        return None
-
-    offset = document.offset_at_position(position)
-    rope_config = config.settings(document_path=document.path).get("rope", {})
-    rope_project = workspace._rope_project_builder(rope_config)
-    document_rope = document._rope_resource(rope_config)
-
-    completion_capabilities = config.capabilities.get("textDocument", {}).get(
-        "completion", {}
-    )
-    item_capabilities = completion_capabilities.get("completionItem", {})
-    supported_markup_kinds = item_capabilities.get("documentationFormat", ["markdown"])
-    preferred_markup_kind = _utils.choose_markup_kind(supported_markup_kinds)
-
-    try:
-        definitions = code_assist(
-            rope_project, document.source, offset, document_rope, maxfixes=3
-        )
-    except Exception as e:
-        log.debug("Failed to run Rope code assist: %s", e)
-        return []
-
-    definitions = sorted_proposals(definitions)
-    new_definitions = []
-    for d in definitions:
-        item = {
-            "label": d.name,
-            "kind": _kind(d),
-            "sortText": _sort_text(d),
-            "data": {"doc_uri": document.uri},
-        }
-        if resolve_eagerly:
-            item = _resolve_completion(item, d, preferred_markup_kind)
-        new_definitions.append(item)
-
-    # most recently retrieved completion items, used for resolution
-    document.shared_data["LAST_ROPE_COMPLETIONS"] = {
-        # label is the only required property; here it is assumed to be unique
-        completion["label"]: (completion, data)
-        for completion, data in zip(new_definitions, definitions)
-    }
-
-    definitions = new_definitions
-
-    return definitions or None
-
-
-@hookimpl
-def pylsp_completion_item_resolve(config, completion_item, document):
-    """Resolve formatted completion for given non-resolved completion"""
-    shared_data = document.shared_data["LAST_ROPE_COMPLETIONS"].get(
-        completion_item["label"]
-    )
-
-    completion_capabilities = config.capabilities.get("textDocument", {}).get(
-        "completion", {}
-    )
-    item_capabilities = completion_capabilities.get("completionItem", {})
-    supported_markup_kinds = item_capabilities.get("documentationFormat", ["markdown"])
-    preferred_markup_kind = _utils.choose_markup_kind(supported_markup_kinds)
-
-    if shared_data:
-        completion, data = shared_data
-        return _resolve_completion(completion, data, preferred_markup_kind)
-    return completion_item
-
-
-def _sort_text(definition):
-    """Ensure builtins appear at the bottom.
-    Description is of format <type>: <module>.<item>
-    """
-    if definition.name.startswith("_"):
-        # It's a 'hidden' func, put it next last
-        return "z" + definition.name
-    if definition.scope == "builtin":
-        return "y" + definition.name
-
-    # Else put it at the front
-    return "a" + definition.name
-
-
-def _kind(d):
-    """Return the LSP type"""
-    MAP = {
-        "none": lsp.CompletionItemKind.Value,
-        "type": lsp.CompletionItemKind.Class,
-        "tuple": lsp.CompletionItemKind.Class,
-        "dict": lsp.CompletionItemKind.Class,
-        "dictionary": lsp.CompletionItemKind.Class,
-        "function": lsp.CompletionItemKind.Function,
-        "lambda": lsp.CompletionItemKind.Function,
-        "generator": lsp.CompletionItemKind.Function,
-        "class": lsp.CompletionItemKind.Class,
-        "instance": lsp.CompletionItemKind.Reference,
-        "method": lsp.CompletionItemKind.Method,
-        "builtin": lsp.CompletionItemKind.Class,
-        "builtinfunction": lsp.CompletionItemKind.Function,
-        "module": lsp.CompletionItemKind.Module,
-        "file": lsp.CompletionItemKind.File,
-        "xrange": lsp.CompletionItemKind.Class,
-        "slice": lsp.CompletionItemKind.Class,
-        "traceback": lsp.CompletionItemKind.Class,
-        "frame": lsp.CompletionItemKind.Class,
-        "buffer": lsp.CompletionItemKind.Class,
-        "dictproxy": lsp.CompletionItemKind.Class,
-        "funcdef": lsp.CompletionItemKind.Function,
-        "property": lsp.CompletionItemKind.Property,
-        "import": lsp.CompletionItemKind.Module,
-        "keyword": lsp.CompletionItemKind.Keyword,
-        "constant": lsp.CompletionItemKind.Variable,
-        "variable": lsp.CompletionItemKind.Variable,
-        "value": lsp.CompletionItemKind.Value,
-        "param": lsp.CompletionItemKind.Variable,
-        "statement": lsp.CompletionItemKind.Keyword,
-    }
-
-    return MAP.get(d.type)
diff --git a/pylsp/plugins/signature.py b/pylsp/plugins/signature.py
index 7ad5b20..58d3417 100644
--- a/pylsp/plugins/signature.py
+++ b/pylsp/plugins/signature.py
@@ -1,81 +1,10 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import _param_docs
 import logging
 import re
 
 from pylsp import _utils, hookimpl
 
 log = logging.getLogger(__name__)
-
-SPHINX = re.compile(r"\s*:param\s+(?P<param>\w+):\s*(?P<doc>[^\n]+)")
-EPYDOC = re.compile(r"\s*@param\s+(?P<param>\w+):\s*(?P<doc>[^\n]+)")
-GOOGLE = re.compile(r"\s*(?P<param>\w+).*:\s*(?P<doc>[^\n]+)")
-
-DOC_REGEX = [SPHINX, EPYDOC, GOOGLE]
-
-
-@hookimpl
-def pylsp_signature_help(config, document, position):
-    code_position = _utils.position_to_jedi_linecolumn(document, position)
-    signatures = document.jedi_script().get_signatures(**code_position)
-
-    if not signatures:
-        return {"signatures": []}
-
-    signature_capabilities = config.capabilities.get("textDocument", {}).get(
-        "signatureHelp", {}
-    )
-    signature_information_support = signature_capabilities.get(
-        "signatureInformation", {}
-    )
-    supported_markup_kinds = signature_information_support.get(
-        "documentationFormat", ["markdown"]
-    )
-    preferred_markup_kind = _utils.choose_markup_kind(supported_markup_kinds)
-
-    s = signatures[0]
-
-    docstring = s.docstring()
-
-    # Docstring contains one or more lines of signature, followed by empty line, followed by docstring
-    function_sig_lines = (docstring.split("\n\n") or [""])[0].splitlines()
-    function_sig = " ".join([line.strip() for line in function_sig_lines])
-    sig = {
-        "label": function_sig,
-        "documentation": _utils.format_docstring(
-            s.docstring(raw=True), markup_kind=preferred_markup_kind
-        ),
-    }
-
-    # If there are params, add those
-    if s.params:
-        sig["parameters"] = [
-            {
-                "label": p.name,
-                "documentation": _utils.format_docstring(
-                    _param_docs(docstring, p.name), markup_kind=preferred_markup_kind
-                ),
-            }
-            for p in s.params
-        ]
-
-    # We only return a single signature because Python doesn't allow overloading
-    sig_info = {"signatures": [sig], "activeSignature": 0}
-
-    if s.index is not None and s.params:
-        # Then we know which parameter we're looking at
-        sig_info["activeParameter"] = s.index
-
-    return sig_info
-
-
-def _param_docs(docstring, param_name):
-    for line in docstring.splitlines():
-        for regex in DOC_REGEX:
-            m = regex.match(line)
-            if not m:
-                continue
-            if m.group("param") != param_name:
-                continue
-            return m.group("doc") or ""
diff --git a/pylsp/plugins/symbols.py b/pylsp/plugins/symbols.py
index 4e1890c..24344a0 100644
--- a/pylsp/plugins/symbols.py
+++ b/pylsp/plugins/symbols.py
@@ -1,6 +1,12 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import _include_def
+from pylsp.pylsp_shared import _container
+from pylsp.pylsp_shared import _range
+from pylsp.pylsp_shared import _tuple_range
+from pylsp.pylsp_shared import _SYMBOL_KIND_MAP
+from pylsp.pylsp_shared import _kind
 import logging
 from pathlib import Path
 
@@ -8,207 +14,3 @@ from pylsp import hookimpl
 from pylsp.lsp import SymbolKind
 
 log = logging.getLogger(__name__)
-
-
-@hookimpl
-def pylsp_document_symbols(config, document):
-    symbols_settings = config.plugin_settings("jedi_symbols")
-    all_scopes = symbols_settings.get("all_scopes", True)
-    add_import_symbols = symbols_settings.get("include_import_symbols", True)
-    definitions = document.jedi_names(all_scopes=all_scopes)
-    symbols = []
-    exclude = set({})
-    redefinitions = {}
-
-    while definitions != []:
-        d = definitions.pop(0)
-
-        # Skip symbols imported from other modules.
-        if not add_import_symbols:
-            # Skip if there's an import in the code the symbol is defined.
-            code = d.get_line_code()
-            if " import " in code or "import " in code:
-                continue
-
-            # Skip imported symbols comparing module names.
-            sym_full_name = d.full_name
-            if sym_full_name is not None:
-                document_dot_path = document.dot_path
-
-                # We assume a symbol is imported from another module to start
-                # with.
-                imported_symbol = True
-
-                # The last element of sym_full_name is the symbol itself, so
-                # we need to discard it to do module comparisons below.
-                if "." in sym_full_name:
-                    sym_module_name = sym_full_name.rpartition(".")[0]
-                else:
-                    sym_module_name = sym_full_name
-
-                # This is necessary to display symbols in init files (the checks
-                # below fail without it).
-                if document_dot_path.endswith("__init__"):
-                    document_dot_path = document_dot_path.rpartition(".")[0]
-
-                # document_dot_path is the module where the symbol is imported,
-                # whereas sym_module_name is the one where it was declared.
-                if document_dot_path in sym_module_name:
-                    # If document_dot_path is in sym_module_name, we can safely assume
-                    # that the symbol was declared in the document.
-                    imported_symbol = False
-                elif sym_module_name.split(".")[0] in document_dot_path.split("."):
-                    # If the first module in sym_module_name is one of the modules in
-                    # document_dot_path, we need to check if sym_module_name starts
-                    # with the modules in document_dot_path.
-                    document_mods = document_dot_path.split(".")
-                    for i in range(1, len(document_mods) + 1):
-                        submod = ".".join(document_mods[-i:])
-                        if sym_module_name.startswith(submod):
-                            imported_symbol = False
-                            break
-
-                # When there's no __init__.py next to a file or in one of its
-                # parents, the checks above fail. However, Jedi has a nice way
-                # to tell if the symbol was declared in the same file: if
-                # sym_module_name starts by __main__.
-                if imported_symbol:
-                    if not sym_module_name.startswith("__main__"):
-                        continue
-            else:
-                # We need to skip symbols if their definition doesn't have `full_name` info, they
-                # are detected as a definition, but their description (e.g. `class Foo`) doesn't
-                # match the code where they're detected by Jedi. This happens for relative imports.
-                if _include_def(d):
-                    if d.description not in d.get_line_code():
-                        continue
-                else:
-                    continue
-
-        if _include_def(d) and Path(document.path) == Path(d.module_path):
-            tuple_range = _tuple_range(d)
-            if tuple_range in exclude:
-                continue
-
-            kind = redefinitions.get(tuple_range, None)
-            if kind is not None:
-                exclude |= {tuple_range}
-
-            if d.type == "statement":
-                if d.description.startswith("self"):
-                    kind = "field"
-
-            symbol = {
-                "name": d.name,
-                "containerName": _container(d),
-                "location": {
-                    "uri": document.uri,
-                    "range": _range(d),
-                },
-                "kind": _kind(d) if kind is None else _SYMBOL_KIND_MAP[kind],
-            }
-            symbols.append(symbol)
-
-            if d.type == "class":
-                try:
-                    defined_names = list(d.defined_names())
-                    for method in defined_names:
-                        if method.type == "function":
-                            redefinitions[_tuple_range(method)] = "method"
-                        elif method.type == "statement":
-                            redefinitions[_tuple_range(method)] = "field"
-                        else:
-                            redefinitions[_tuple_range(method)] = method.type
-                    definitions = list(defined_names) + definitions
-                except Exception:
-                    pass
-    return symbols
-
-
-def _include_def(definition):
-    return (
-        # Don't tend to include parameters as symbols
-        definition.type != "param"
-        and
-        # Unused vars should also be skipped
-        definition.name != "_"
-        and _kind(definition) is not None
-    )
-
-
-def _container(definition):
-    try:
-        # Jedi sometimes fails here.
-        parent = definition.parent()
-        # Here we check that a grand-parent exists to avoid declaring symbols
-        # as children of the module.
-        if parent.parent():
-            return parent.name
-    except:
-        return None
-
-    return None
-
-
-def _range(definition):
-    # This gets us more accurate end position
-    definition = definition._name.tree_name.get_definition()
-    (start_line, start_column) = definition.start_pos
-    (end_line, end_column) = definition.end_pos
-    return {
-        "start": {"line": start_line - 1, "character": start_column},
-        "end": {"line": end_line - 1, "character": end_column},
-    }
-
-
-def _tuple_range(definition):
-    definition = definition._name.tree_name.get_definition()
-    return (definition.start_pos, definition.end_pos)
-
-
-_SYMBOL_KIND_MAP = {
-    "none": SymbolKind.Variable,
-    "type": SymbolKind.Class,
-    "tuple": SymbolKind.Class,
-    "dict": SymbolKind.Class,
-    "dictionary": SymbolKind.Class,
-    "function": SymbolKind.Function,
-    "lambda": SymbolKind.Function,
-    "generator": SymbolKind.Function,
-    "class": SymbolKind.Class,
-    "instance": SymbolKind.Class,
-    "method": SymbolKind.Method,
-    "builtin": SymbolKind.Class,
-    "builtinfunction": SymbolKind.Function,
-    "module": SymbolKind.Module,
-    "file": SymbolKind.File,
-    "xrange": SymbolKind.Array,
-    "slice": SymbolKind.Class,
-    "traceback": SymbolKind.Class,
-    "frame": SymbolKind.Class,
-    "buffer": SymbolKind.Array,
-    "dictproxy": SymbolKind.Class,
-    "funcdef": SymbolKind.Function,
-    "property": SymbolKind.Property,
-    "import": SymbolKind.Module,
-    "keyword": SymbolKind.Variable,
-    "constant": SymbolKind.Constant,
-    "variable": SymbolKind.Variable,
-    "value": SymbolKind.Variable,
-    "param": SymbolKind.Variable,
-    "statement": SymbolKind.Variable,
-    "boolean": SymbolKind.Boolean,
-    "int": SymbolKind.Number,
-    "longlean": SymbolKind.Number,
-    "float": SymbolKind.Number,
-    "complex": SymbolKind.Number,
-    "string": SymbolKind.String,
-    "unicode": SymbolKind.String,
-    "list": SymbolKind.Array,
-    "field": SymbolKind.Field,
-}
-
-
-def _kind(d):
-    """Return the VSCode Symbol Type"""
-    return _SYMBOL_KIND_MAP.get(d.type)
diff --git a/pylsp/plugins/yapf_format.py b/pylsp/plugins/yapf_format.py
index 72aa740..363f2ab 100644
--- a/pylsp/plugins/yapf_format.py
+++ b/pylsp/plugins/yapf_format.py
@@ -1,6 +1,8 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import log
+from pylsp.pylsp_shared import _format
 import logging
 import os
 
@@ -10,190 +12,3 @@ from yapf.yapflib.yapf_api import FormatCode
 
 from pylsp import hookimpl
 from pylsp._utils import get_eol_chars
-
-log = logging.getLogger(__name__)
-
-
-@hookimpl
-def pylsp_format_document(workspace, document, options):
-    log.info("Formatting document %s with yapf", document)
-    with workspace.report_progress("format: yapf"):
-        return _format(document, options=options)
-
-
-@hookimpl
-def pylsp_format_range(document, range, options):
-    log.info("Formatting document %s in range %s with yapf", document, range)
-    # First we 'round' the range up/down to full lines only
-    range["start"]["character"] = 0
-    range["end"]["line"] += 1
-    range["end"]["character"] = 0
-
-    # From Yapf docs:
-    # lines: (list of tuples of integers) A list of tuples of lines, [start, end],
-    #   that we want to format. The lines are 1-based indexed. It can be used by
-    #   third-party code (e.g., IDEs) when reformatting a snippet of code rather
-    #   than a whole file.
-
-    # Add 1 for 1-indexing vs LSP's 0-indexing
-    lines = [(range["start"]["line"] + 1, range["end"]["line"] + 1)]
-    return _format(document, lines=lines, options=options)
-
-
-def get_style_config(document_path, options=None):
-    # Exclude file if it follows the patterns for that
-    exclude_patterns_from_ignore_file = file_resources.GetExcludePatternsForDir(
-        os.getcwd()
-    )
-    if file_resources.IsIgnored(document_path, exclude_patterns_from_ignore_file):
-        return []
-
-    # Get the default styles as a string
-    # for a preset configuration, i.e. "pep8"
-    style_config = file_resources.GetDefaultStyleForDir(os.path.dirname(document_path))
-    if options is None:
-        return style_config
-
-    # We have options passed from LSP format request
-    # let's pass them to the formatter.
-    # First we want to get a dictionary of the preset style
-    # to pass instead of a string so that we can modify it
-    style_config = style.CreateStyleFromConfig(style_config)
-
-    use_tabs = style_config["USE_TABS"]
-    indent_width = style_config["INDENT_WIDTH"]
-
-    if options.get("tabSize") is not None:
-        indent_width = max(int(options.get("tabSize")), 1)
-
-    if options.get("insertSpaces") is not None:
-        # TODO is it guaranteed to be a boolean, or can it be a string?
-        use_tabs = not options.get("insertSpaces")
-
-        if use_tabs:
-            # Indent width doesn't make sense when using tabs
-            # the specifications state: "Size of a tab in spaces"
-            indent_width = 1
-
-    style_config["USE_TABS"] = use_tabs
-    style_config["INDENT_WIDTH"] = indent_width
-    style_config["CONTINUATION_INDENT_WIDTH"] = indent_width
-
-    for style_option, value in options.items():
-        # Apply arbitrary options passed as formatter options
-        if style_option not in style_config:
-            # ignore if it's not a known yapf config
-            continue
-
-        style_config[style_option] = value
-
-    return style_config
-
-
-def diff_to_text_edits(diff, eol_chars):
-    # To keep things simple our text edits will be line based.
-    # We will also return the edits uncompacted, meaning a
-    # line replacement will come in as a line remove followed
-    # by a line add instead of a line replace.
-    text_edits = []
-    # keep track of line number since additions
-    # don't include the line number it's being added
-    # to in diffs. lsp is 0-indexed so we'll start with -1
-    prev_line_no = -1
-
-    for change in diff.changes:
-        if change.old and change.new:
-            # old and new are the same line, no change
-            # diffs are 1-indexed
-            prev_line_no = change.old - 1
-        elif change.new:
-            # addition
-            text_edits.append(
-                {
-                    "range": {
-                        "start": {"line": prev_line_no + 1, "character": 0},
-                        "end": {"line": prev_line_no + 1, "character": 0},
-                    },
-                    "newText": change.line + eol_chars,
-                }
-            )
-        elif change.old:
-            # remove
-            lsp_line_no = change.old - 1
-            text_edits.append(
-                {
-                    "range": {
-                        "start": {"line": lsp_line_no, "character": 0},
-                        "end": {
-                            # From LSP spec:
-                            # If you want to specify a range that contains a line
-                            # including the line ending character(s) then use an
-                            # end position denoting the start of the next line.
-                            "line": lsp_line_no + 1,
-                            "character": 0,
-                        },
-                    },
-                    "newText": "",
-                }
-            )
-            prev_line_no = lsp_line_no
-
-    return text_edits
-
-
-def ensure_eof_new_line(document, eol_chars, text_edits):
-    # diffs don't include EOF newline https://github.com/google/yapf/issues/1008
-    # we'll add it ourselves if our document doesn't already have it and the diff
-    # does not change the last line.
-    if document.source.endswith(eol_chars):
-        return
-
-    lines = document.lines
-    last_line_number = len(lines) - 1
-
-    if text_edits and text_edits[-1]["range"]["start"]["line"] >= last_line_number:
-        return
-
-    text_edits.append(
-        {
-            "range": {
-                "start": {"line": last_line_number, "character": 0},
-                "end": {"line": last_line_number + 1, "character": 0},
-            },
-            "newText": lines[-1] + eol_chars,
-        }
-    )
-
-
-def _format(document, lines=None, options=None):
-    source = document.source
-    # Yapf doesn't work with CRLF/CR line endings, so we replace them by '\n'
-    # and restore them below when adding new lines
-    eol_chars = get_eol_chars(source)
-    if eol_chars in ["\r", "\r\n"]:
-        source = source.replace(eol_chars, "\n")
-    else:
-        eol_chars = "\n"
-
-    style_config = get_style_config(document_path=document.path, options=options)
-
-    diff_txt, changed = FormatCode(
-        source,
-        lines=lines,
-        filename=document.filename,
-        print_diff=True,
-        style_config=style_config,
-    )
-
-    if not changed:
-        return []
-
-    patch_generator = whatthepatch.parse_patch(diff_txt)
-    diff = next(patch_generator)
-    patch_generator.close()
-
-    text_edits = diff_to_text_edits(diff=diff, eol_chars=eol_chars)
-
-    ensure_eof_new_line(document=document, eol_chars=eol_chars, text_edits=text_edits)
-
-    return text_edits
diff --git a/pylsp/pylsp_shared.py b/pylsp/pylsp_shared.py
new file mode 100644
index 0000000..24f571a
--- /dev/null
+++ b/pylsp/pylsp_shared.py
@@ -0,0 +1,3622 @@
+import logging
+import pycodestyle
+from autopep8 import continued_indentation as autopep8_c_i
+from autopep8 import fix_code
+from pylsp._utils import get_eol_chars
+from typing import TYPE_CHECKING, Any, Dict, List
+import jedi
+from pylsp import hookimpl
+from pylsp import uris
+from pylsp import _utils
+from pylsp.config.config import Config
+from pylsp.workspace import Document
+from jedi.api import Script
+from jedi.api.classes import Name
+import threading
+import uuid
+from typing import Any, Dict, List
+from pylsp_jsonrpc.dispatchers import MethodDispatcher
+from pylsp_jsonrpc.endpoint import Endpoint
+from pylsp_jsonrpc.streams import JsonRpcStreamReader, JsonRpcStreamWriter
+from . import _utils, lsp, uris
+from ._version import __version__
+from .config import config
+from .workspace import Cell, Document, Notebook, Workspace
+from io import StringIO
+import pytest
+from pylsp_jsonrpc.exceptions import JsonRpcException
+import os.path
+from pathlib import PurePath
+import sys
+from subprocess import PIPE, Popen
+import re
+from pylsp import lsp
+import parso
+import parso.python.tree as tree_nodes
+from pylsp import hookspec
+from pylsp.plugins._resolvers import LABEL_RESOLVER
+from pylsp.plugins._resolvers import SNIPPET_RESOLVER
+import os
+import ast
+import mccabe
+import pydocstyle
+import contextlib
+from pyflakes import api as pyflakes_api
+from pyflakes import messages
+import collections
+import shlex
+import json
+from typing import Any, Dict, Generator, List, Optional, Set, Union
+from pylsp.workspace import Workspace
+from parso.python import tree
+from parso.tree import NodeOrLeaf
+from rope.contrib.autoimport.defs import SearchResult
+from rope.contrib.autoimport.sqlite import AutoImport
+from jedi import Script
+from rope.base.resources import Resource
+from ._rope_task_handle import PylspTaskHandle
+from rope.contrib.codeassist import code_assist, sorted_proposals
+from pathlib import Path
+from pylsp.lsp import SymbolKind
+import whatthepatch
+from yapf.yapflib.yapf_api import FormatCode
+from yapf.yapflib import file_resources, style
+
+
+log = logging.getLogger(__name__)
+
+def _autopep8_config(config, document=None):
+    # We user pycodestyle settings to avoid redefining things
+    path = document.path if document is not None else None
+    settings = config.plugin_settings("pycodestyle", document_path=path)
+    options = {
+        "exclude": settings.get("exclude"),
+        "hang_closing": settings.get("hangClosing"),
+        "ignore": settings.get("ignore"),
+        "max_line_length": settings.get("maxLineLength"),
+        "select": settings.get("select"),
+        "aggressive": settings.get("aggressive"),
+    }
+
+    # Filter out null options
+    return {k: v for k, v in options.items() if v}
+
+def _format(config, document, line_range=None):
+    options = _autopep8_config(config, document)
+    if line_range:
+        options["line_range"] = list(line_range)
+
+    # Temporarily re-monkey-patch the continued_indentation checker - #771
+    del pycodestyle._checks["logical_line"][pycodestyle.continued_indentation]
+    pycodestyle.register_check(autopep8_c_i)
+
+    # Autopep8 doesn't work with CR line endings, so we replace them by '\n'
+    # and restore them below.
+    replace_cr = False
+    source = document.source
+    eol_chars = get_eol_chars(source)
+    if eol_chars == "\r":
+        replace_cr = True
+        source = source.replace("\r", "\n")
+
+    new_source = fix_code(source, options=options)
+
+    # Switch it back
+    del pycodestyle._checks["logical_line"][autopep8_c_i]
+    pycodestyle.register_check(pycodestyle.continued_indentation)
+
+    if new_source == source:
+        return []
+
+    if replace_cr:
+        new_source = new_source.replace("\n", "\r")
+
+    # I'm too lazy at the moment to parse diffs into TextEdit items
+    # So let's just return the entire file...
+    return [
+        {
+            "range": {
+                "start": {"line": 0, "character": 0},
+                # End char 0 of the line after our document
+                "end": {"line": len(document.lines), "character": 0},
+            },
+            "newText": new_source,
+        }
+    ]
+
+@hookimpl(tryfirst=True)  # Prefer autopep8 over YAPF
+def pylsp_format_document(config, workspace, document, options):
+    with workspace.report_progress("format: autopep8"):
+        log.info("Formatting document %s with autopep8", document)
+        return _format(config, document)
+
+@hookimpl(tryfirst=True)  # Prefer autopep8 over YAPF
+def pylsp_format_range(config, workspace, document, range, options):
+    log.info("Formatting document %s in range %s with autopep8", document, range)
+
+    # First we 'round' the range up/down to full lines only
+    range["start"]["character"] = 0
+    range["end"]["line"] += 1
+    range["end"]["character"] = 0
+
+    # Add 1 for 1-indexing vs LSP's 0-indexing
+    line_range = (range["start"]["line"] + 1, range["end"]["line"])
+    return _format(config, document, line_range=line_range)
+
+MAX_JEDI_GOTO_HOPS = 100
+
+def _resolve_definition(
+    maybe_defn: Name, script: Script, settings: Dict[str, Any]
+) -> Name:
+    for _ in range(MAX_JEDI_GOTO_HOPS):
+        if maybe_defn.is_definition() or maybe_defn.module_path != script.path:
+            break
+        defns = script.goto(
+            follow_imports=settings.get("follow_imports", True),
+            follow_builtin_imports=settings.get("follow_builtin_imports", True),
+            line=maybe_defn.line,
+            column=maybe_defn.column,
+        )
+        if len(defns) == 1:
+            maybe_defn = defns[0]
+        else:
+            break
+    return maybe_defn
+
+def _not_internal_definition(definition: Name) -> bool:
+    return (
+        definition.line is not None
+        and definition.column is not None
+        and definition.module_path is not None
+        and not definition.in_builtin_module()
+    )
+
+@hookimpl
+def pylsp_definitions(
+    config: Config, document: Document, position: Dict[str, int]
+) -> List[Dict[str, Any]]:
+    settings = config.plugin_settings("jedi_definition")
+    code_position = _utils.position_to_jedi_linecolumn(document, position)
+    script = document.jedi_script(use_document_path=True)
+    auto_import_modules = jedi.settings.auto_import_modules
+
+    try:
+        jedi.settings.auto_import_modules = []
+        definitions = script.goto(
+            follow_imports=settings.get("follow_imports", True),
+            follow_builtin_imports=settings.get("follow_builtin_imports", True),
+            **code_position,
+        )
+        definitions = [_resolve_definition(d, script, settings) for d in definitions]
+    finally:
+        jedi.settings.auto_import_modules = auto_import_modules
+
+    follow_builtin_defns = settings.get("follow_builtin_definitions", True)
+    return [
+        {
+            "uri": uris.uri_with(document.uri, path=str(d.module_path)),
+            "range": {
+                "start": {"line": d.line - 1, "character": d.column},
+                "end": {"line": d.line - 1, "character": d.column + len(d.name)},
+            },
+        }
+        for d in definitions
+        if d.is_definition() and (follow_builtin_defns or _not_internal_definition(d))
+    ]
+
+LINT_DEBOUNCE_S = 0.5
+# 500 ms
+
+# 500 ms
+PARENT_PROCESS_WATCH_INTERVAL = 10
+# 10 s
+
+# 10 s
+MAX_WORKERS = 64
+
+PYTHON_FILE_EXTENSIONS = (".py", ".pyi")
+
+CONFIG_FILEs = ("pycodestyle.cfg", "setup.cfg", "tox.ini", ".flake8")
+
+def flatten(list_of_lists):
+    return [item for lst in list_of_lists for item in lst]
+
+def merge(list_of_dicts):
+    return {k: v for dictionary in list_of_dicts for k, v in dictionary.items()}
+
+class PythonLSPServer(MethodDispatcher):
+    """Implementation of the Microsoft VSCode Language Server Protocol
+    https://github.com/Microsoft/language-server-protocol/blob/master/versions/protocol-1-x.md
+    """
+
+    def __init__(
+        self, rx, tx, check_parent_process=False, consumer=None, *, endpoint_cls=None
+    ) -> None:
+        self.workspace = None
+        self.config = None
+        self.root_uri = None
+        self.watching_thread = None
+        self.workspaces = {}
+        self.uri_workspace_mapper = {}
+
+        self._check_parent_process = check_parent_process
+
+        if rx is not None:
+            self._jsonrpc_stream_reader = JsonRpcStreamReader(rx)
+        else:
+            self._jsonrpc_stream_reader = None
+
+        if tx is not None:
+            self._jsonrpc_stream_writer = JsonRpcStreamWriter(tx)
+        else:
+            self._jsonrpc_stream_writer = None
+
+        endpoint_cls = endpoint_cls or Endpoint
+
+        # if consumer is None, it is assumed that the default streams-based approach is being used
+        if consumer is None:
+            self._endpoint = endpoint_cls(
+                self, self._jsonrpc_stream_writer.write, max_workers=MAX_WORKERS
+            )
+        else:
+            self._endpoint = endpoint_cls(self, consumer, max_workers=MAX_WORKERS)
+
+        self._dispatchers = []
+        self._shutdown = False
+
+    def start(self) -> None:
+        """Entry point for the server."""
+        self._jsonrpc_stream_reader.listen(self._endpoint.consume)
+
+    def consume(self, message) -> None:
+        """Entry point for consumer based server. Alternative to stream listeners."""
+        # assuming message will be JSON
+        self._endpoint.consume(message)
+
+    def __getitem__(self, item):
+        """Override getitem to fallback through multiple dispatchers."""
+        if self._shutdown and item != "exit":
+            # exit is the only allowed method during shutdown
+            log.debug("Ignoring non-exit method during shutdown: %s", item)
+            item = "invalid_request_after_shutdown"
+
+        try:
+            return super().__getitem__(item)
+        except KeyError:
+            # Fallback through extra dispatchers
+            for dispatcher in self._dispatchers:
+                try:
+                    return dispatcher[item]
+                except KeyError:
+                    continue
+
+        raise KeyError()
+
+    def m_shutdown(self, **_kwargs) -> None:
+        for workspace in self.workspaces.values():
+            workspace.close()
+        self._shutdown = True
+
+    def m_invalid_request_after_shutdown(self, **_kwargs):
+        return {
+            "error": {
+                "code": lsp.ErrorCodes.InvalidRequest,
+                "message": "Requests after shutdown are not valid",
+            }
+        }
+
+    def m_exit(self, **_kwargs) -> None:
+        self._endpoint.shutdown()
+        if self._jsonrpc_stream_reader is not None:
+            self._jsonrpc_stream_reader.close()
+        if self._jsonrpc_stream_writer is not None:
+            self._jsonrpc_stream_writer.close()
+
+    def _match_uri_to_workspace(self, uri):
+        workspace_uri = _utils.match_uri_to_workspace(uri, self.workspaces)
+        return self.workspaces.get(workspace_uri, self.workspace)
+
+    def _hook(self, hook_name, doc_uri=None, **kwargs):
+        """Calls hook_name and returns a list of results from all registered handlers"""
+        workspace = self._match_uri_to_workspace(doc_uri)
+        doc = workspace.get_document(doc_uri) if doc_uri else None
+        hook_handlers = self.config.plugin_manager.subset_hook_caller(
+            hook_name, self.config.disabled_plugins
+        )
+        return hook_handlers(
+            config=self.config, workspace=workspace, document=doc, **kwargs
+        )
+
+    def capabilities(self):
+        server_capabilities = {
+            "codeActionProvider": True,
+            "codeLensProvider": {
+                "resolveProvider": False,  # We may need to make this configurable
+            },
+            "completionProvider": {
+                "resolveProvider": True,  # We could know everything ahead of time, but this takes time to transfer
+                "triggerCharacters": ["."],
+            },
+            "documentFormattingProvider": True,
+            "documentHighlightProvider": True,
+            "documentRangeFormattingProvider": True,
+            "documentSymbolProvider": True,
+            "definitionProvider": True,
+            "executeCommandProvider": {
+                "commands": flatten(self._hook("pylsp_commands"))
+            },
+            "hoverProvider": True,
+            "referencesProvider": True,
+            "renameProvider": True,
+            "foldingRangeProvider": True,
+            "signatureHelpProvider": {"triggerCharacters": ["(", ",", "="]},
+            "textDocumentSync": {
+                "change": lsp.TextDocumentSyncKind.INCREMENTAL,
+                "save": {
+                    "includeText": True,
+                },
+                "openClose": True,
+            },
+            "notebookDocumentSync": {
+                "notebookSelector": [{"cells": [{"language": "python"}]}]
+            },
+            "workspace": {
+                "workspaceFolders": {"supported": True, "changeNotifications": True}
+            },
+            "experimental": merge(self._hook("pylsp_experimental_capabilities")),
+        }
+        log.info("Server capabilities: %s", server_capabilities)
+        return server_capabilities
+
+    def m_initialize(
+        self,
+        processId=None,
+        rootUri=None,
+        rootPath=None,
+        initializationOptions=None,
+        workspaceFolders=None,
+        **_kwargs,
+    ):
+        log.debug(
+            "Language server initialized with %s %s %s %s",
+            processId,
+            rootUri,
+            rootPath,
+            initializationOptions,
+        )
+        if rootUri is None:
+            rootUri = uris.from_fs_path(rootPath) if rootPath is not None else ""
+
+        self.workspaces.pop(self.root_uri, None)
+        self.root_uri = rootUri
+        self.config = config.Config(
+            rootUri,
+            initializationOptions or {},
+            processId,
+            _kwargs.get("capabilities", {}),
+        )
+        self.workspace = Workspace(rootUri, self._endpoint, self.config)
+        self.workspaces[rootUri] = self.workspace
+        if workspaceFolders:
+            for folder in workspaceFolders:
+                uri = folder["uri"]
+                if uri == rootUri:
+                    # Already created
+                    continue
+                workspace_config = config.Config(
+                    uri,
+                    self.config._init_opts,
+                    self.config._process_id,
+                    self.config._capabilities,
+                )
+                workspace_config.update(self.config._settings)
+                self.workspaces[uri] = Workspace(uri, self._endpoint, workspace_config)
+
+        self._dispatchers = self._hook("pylsp_dispatchers")
+        self._hook("pylsp_initialize")
+
+        if (
+            self._check_parent_process
+            and processId is not None
+            and self.watching_thread is None
+        ):
+
+            def watch_parent_process(pid):
+                # exit when the given pid is not alive
+                if not _utils.is_process_alive(pid):
+                    log.info("parent process %s is not alive, exiting!", pid)
+                    self.m_exit()
+                else:
+                    threading.Timer(
+                        PARENT_PROCESS_WATCH_INTERVAL, watch_parent_process, args=[pid]
+                    ).start()
+
+            self.watching_thread = threading.Thread(
+                target=watch_parent_process, args=(processId,)
+            )
+            self.watching_thread.daemon = True
+            self.watching_thread.start()
+        # Get our capabilities
+        return {
+            "capabilities": self.capabilities(),
+            "serverInfo": {
+                "name": "pylsp",
+                "version": __version__,
+            },
+        }
+
+    def m_initialized(self, **_kwargs) -> None:
+        self._hook("pylsp_initialized")
+
+    def code_actions(self, doc_uri: str, range: Dict, context: Dict):
+        return flatten(
+            self._hook("pylsp_code_actions", doc_uri, range=range, context=context)
+        )
+
+    def code_lens(self, doc_uri):
+        return flatten(self._hook("pylsp_code_lens", doc_uri))
+
+    def completions(self, doc_uri, position):
+        workspace = self._match_uri_to_workspace(doc_uri)
+        document = workspace.get_document(doc_uri)
+        ignored_names = None
+        if isinstance(document, Cell):
+            # We need to get the ignored names from the whole notebook document
+            notebook_document = workspace.get_maybe_document(document.notebook_uri)
+            ignored_names = notebook_document.jedi_names(doc_uri)
+        completions = self._hook(
+            "pylsp_completions", doc_uri, position=position, ignored_names=ignored_names
+        )
+        return {"isIncomplete": False, "items": flatten(completions)}
+
+    def completion_item_resolve(self, completion_item):
+        doc_uri = completion_item.get("data", {}).get("doc_uri", None)
+        return self._hook(
+            "pylsp_completion_item_resolve", doc_uri, completion_item=completion_item
+        )
+
+    def definitions(self, doc_uri, position):
+        return flatten(self._hook("pylsp_definitions", doc_uri, position=position))
+
+    def document_symbols(self, doc_uri):
+        return flatten(self._hook("pylsp_document_symbols", doc_uri))
+
+    def document_did_save(self, doc_uri):
+        return self._hook("pylsp_document_did_save", doc_uri)
+
+    def execute_command(self, command, arguments):
+        return self._hook("pylsp_execute_command", command=command, arguments=arguments)
+
+    def format_document(self, doc_uri, options):
+        return lambda: self._hook("pylsp_format_document", doc_uri, options=options)
+
+    def format_range(self, doc_uri, range, options):
+        return self._hook("pylsp_format_range", doc_uri, range=range, options=options)
+
+    def highlight(self, doc_uri, position):
+        return (
+            flatten(self._hook("pylsp_document_highlight", doc_uri, position=position))
+            or None
+        )
+
+    def hover(self, doc_uri, position):
+        return self._hook("pylsp_hover", doc_uri, position=position) or {"contents": ""}
+
+    @_utils.debounce(LINT_DEBOUNCE_S, keyed_by="doc_uri")
+    def lint(self, doc_uri, is_saved) -> None:
+        # Since we're debounced, the document may no longer be open
+        workspace = self._match_uri_to_workspace(doc_uri)
+        document_object = workspace.documents.get(doc_uri, None)
+        if isinstance(document_object, Document):
+            self._lint_text_document(
+                doc_uri, workspace, is_saved, document_object.version
+            )
+        elif isinstance(document_object, Notebook):
+            self._lint_notebook_document(document_object, workspace)
+
+    def _lint_text_document(
+        self, doc_uri, workspace, is_saved, doc_version=None
+    ) -> None:
+        workspace.publish_diagnostics(
+            doc_uri,
+            flatten(self._hook("pylsp_lint", doc_uri, is_saved=is_saved)),
+            doc_version,
+        )
+
+    def _lint_notebook_document(self, notebook_document, workspace) -> None:
+        """
+        Lint a notebook document.
+
+        This is a bit more complicated than linting a text document, because we need to
+        send the entire notebook document to the pylsp_lint hook, but we need to send
+        the diagnostics back to the client on a per-cell basis.
+        """
+
+        # First, we create a temp TextDocument that represents the whole notebook
+        # contents. We'll use this to send to the pylsp_lint hook.
+        random_uri = str(uuid.uuid4())
+
+        # cell_list helps us map the diagnostics back to the correct cell later.
+        cell_list: List[Dict[str, Any]] = []
+
+        offset = 0
+        total_source = ""
+        for cell in notebook_document.cells:
+            cell_uri = cell["document"]
+            cell_document = workspace.get_cell_document(cell_uri)
+
+            num_lines = cell_document.line_count
+
+            data = {
+                "uri": cell_uri,
+                "line_start": offset,
+                "line_end": offset + num_lines - 1,
+                "source": cell_document.source,
+            }
+
+            cell_list.append(data)
+            if offset == 0:
+                total_source = cell_document.source
+            else:
+                total_source += "\n" + cell_document.source
+
+            offset += num_lines
+
+        workspace.put_document(random_uri, total_source)
+
+        try:
+            document_diagnostics = flatten(
+                self._hook("pylsp_lint", random_uri, is_saved=True)
+            )
+
+            # Now we need to map the diagnostics back to the correct cell and publish them.
+            # Note: this is O(n*m) in the number of cells and diagnostics, respectively.
+            for cell in cell_list:
+                cell_diagnostics = []
+                for diagnostic in document_diagnostics:
+                    start_line = diagnostic["range"]["start"]["line"]
+                    end_line = diagnostic["range"]["end"]["line"]
+
+                    if start_line > cell["line_end"] or end_line < cell["line_start"]:
+                        continue
+                    diagnostic["range"]["start"]["line"] = (
+                        start_line - cell["line_start"]
+                    )
+                    diagnostic["range"]["end"]["line"] = end_line - cell["line_start"]
+                    cell_diagnostics.append(diagnostic)
+
+                workspace.publish_diagnostics(cell["uri"], cell_diagnostics)
+        finally:
+            workspace.rm_document(random_uri)
+
+    def references(self, doc_uri, position, exclude_declaration):
+        return flatten(
+            self._hook(
+                "pylsp_references",
+                doc_uri,
+                position=position,
+                exclude_declaration=exclude_declaration,
+            )
+        )
+
+    def rename(self, doc_uri, position, new_name):
+        return self._hook("pylsp_rename", doc_uri, position=position, new_name=new_name)
+
+    def signature_help(self, doc_uri, position):
+        return self._hook("pylsp_signature_help", doc_uri, position=position)
+
+    def folding(self, doc_uri):
+        return flatten(self._hook("pylsp_folding_range", doc_uri))
+
+    def m_completion_item__resolve(self, **completionItem):
+        return self.completion_item_resolve(completionItem)
+
+    def m_notebook_document__did_open(
+        self, notebookDocument=None, cellTextDocuments=None, **_kwargs
+    ) -> None:
+        workspace = self._match_uri_to_workspace(notebookDocument["uri"])
+        workspace.put_notebook_document(
+            notebookDocument["uri"],
+            notebookDocument["notebookType"],
+            cells=notebookDocument["cells"],
+            version=notebookDocument.get("version"),
+            metadata=notebookDocument.get("metadata"),
+        )
+        for cell in cellTextDocuments or []:
+            workspace.put_cell_document(
+                cell["uri"],
+                notebookDocument["uri"],
+                cell["languageId"],
+                cell["text"],
+                version=cell.get("version"),
+            )
+        self.lint(notebookDocument["uri"], is_saved=True)
+
+    def m_notebook_document__did_close(
+        self, notebookDocument=None, cellTextDocuments=None, **_kwargs
+    ) -> None:
+        workspace = self._match_uri_to_workspace(notebookDocument["uri"])
+        for cell in cellTextDocuments or []:
+            workspace.publish_diagnostics(cell["uri"], [])
+            workspace.rm_document(cell["uri"])
+        workspace.rm_document(notebookDocument["uri"])
+
+    def m_notebook_document__did_change(
+        self, notebookDocument=None, change=None, **_kwargs
+    ) -> None:
+        """
+        Changes to the notebook document.
+
+        This could be one of the following:
+        1. Notebook metadata changed
+        2. Cell(s) added
+        3. Cell(s) deleted
+        4. Cell(s) data changed
+            4.1 Cell metadata changed
+            4.2 Cell source changed
+        """
+        workspace = self._match_uri_to_workspace(notebookDocument["uri"])
+
+        if change.get("metadata"):
+            # Case 1
+            workspace.update_notebook_metadata(
+                notebookDocument["uri"], change.get("metadata")
+            )
+
+        cells = change.get("cells")
+        if cells:
+            # Change to cells
+            structure = cells.get("structure")
+            if structure:
+                # Case 2 or 3
+                notebook_cell_array_change = structure["array"]
+                start = notebook_cell_array_change["start"]
+                cell_delete_count = notebook_cell_array_change["deleteCount"]
+                if cell_delete_count == 0:
+                    # Case 2
+                    # Cell documents
+                    for cell_document in structure["didOpen"]:
+                        workspace.put_cell_document(
+                            cell_document["uri"],
+                            notebookDocument["uri"],
+                            cell_document["languageId"],
+                            cell_document["text"],
+                            cell_document.get("version"),
+                        )
+                    # Cell metadata which is added to Notebook
+                    workspace.add_notebook_cells(
+                        notebookDocument["uri"],
+                        notebook_cell_array_change["cells"],
+                        start,
+                    )
+                else:
+                    # Case 3
+                    # Cell documents
+                    for cell_document in structure["didClose"]:
+                        workspace.rm_document(cell_document["uri"])
+                        workspace.publish_diagnostics(cell_document["uri"], [])
+                    # Cell metadata which is removed from Notebook
+                    workspace.remove_notebook_cells(
+                        notebookDocument["uri"], start, cell_delete_count
+                    )
+
+            data = cells.get("data")
+            if data:
+                # Case 4.1
+                for cell in data:
+                    # update NotebookDocument.cells properties
+                    pass
+
+            text_content = cells.get("textContent")
+            if text_content:
+                # Case 4.2
+                for cell in text_content:
+                    cell_uri = cell["document"]["uri"]
+                    # Even though the protocol says that `changes` is an array, we assume that it's always a single
+                    # element array that contains the last change to the cell source.
+                    workspace.update_document(cell_uri, cell["changes"][0])
+        self.lint(notebookDocument["uri"], is_saved=True)
+
+    def m_text_document__did_close(self, textDocument=None, **_kwargs) -> None:
+        workspace = self._match_uri_to_workspace(textDocument["uri"])
+        workspace.publish_diagnostics(textDocument["uri"], [])
+        workspace.rm_document(textDocument["uri"])
+
+    def m_text_document__did_open(self, textDocument=None, **_kwargs) -> None:
+        workspace = self._match_uri_to_workspace(textDocument["uri"])
+        workspace.put_document(
+            textDocument["uri"],
+            textDocument["text"],
+            version=textDocument.get("version"),
+        )
+        self._hook("pylsp_document_did_open", textDocument["uri"])
+        self.lint(textDocument["uri"], is_saved=True)
+
+    def m_text_document__did_change(
+        self, contentChanges=None, textDocument=None, **_kwargs
+    ) -> None:
+        workspace = self._match_uri_to_workspace(textDocument["uri"])
+        for change in contentChanges:
+            workspace.update_document(
+                textDocument["uri"], change, version=textDocument.get("version")
+            )
+        self.lint(textDocument["uri"], is_saved=False)
+
+    def m_text_document__did_save(self, textDocument=None, **_kwargs) -> None:
+        self.lint(textDocument["uri"], is_saved=True)
+        self.document_did_save(textDocument["uri"])
+
+    def m_text_document__code_action(
+        self, textDocument=None, range=None, context=None, **_kwargs
+    ):
+        return self.code_actions(textDocument["uri"], range, context)
+
+    def m_text_document__code_lens(self, textDocument=None, **_kwargs):
+        return self.code_lens(textDocument["uri"])
+
+    def _cell_document__completion(self, cellDocument, position=None, **_kwargs):
+        workspace = self._match_uri_to_workspace(cellDocument.notebook_uri)
+        notebookDocument = workspace.get_maybe_document(cellDocument.notebook_uri)
+        if notebookDocument is None:
+            raise ValueError("Invalid notebook document")
+
+        cell_data = notebookDocument.cell_data()
+
+        # Concatenate all cells to be a single temporary document
+        total_source = "\n".join(data["source"] for data in cell_data.values())
+        with workspace.temp_document(total_source) as temp_uri:
+            # update position to be the position in the temp document
+            if position is not None:
+                position["line"] += cell_data[cellDocument.uri]["line_start"]
+
+            completions = self.completions(temp_uri, position)
+
+            # Translate temp_uri locations to cell document locations
+            for item in completions.get("items", []):
+                if item.get("data", {}).get("doc_uri") == temp_uri:
+                    item["data"]["doc_uri"] = cellDocument.uri
+
+            return completions
+
+    def m_text_document__completion(self, textDocument=None, position=None, **_kwargs):
+        # textDocument here is just a dict with a uri
+        workspace = self._match_uri_to_workspace(textDocument["uri"])
+        document = workspace.get_document(textDocument["uri"])
+        if isinstance(document, Cell):
+            return self._cell_document__completion(document, position, **_kwargs)
+        return self.completions(textDocument["uri"], position)
+
+    def _cell_document__definition(self, cellDocument, position=None, **_kwargs):
+        workspace = self._match_uri_to_workspace(cellDocument.notebook_uri)
+        notebookDocument = workspace.get_maybe_document(cellDocument.notebook_uri)
+        if notebookDocument is None:
+            raise ValueError("Invalid notebook document")
+
+        cell_data = notebookDocument.cell_data()
+
+        # Concatenate all cells to be a single temporary document
+        total_source = "\n".join(data["source"] for data in cell_data.values())
+        with workspace.temp_document(total_source) as temp_uri:
+            # update position to be the position in the temp document
+            if position is not None:
+                position["line"] += cell_data[cellDocument.uri]["line_start"]
+
+            definitions = self.definitions(temp_uri, position)
+
+            # Translate temp_uri locations to cell document locations
+            for definition in definitions:
+                if definition["uri"] == temp_uri:
+                    # Find the cell the start line is in and adjust the uri and line numbers
+                    for cell_uri, data in cell_data.items():
+                        if (
+                            data["line_start"]
+                            <= definition["range"]["start"]["line"]
+                            <= data["line_end"]
+                        ):
+                            definition["uri"] = cell_uri
+                            definition["range"]["start"]["line"] -= data["line_start"]
+                            definition["range"]["end"]["line"] -= data["line_start"]
+                            break
+
+            return definitions
+
+    def m_text_document__definition(self, textDocument=None, position=None, **_kwargs):
+        # textDocument here is just a dict with a uri
+        workspace = self._match_uri_to_workspace(textDocument["uri"])
+        document = workspace.get_document(textDocument["uri"])
+        if isinstance(document, Cell):
+            return self._cell_document__definition(document, position, **_kwargs)
+        return self.definitions(textDocument["uri"], position)
+
+    def m_text_document__document_highlight(
+        self, textDocument=None, position=None, **_kwargs
+    ):
+        return self.highlight(textDocument["uri"], position)
+
+    def m_text_document__hover(self, textDocument=None, position=None, **_kwargs):
+        return self.hover(textDocument["uri"], position)
+
+    def m_text_document__document_symbol(self, textDocument=None, **_kwargs):
+        return self.document_symbols(textDocument["uri"])
+
+    def m_text_document__formatting(self, textDocument=None, options=None, **_kwargs):
+        return self.format_document(textDocument["uri"], options)
+
+    def m_text_document__rename(
+        self, textDocument=None, position=None, newName=None, **_kwargs
+    ):
+        return self.rename(textDocument["uri"], position, newName)
+
+    def m_text_document__folding_range(self, textDocument=None, **_kwargs):
+        return self.folding(textDocument["uri"])
+
+    def m_text_document__range_formatting(
+        self, textDocument=None, range=None, options=None, **_kwargs
+    ):
+        return self.format_range(textDocument["uri"], range, options)
+
+    def m_text_document__references(
+        self, textDocument=None, position=None, context=None, **_kwargs
+    ):
+        exclude_declaration = not context["includeDeclaration"]
+        return self.references(textDocument["uri"], position, exclude_declaration)
+
+    def m_text_document__signature_help(
+        self, textDocument=None, position=None, **_kwargs
+    ):
+        return self.signature_help(textDocument["uri"], position)
+
+    def m_workspace__did_change_configuration(self, settings=None) -> None:
+        if self.config is not None:
+            self.config.update((settings or {}).get("pylsp", {}))
+        for workspace in self.workspaces.values():
+            workspace.update_config(settings)
+            self._hook("pylsp_workspace_configuration_changed")
+            for doc_uri in workspace.documents:
+                self.lint(doc_uri, is_saved=False)
+
+    def m_workspace__did_change_workspace_folders(self, event=None, **_kwargs):
+        if event is None:
+            return
+        added = event.get("added", [])
+        removed = event.get("removed", [])
+
+        for removed_info in removed:
+            if "uri" in removed_info:
+                removed_uri = removed_info["uri"]
+                self.workspaces.pop(removed_uri, None)
+
+        for added_info in added:
+            if "uri" in added_info:
+                added_uri = added_info["uri"]
+                workspace_config = config.Config(
+                    added_uri,
+                    self.config._init_opts,
+                    self.config._process_id,
+                    self.config._capabilities,
+                )
+                workspace_config.update(self.config._settings)
+                self.workspaces[added_uri] = Workspace(
+                    added_uri, self._endpoint, workspace_config
+                )
+
+        root_workspace_removed = any(
+            removed_info["uri"] == self.root_uri for removed_info in removed
+        )
+        workspace_added = len(added) > 0 and "uri" in added[0]
+        if root_workspace_removed and workspace_added:
+            added_uri = added[0]["uri"]
+            self.root_uri = added_uri
+            new_root_workspace = self.workspaces[added_uri]
+            self.config = new_root_workspace._config
+            self.workspace = new_root_workspace
+        elif root_workspace_removed:
+            # NOTE: Removing the root workspace can only happen when the server
+            # is closed, thus the else condition of this if can never happen.
+            if self.workspaces:
+                log.debug("Root workspace deleted!")
+                available_workspaces = sorted(self.workspaces)
+                first_workspace = available_workspaces[0]
+                new_root_workspace = self.workspaces[first_workspace]
+                self.root_uri = first_workspace
+                self.config = new_root_workspace._config
+                self.workspace = new_root_workspace
+
+        # Migrate documents that are on the root workspace and have a better
+        # match now
+        doc_uris = list(self.workspace._docs.keys())
+        for uri in doc_uris:
+            doc = self.workspace._docs.pop(uri)
+            new_workspace = self._match_uri_to_workspace(uri)
+            new_workspace._docs[uri] = doc
+
+    def m_workspace__did_change_watched_files(self, changes=None, **_kwargs):
+        changed_py_files = set()
+        config_changed = False
+        for d in changes or []:
+            if d["uri"].endswith(PYTHON_FILE_EXTENSIONS):
+                changed_py_files.add(d["uri"])
+            elif d["uri"].endswith(CONFIG_FILEs):
+                config_changed = True
+
+        if config_changed:
+            self.config.settings.cache_clear()
+        elif not changed_py_files:
+            # Only externally changed python files and lint configs may result in changed diagnostics.
+            return
+
+        for workspace in self.workspaces.values():
+            for doc_uri in workspace.documents:
+                # Changes in doc_uri are already handled by m_text_document__did_save
+                if doc_uri not in changed_py_files:
+                    self.lint(doc_uri, is_saved=False)
+
+    def m_workspace__execute_command(self, command=None, arguments=None):
+        return self.execute_command(command, arguments)
+
+class FakeEditorMethodsMixin:
+    """
+    Represents the methods to be added to a dispatcher class when faking an editor.
+    """
+
+    def m_window__work_done_progress__create(self, *_args, **_kwargs):
+        """
+        Fake editor method `window/workDoneProgress/create`.
+
+        related spec:
+        https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#window_workDoneProgress_create
+        """
+        return None
+
+class FakePythonLSPServer(FakeEditorMethodsMixin, PythonLSPServer):
+    pass
+
+class FakeEndpoint(Endpoint):
+    """
+    Fake Endpoint representing the editor / LSP client.
+
+    The `dispatcher` dict will be used to synchronously calculate the responses
+    for calls to `.request` and resolve the futures with the value or errors.
+
+    Fake methods in the `dispatcher` should raise `JsonRpcException` for any
+    error.
+    """
+
+    def request(self, method, params=None):
+        request_future = super().request(method, params)
+        try:
+            request_future.set_result(self._dispatcher[method](params))
+        except JsonRpcException as e:
+            request_future.set_exception(e)
+
+        return request_future
+
+@pytest.fixture
+def pylsp_w_workspace_folders(tmpdir):
+    """Return an initialized python LS"""
+    ls = FakePythonLSPServer(StringIO, StringIO, endpoint_cls=FakeEndpoint)
+
+    folder1 = tmpdir.mkdir("folder1")
+    folder2 = tmpdir.mkdir("folder2")
+
+    ls.m_initialize(
+        processId=1,
+        rootUri=uris.from_fs_path(str(folder1)),
+        initializationOptions={},
+        workspaceFolders=[
+            {"uri": uris.from_fs_path(str(folder1)), "name": "folder1"},
+            {"uri": uris.from_fs_path(str(folder2)), "name": "folder2"},
+        ],
+    )
+
+    workspace_folders = [folder1, folder2]
+    return (ls, workspace_folders)
+
+@hookimpl
+def pylsp_settings():
+    # Default flake8 to disabled
+    return {"plugins": {"flake8": {"enabled": False}}}
+
+FIX_IGNORES_RE = re.compile(r"([^a-zA-Z0-9_,]*;.*(\W+||$))")
+
+def run_flake8(flake8_executable, args, document, source):
+    """Run flake8 with the provided arguments, logs errors
+    from stderr if any.
+    """
+    # a quick temporary fix to deal with Atom
+    args = [
+        (i if not i.startswith("--ignore=") else FIX_IGNORES_RE.sub("", i))
+        for i in args
+        if i is not None
+    ]
+
+    if document.path and document.path.startswith(document._workspace.root_path):
+        args.extend(
+            [
+                "--stdin-display-name",
+                os.path.relpath(document.path, document._workspace.root_path),
+            ]
+        )
+
+    # if executable looks like a path resolve it
+    if not os.path.isfile(flake8_executable) and os.sep in flake8_executable:
+        flake8_executable = os.path.abspath(
+            os.path.expanduser(os.path.expandvars(flake8_executable))
+        )
+
+    log.debug("Calling %s with args: '%s'", flake8_executable, args)
+    popen_kwargs = {}
+    if cwd := document._workspace.root_path:
+        popen_kwargs["cwd"] = cwd
+    try:
+        cmd = [flake8_executable]
+        cmd.extend(args)
+        p = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE, **popen_kwargs)
+    except IOError:
+        log.debug(
+            "Can't execute %s. Trying with '%s -m flake8'",
+            flake8_executable,
+            sys.executable,
+        )
+        cmd = [sys.executable, "-m", "flake8"]
+        cmd.extend(args)
+        p = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE, **popen_kwargs)
+    (stdout, stderr) = p.communicate(source.encode())
+    if stderr:
+        log.error("Error while running flake8 '%s'", stderr.decode())
+    return stdout.decode()
+
+def build_args(options):
+    """Build arguments for calling flake8.
+
+    Args:
+        options: dictionary of argument names and their values.
+    """
+    args = ["-"]  # use stdin
+    for arg_name, arg_val in options.items():
+        if arg_val is None:
+            continue
+        arg = None
+        if isinstance(arg_val, list):
+            arg = "--{}={}".format(arg_name, ",".join(arg_val))
+        elif isinstance(arg_val, bool):
+            if arg_val:
+                arg = "--{}".format(arg_name)
+        else:
+            arg = "--{}={}".format(arg_name, arg_val)
+        args.append(arg)
+    return args
+
+UNNECESSITY_CODES = {
+    "F401",  # `module` imported but unused
+    "F504",  # % format unused named arguments
+    "F522",  # .format(...) unused named arguments
+    "F523",  # .format(...) unused positional arguments
+    "F841",  # local variable `name` is assigned to but never used
+}
+
+# NOTE: If the user sets the flake8 executable with workspace configuration, the
+# error codes in this set may be inaccurate.
+ERROR_CODES = (
+    # Errors from the pyflakes plugin of flake8
+    {FLAKE8_PYFLAKES_CODES.get(m.__name__, "E999") for m in PYFLAKES_ERROR_MESSAGES}
+    # Syntax error from flake8 itself
+    | {"E999"}
+)
+
+def parse_stdout(source, stdout):
+    """
+    Build a diagnostics from flake8's output, it should extract every result and format
+    it into a dict that looks like this:
+        {
+            'source': 'flake8',
+            'code': code, # 'E501'
+            'range': {
+                'start': {
+                    'line': start_line,
+                    'character': start_column,
+                },
+                'end': {
+                    'line': end_line,
+                    'character': end_column,
+                },
+            },
+            'message': msg,
+            'severity': lsp.DiagnosticSeverity.*,
+        }
+
+    Args:
+        document: The document to be linted.
+        stdout: output from flake8
+    Returns:
+        A list of dictionaries.
+    """
+
+    document_lines = source.splitlines(True)
+    diagnostics = []
+    lines = stdout.splitlines()
+    for raw_line in lines:
+        parsed_line = re.match(r"(.*):(\d*):(\d*): (\w*) (.*)", raw_line)
+        if not parsed_line:
+            log.debug("Flake8 output parser can't parse line '%s'", raw_line)
+            continue
+
+        parsed_line = parsed_line.groups()
+        if len(parsed_line) != 5:
+            log.debug("Flake8 output parser can't parse line '%s'", raw_line)
+            continue
+
+        _, line, character, code, msg = parsed_line
+        line = int(line) - 1
+        character = int(character) - 1
+        # show also the code in message
+        msg = code + " " + msg
+        severity = lsp.DiagnosticSeverity.Warning
+        if code in ERROR_CODES:
+            severity = lsp.DiagnosticSeverity.Error
+        diagnostic = {
+            "source": "flake8",
+            "code": code,
+            "range": {
+                "start": {"line": line, "character": character},
+                "end": {
+                    "line": line,
+                    # no way to determine the column
+                    "character": len(document_lines[line]),
+                },
+            },
+            "message": msg,
+            "severity": severity,
+        }
+        if code in UNNECESSITY_CODES:
+            diagnostic["tags"] = [lsp.DiagnosticTag.Unnecessary]
+        diagnostics.append(diagnostic)
+
+    return diagnostics
+
+@hookimpl
+def pylsp_lint(workspace, document):
+    with workspace.report_progress("lint: flake8"):
+        config = workspace._config
+        settings = config.plugin_settings("flake8", document_path=document.path)
+        log.debug("Got flake8 settings: %s", settings)
+
+        ignores = settings.get("ignore", [])
+        per_file_ignores = settings.get("perFileIgnores")
+
+        if per_file_ignores:
+            prev_file_pat = None
+            for path in per_file_ignores:
+                try:
+                    file_pat, errors = path.split(":")
+                    prev_file_pat = file_pat
+                except ValueError:
+                    # It's legal to just specify another error type for the same
+                    # file pattern:
+                    if prev_file_pat is None:
+                        log.warning("skipping a Per-file-ignore with no file pattern")
+                        continue
+                    file_pat = prev_file_pat
+                    errors = path
+                if PurePath(document.path).match(file_pat):
+                    ignores.extend(errors.split(","))
+
+        opts = {
+            "config": settings.get("config"),
+            "exclude": settings.get("exclude"),
+            "extend-ignore": settings.get("extendIgnore"),
+            "extend-select": settings.get("extendSelect"),
+            "filename": settings.get("filename"),
+            "hang-closing": settings.get("hangClosing"),
+            "ignore": ignores or None,
+            "max-complexity": settings.get("maxComplexity"),
+            "max-line-length": settings.get("maxLineLength"),
+            "indent-size": settings.get("indentSize"),
+            "select": settings.get("select"),
+        }
+
+        # flake takes only absolute path to the config. So we should check and
+        # convert if necessary
+        if opts.get("config") and not os.path.isabs(opts.get("config")):
+            opts["config"] = os.path.abspath(
+                os.path.expanduser(os.path.expandvars(opts.get("config")))
+            )
+            log.debug("using flake8 with config: %s", opts["config"])
+
+        # Call the flake8 utility then parse diagnostics from stdout
+        flake8_executable = settings.get("executable", "flake8")
+
+        args = build_args(opts)
+
+        # ensure the same source is used for flake8 execution and result parsing;
+        # single source access improves performance as it is only one disk access
+        source = document.source
+        output = run_flake8(flake8_executable, args, document, source)
+        return parse_stdout(source, output)
+
+SKIP_NODES = (tree_nodes.Module, tree_nodes.IfStmt, tree_nodes.TryStmt)
+
+def __merge_folding_ranges(left, right):
+    for start in list(left.keys()):
+        right_start = right.pop(start, None)
+        if right_start is not None:
+            left[start] = max(right_start, start)
+    left.update(right)
+    return left
+
+IDENTATION_REGEX = re.compile(r"(\s+).+")
+
+def __empty_identation_stack(
+    identation_stack, level_limits, current_line, folding_ranges
+):
+    while identation_stack != []:
+        upper_level = identation_stack.pop(0)
+        level_start = level_limits.pop(upper_level)
+        folding_ranges.append((level_start, current_line))
+    return folding_ranges
+
+def __match_identation_stack(
+    identation_stack, level, level_limits, folding_ranges, current_line
+):
+    upper_level = identation_stack.pop(0)
+    while upper_level >= level:
+        level_start = level_limits.pop(upper_level)
+        folding_ranges.append((level_start, current_line))
+        upper_level = identation_stack.pop(0)
+    identation_stack.insert(0, upper_level)
+    return identation_stack, folding_ranges
+
+def __compute_folding_ranges_identation(text):
+    lines = text.splitlines()
+    folding_ranges = []
+    identation_stack = []
+    level_limits = {}
+    current_level = 0
+    current_line = 0
+    while lines[current_line] == "":
+        current_line += 1
+    for i, line in enumerate(lines):
+        if i < current_line:
+            continue
+        i += 1
+        identation_match = IDENTATION_REGEX.match(line)
+        if identation_match is not None:
+            whitespace = identation_match.group(1)
+            level = len(whitespace)
+            if level > current_level:
+                level_limits[current_level] = current_line
+                identation_stack.insert(0, current_level)
+                current_level = level
+            elif level < current_level:
+                identation_stack, folding_ranges = __match_identation_stack(
+                    identation_stack, level, level_limits, folding_ranges, current_line
+                )
+                current_level = level
+        else:
+            folding_ranges = __empty_identation_stack(
+                identation_stack, level_limits, current_line, folding_ranges
+            )
+            current_level = 0
+        if line.strip() != "":
+            current_line = i
+    folding_ranges = __empty_identation_stack(
+        identation_stack, level_limits, current_line, folding_ranges
+    )
+    return dict(folding_ranges)
+
+def __check_if_node_is_valid(node):
+    valid = True
+    if isinstance(node, tree_nodes.PythonNode):
+        kind = node.type
+        valid = kind not in {
+            "decorated",
+            "parameters",
+            "dictorsetmaker",
+            "testlist_comp",
+        }
+        if kind == "suite":
+            if isinstance(node.parent, tree_nodes.Function):
+                valid = False
+    return valid
+
+def __handle_skip(stack, skip):
+    body = stack[skip]
+    children = [body]
+    if hasattr(body, "children"):
+        children = body.children
+    stack = stack[:skip] + children + stack[skip + 1 :]
+    node = body
+    end_line, _ = body.end_pos
+    return node, end_line
+
+def __handle_flow_nodes(node, end_line, stack):
+    from_keyword = False
+    if isinstance(node, tree_nodes.Keyword):
+        from_keyword = True
+        if node.value in {"if", "elif", "with", "while"}:
+            node, end_line = __handle_skip(stack, 2)
+        elif node.value in {"except"}:
+            first_node = stack[0]
+            if isinstance(first_node, tree_nodes.Operator):
+                node, end_line = __handle_skip(stack, 1)
+            else:
+                node, end_line = __handle_skip(stack, 2)
+        elif node.value in {"for"}:
+            node, end_line = __handle_skip(stack, 4)
+        elif node.value in {"else"}:
+            node, end_line = __handle_skip(stack, 1)
+    return end_line, from_keyword, node, stack
+
+def __compute_start_end_lines(node, stack):
+    start_line, _ = node.start_pos
+    end_line, _ = node.end_pos
+    modified = False
+    end_line, from_keyword, node, stack = __handle_flow_nodes(node, end_line, stack)
+
+    last_leaf = node.get_last_leaf()
+    last_newline = isinstance(last_leaf, tree_nodes.Newline)
+    last_operator = isinstance(last_leaf, tree_nodes.Operator)
+    node_is_operator = isinstance(node, tree_nodes.Operator)
+    last_operator = last_operator or not node_is_operator
+
+    end_line -= 1
+
+    if isinstance(node.parent, tree_nodes.PythonNode) and not from_keyword:
+        kind = node.type
+        if kind in {"suite", "atom", "atom_expr", "arglist"}:
+            if len(stack) > 0:
+                next_node = stack[0]
+                next_line, _ = next_node.start_pos
+                if next_line > end_line:
+                    end_line += 1
+                    modified = True
+    if not last_newline and not modified and not last_operator:
+        end_line += 1
+    return start_line, end_line, stack
+
+def __compute_folding_ranges(tree, lines):
+    folding_ranges = {}
+    stack = [tree]
+
+    while len(stack) > 0:
+        node = stack.pop(0)
+        if isinstance(node, tree_nodes.Newline):
+            # Skip newline nodes
+            continue
+        if isinstance(node, tree_nodes.PythonErrorNode):
+            # Fallback to indentation-based (best-effort) folding
+            start_line, _ = node.start_pos
+            start_line -= 1
+            padding = [""] * start_line
+            text = "\n".join(padding + lines[start_line:]) + "\n"
+            identation_ranges = __compute_folding_ranges_identation(text)
+            folding_ranges = __merge_folding_ranges(folding_ranges, identation_ranges)
+            break
+        if not isinstance(node, SKIP_NODES):
+            valid = __check_if_node_is_valid(node)
+            if valid:
+                start_line, end_line, stack = __compute_start_end_lines(node, stack)
+                if end_line > start_line:
+                    current_end = folding_ranges.get(start_line, -1)
+                    folding_ranges[start_line] = max(current_end, end_line)
+        if hasattr(node, "children"):
+            stack = node.children + stack
+
+    folding_ranges = sorted(folding_ranges.items())
+    return folding_ranges
+
+@hookimpl
+def pylsp_folding_range(document):
+    program = document.source + "\n"
+    lines = program.splitlines()
+    tree = parso.parse(program)
+    ranges = __compute_folding_ranges(tree, lines)
+
+    results = []
+    for start_line, end_line in ranges:
+        start_line -= 1
+        end_line -= 1
+        # If start/end character is not defined, then it defaults to the
+        # corresponding line last character
+        results.append(
+            {
+                "startLine": start_line,
+                "endLine": end_line,
+            }
+        )
+    return results
+
+@hookimpl
+def pylsp_document_highlight(document, position):
+    code_position = _utils.position_to_jedi_linecolumn(document, position)
+    usages = document.jedi_script().get_references(**code_position)
+
+    def is_valid(definition):
+        return definition.line is not None and definition.column is not None
+
+    def local_to_document(definition):
+        return (
+            not definition.module_path or str(definition.module_path) == document.path
+        )
+
+    return [
+        {
+            "range": {
+                "start": {"line": d.line - 1, "character": d.column},
+                "end": {"line": d.line - 1, "character": d.column + len(d.name)},
+            },
+            "kind": lsp.DocumentHighlightKind.Write
+            if d.is_definition()
+            else lsp.DocumentHighlightKind.Read,
+        }
+        for d in usages
+        if is_valid(d) and local_to_document(d)
+    ]
+
+@hookspec
+def pylsp_code_actions(config, workspace, document, range, context):
+    pass
+
+@hookspec
+def pylsp_code_lens(config, workspace, document) -> None:
+    pass
+
+@hookspec
+def pylsp_commands(config, workspace) -> None:
+    """The list of command strings supported by the server.
+
+    Returns:
+        List[str]: The supported commands.
+    """
+
+@hookspec
+def pylsp_completions(config, workspace, document, position, ignored_names) -> None:
+    pass
+
+@hookspec(firstresult=True)
+def pylsp_completion_item_resolve(config, workspace, document, completion_item) -> None:
+    pass
+
+@hookspec
+def pylsp_definitions(config, workspace, document, position) -> None:
+    pass
+
+@hookspec
+def pylsp_dispatchers(config, workspace) -> None:
+    pass
+
+@hookspec
+def pylsp_document_did_open(config, workspace, document) -> None:
+    pass
+
+@hookspec
+def pylsp_document_did_save(config, workspace, document) -> None:
+    pass
+
+@hookspec
+def pylsp_document_highlight(config, workspace, document, position) -> None:
+    pass
+
+@hookspec
+def pylsp_document_symbols(config, workspace, document) -> None:
+    pass
+
+@hookspec(firstresult=True)
+def pylsp_execute_command(config, workspace, command, arguments) -> None:
+    pass
+
+@hookspec
+def pylsp_experimental_capabilities(config, workspace) -> None:
+    pass
+
+@hookspec
+def pylsp_folding_range(config, workspace, document) -> None:
+    pass
+
+@hookspec(firstresult=True)
+def pylsp_format_document(config, workspace, document, options) -> None:
+    pass
+
+@hookspec(firstresult=True)
+def pylsp_format_range(config, workspace, document, range, options) -> None:
+    pass
+
+@hookspec(firstresult=True)
+def pylsp_hover(config, workspace, document, position) -> None:
+    pass
+
+@hookspec
+def pylsp_initialize(config, workspace) -> None:
+    pass
+
+@hookspec
+def pylsp_initialized() -> None:
+    pass
+
+@hookspec
+def pylsp_lint(config, workspace, document, is_saved) -> None:
+    pass
+
+@hookspec
+def pylsp_references(
+    config, workspace, document, position, exclude_declaration
+) -> None:
+    pass
+
+@hookspec(firstresult=True)
+def pylsp_rename(config, workspace, document, position, new_name) -> None:
+    pass
+
+@hookspec
+def pylsp_settings(config) -> None:
+    pass
+
+@hookspec(firstresult=True)
+def pylsp_signature_help(config, workspace, document, position) -> None:
+    pass
+
+@hookspec
+def pylsp_workspace_configuration_changed(config, workspace) -> None:
+    pass
+
+@hookimpl
+def pylsp_hover(config, document, position):
+    code_position = _utils.position_to_jedi_linecolumn(document, position)
+    definitions = document.jedi_script(use_document_path=True).infer(**code_position)
+    word = document.word_at_position(position)
+
+    # Find first exact matching definition
+    definition = next((x for x in definitions if x.name == word), None)
+
+    # Ensure a definition is used if only one is available
+    # even if the word doesn't match. An example of this case is 'np'
+    # where 'numpy' doesn't match with 'np'. Same for NumPy ufuncs
+    if len(definitions) == 1:
+        definition = definitions[0]
+
+    if not definition:
+        return {"contents": ""}
+
+    hover_capabilities = config.capabilities.get("textDocument", {}).get("hover", {})
+    supported_markup_kinds = hover_capabilities.get("contentFormat", ["markdown"])
+    preferred_markup_kind = _utils.choose_markup_kind(supported_markup_kinds)
+
+    # Find first exact matching signature
+    signature = next(
+        (
+            x.to_string()
+            for x in definition.get_signatures()
+            if (x.name == word and x.type not in ["module"])
+        ),
+        "",
+    )
+
+    return {
+        "contents": _utils.format_docstring(
+            # raw docstring returns only doc, without signature
+            definition.docstring(raw=True),
+            preferred_markup_kind,
+            signatures=[signature] if signature else None,
+        )
+    }
+
+# Types of parso nodes for which snippet is not included in the completion
+_IMPORTS = ("import_name", "import_from")
+
+# Types of parso node for errors
+_ERRORS = ("error_node",)
+
+def use_snippets(document, position):
+    """
+    Determine if it's necessary to return snippets in code completions.
+
+    This returns `False` if a completion is being requested on an import
+    statement, `True` otherwise.
+    """
+    line = position["line"]
+    lines = document.source.split("\n", line)
+    act_lines = [lines[line][: position["character"]]]
+    line -= 1
+    last_character = ""
+    while line > -1:
+        act_line = lines[line]
+        if (
+            act_line.rstrip().endswith("\\")
+            or act_line.rstrip().endswith("(")
+            or act_line.rstrip().endswith(",")
+        ):
+            act_lines.insert(0, act_line)
+            line -= 1
+            if act_line.rstrip().endswith("("):
+                # Needs to be added to the end of the code before parsing
+                # to make it valid, otherwise the node type could end
+                # being an 'error_node' for multi-line imports that use '('
+                last_character = ")"
+        else:
+            break
+    if "(" in act_lines[-1].strip():
+        last_character = ")"
+    code = "\n".join(act_lines).rsplit(";", maxsplit=1)[-1].strip() + last_character
+    tokens = parso.parse(code)
+    expr_type = tokens.children[0].type
+    return expr_type not in _IMPORTS and not (expr_type in _ERRORS and "import" in code)
+
+# Map to the LSP type
+# > Valid values for type are ``module``, `` class ``, ``instance``, ``function``,
+# > ``param``, ``path``, ``keyword``, ``property`` and ``statement``.
+# see: https://jedi.readthedocs.io/en/latest/docs/api-classes.html#jedi.api.classes.BaseName.type
+_TYPE_MAP = {
+    "module": lsp.CompletionItemKind.Module,
+    "namespace": lsp.CompletionItemKind.Module,  # to be added in Jedi 0.18+
+    "class": lsp.CompletionItemKind.Class,
+    "instance": lsp.CompletionItemKind.Reference,
+    "function": lsp.CompletionItemKind.Function,
+    "param": lsp.CompletionItemKind.Variable,
+    "path": lsp.CompletionItemKind.File,
+    "keyword": lsp.CompletionItemKind.Keyword,
+    "property": lsp.CompletionItemKind.Property,  # added in Jedi 0.18
+    "statement": lsp.CompletionItemKind.Variable,
+}
+
+def is_exception_class(name):
+    """
+    Determine if a class name is an instance of an Exception.
+
+    This returns `False` if the name given corresponds with a instance of
+    the 'Exception' class, `True` otherwise
+    """
+    try:
+        return name in [cls.__name__ for cls in Exception.__subclasses__()]
+    except AttributeError:
+        # Needed in case a class don't uses new-style
+        # class definition in Python 2
+        return False
+
+def _detail(definition):
+    try:
+        return definition.parent().full_name or ""
+    except AttributeError:
+        return definition.full_name or ""
+
+def _resolve_completion(completion, d, markup_kind: str):
+    completion["detail"] = _detail(d)
+    try:
+        docs = _utils.format_docstring(
+            d.docstring(raw=True),
+            signatures=[signature.to_string() for signature in d.get_signatures()],
+            markup_kind=markup_kind,
+        )
+    except Exception:
+        docs = ""
+    completion["documentation"] = docs
+    return completion
+
+def _label(definition, resolve=False):
+    if not resolve:
+        return definition.name
+    sig = LABEL_RESOLVER.get_or_create(definition)
+    if sig:
+        return sig
+    return definition.name
+
+def _snippet(definition, resolve=False):
+    if not resolve:
+        return {}
+    snippet = SNIPPET_RESOLVER.get_or_create(definition)
+    return snippet
+
+def _sort_text(definition):
+    """Ensure builtins appear at the bottom.
+    Description is of format <type>: <module>.<item>
+    """
+
+    # If its 'hidden', put it next last
+    prefix = "z{}" if definition.name.startswith("_") else "a{}"
+    return prefix.format(definition.name)
+
+def _format_completion(
+    d,
+    markup_kind: str,
+    include_params=True,
+    resolve=False,
+    resolve_label_or_snippet=False,
+    snippet_support=False,
+):
+    completion = {
+        "label": _label(d, resolve_label_or_snippet),
+        "kind": _TYPE_MAP.get(d.type),
+        "sortText": _sort_text(d),
+        "insertText": d.name,
+    }
+
+    if resolve:
+        completion = _resolve_completion(completion, d, markup_kind)
+
+    # Adjustments for file completions
+    if d.type == "path":
+        path = os.path.normpath(d.name)
+
+        # If the completion ends with os.sep, it means it's a directory. So we add os.sep at the end
+        # to ease additional file completions.
+        if d.name.endswith(os.sep):
+            if os.name == "nt":
+                path = path + "\\"
+            else:
+                path = path + "/"
+
+        # Escape to prevent conflicts with the code snippets grammer
+        # See also https://github.com/python-lsp/python-lsp-server/issues/373
+        if snippet_support:
+            path = path.replace("\\", "\\\\")
+            path = path.replace("/", "\\/")
+
+        completion["insertText"] = path
+
+    if include_params and not is_exception_class(d.name):
+        snippet = _snippet(d, resolve_label_or_snippet)
+        completion.update(snippet)
+
+    return completion
+
+@hookimpl
+def pylsp_completions(config, document, position):
+    """Get formatted completions for current code position"""
+    settings = config.plugin_settings("jedi_completion", document_path=document.path)
+    resolve_eagerly = settings.get("eager", False)
+    code_position = _utils.position_to_jedi_linecolumn(document, position)
+
+    code_position["fuzzy"] = settings.get("fuzzy", False)
+    completions = document.jedi_script(use_document_path=True).complete(**code_position)
+
+    if not completions:
+        return None
+
+    completion_capabilities = config.capabilities.get("textDocument", {}).get(
+        "completion", {}
+    )
+    item_capabilities = completion_capabilities.get("completionItem", {})
+    snippet_support = item_capabilities.get("snippetSupport")
+    supported_markup_kinds = item_capabilities.get("documentationFormat", ["markdown"])
+    preferred_markup_kind = _utils.choose_markup_kind(supported_markup_kinds)
+
+    should_include_params = settings.get("include_params")
+    should_include_class_objects = settings.get("include_class_objects", False)
+    should_include_function_objects = settings.get("include_function_objects", False)
+
+    max_to_resolve = settings.get("resolve_at_most", 25)
+    modules_to_cache_for = settings.get("cache_for", None)
+    if modules_to_cache_for is not None:
+        LABEL_RESOLVER.cached_modules = modules_to_cache_for
+        SNIPPET_RESOLVER.cached_modules = modules_to_cache_for
+
+    include_params = (
+        snippet_support and should_include_params and use_snippets(document, position)
+    )
+    include_class_objects = (
+        snippet_support
+        and should_include_class_objects
+        and use_snippets(document, position)
+    )
+    include_function_objects = (
+        snippet_support
+        and should_include_function_objects
+        and use_snippets(document, position)
+    )
+
+    ready_completions = [
+        _format_completion(
+            c,
+            markup_kind=preferred_markup_kind,
+            include_params=include_params if c.type in ["class", "function"] else False,
+            resolve=resolve_eagerly,
+            resolve_label_or_snippet=(i < max_to_resolve),
+            snippet_support=snippet_support,
+        )
+        for i, c in enumerate(completions)
+    ]
+
+    # TODO split up once other improvements are merged
+    if include_class_objects:
+        for i, c in enumerate(completions):
+            if c.type == "class":
+                completion_dict = _format_completion(
+                    c,
+                    markup_kind=preferred_markup_kind,
+                    include_params=False,
+                    resolve=resolve_eagerly,
+                    resolve_label_or_snippet=(i < max_to_resolve),
+                    snippet_support=snippet_support,
+                )
+                completion_dict["kind"] = lsp.CompletionItemKind.TypeParameter
+                completion_dict["label"] += " object"
+                ready_completions.append(completion_dict)
+
+    if include_function_objects:
+        for i, c in enumerate(completions):
+            if c.type == "function":
+                completion_dict = _format_completion(
+                    c,
+                    markup_kind=preferred_markup_kind,
+                    include_params=False,
+                    resolve=resolve_eagerly,
+                    resolve_label_or_snippet=(i < max_to_resolve),
+                    snippet_support=snippet_support,
+                )
+                completion_dict["kind"] = lsp.CompletionItemKind.TypeParameter
+                completion_dict["label"] += " object"
+                ready_completions.append(completion_dict)
+
+    for completion_dict in ready_completions:
+        completion_dict["data"] = {"doc_uri": document.uri}
+
+    # most recently retrieved completion items, used for resolution
+    document.shared_data["LAST_JEDI_COMPLETIONS"] = {
+        # label is the only required property; here it is assumed to be unique
+        completion["label"]: (completion, data)
+        for completion, data in zip(ready_completions, completions)
+    }
+
+    return ready_completions or None
+
+@hookimpl
+def pylsp_completion_item_resolve(config, completion_item, document):
+    """Resolve formatted completion for given non-resolved completion"""
+    shared_data = document.shared_data["LAST_JEDI_COMPLETIONS"].get(
+        completion_item["label"]
+    )
+
+    completion_capabilities = config.capabilities.get("textDocument", {}).get(
+        "completion", {}
+    )
+    item_capabilities = completion_capabilities.get("completionItem", {})
+    supported_markup_kinds = item_capabilities.get("documentationFormat", ["markdown"])
+    preferred_markup_kind = _utils.choose_markup_kind(supported_markup_kinds)
+
+    if shared_data:
+        completion, data = shared_data
+        return _resolve_completion(completion, data, markup_kind=preferred_markup_kind)
+    return completion_item
+
+def _num_lines(file_contents):
+    "Count the number of lines in the given string."
+    if _utils.get_eol_chars(file_contents):
+        return len(file_contents.splitlines())
+    return 0
+
+@hookimpl
+def pylsp_rename(config, workspace, document, position, new_name):
+    log.debug(
+        "Executing rename of %s to %s", document.word_at_position(position), new_name
+    )
+    kwargs = _utils.position_to_jedi_linecolumn(document, position)
+    kwargs["new_name"] = new_name
+    try:
+        refactoring = document.jedi_script().rename(**kwargs)
+    except NotImplementedError as exc:
+        raise Exception(
+            "No support for renaming in Python 2/3.5 with Jedi. "
+            "Consider using the pylsp-rope plugin instead"
+        ) from exc
+    log.debug("Finished rename: %s", refactoring.get_diff())
+    changes = []
+
+    changed_files = refactoring.get_changed_files()
+    for file_path, changed_file in changed_files.items():
+        uri = uris.from_fs_path(str(file_path))
+        doc = workspace.get_maybe_document(uri)
+        changes.append(
+            {
+                "textDocument": {"uri": uri, "version": doc.version if doc else None},
+                "edits": [
+                    {
+                        "range": {
+                            "start": {"line": 0, "character": 0},
+                            "end": {
+                                "line": _num_lines(changed_file.get_new_code()),
+                                "character": 0,
+                            },
+                        },
+                        "newText": changed_file.get_new_code(),
+                    }
+                ],
+            }
+        )
+    return {"documentChanges": changes}
+
+THRESHOLD = "threshold"
+
+DEFAULT_THRESHOLD = 15
+
+@hookimpl
+def pylsp_lint(config, workspace, document):
+    with workspace.report_progress("lint: mccabe"):
+        threshold = config.plugin_settings("mccabe", document_path=document.path).get(
+            THRESHOLD, DEFAULT_THRESHOLD
+        )
+        log.debug("Running mccabe lint with threshold: %s", threshold)
+
+        try:
+            tree = compile(document.source, document.path, "exec", ast.PyCF_ONLY_AST)
+        except SyntaxError:
+            # We'll let the other linters point this one out
+            return None
+
+        visitor = mccabe.PathGraphingAstVisitor()
+        visitor.preorder(tree, visitor)
+
+        diags = []
+        for graph in visitor.graphs.values():
+            if graph.complexity() >= threshold:
+                diags.append(
+                    {
+                        "source": "mccabe",
+                        "range": {
+                            "start": {
+                                "line": graph.lineno - 1,
+                                "character": graph.column,
+                            },
+                            "end": {
+                                "line": graph.lineno - 1,
+                                "character": len(document.lines[graph.lineno]),
+                            },
+                        },
+                        "message": "Cyclomatic complexity too high: %s (threshold %s)"
+                        % (graph.complexity(), threshold),
+                        "severity": lsp.DiagnosticSeverity.Warning,
+                    }
+                )
+
+        return diags
+
+MODULES = [
+    "OpenGL",
+    "PIL",
+    "array",
+    "audioop",
+    "binascii",
+    "cPickle",
+    "cStringIO",
+    "cmath",
+    "collections",
+    "datetime",
+    "errno",
+    "exceptions",
+    "gc",
+    "imageop",
+    "imp",
+    "itertools",
+    "marshal",
+    "math",
+    "matplotlib",
+    "mmap",
+    "mpmath",
+    "msvcrt",
+    "networkx",
+    "nose",
+    "nt",
+    "numpy",
+    "operator",
+    "os",
+    "os.path",
+    "pandas",
+    "parser",
+    "rgbimg",
+    "scipy",
+    "signal",
+    "skimage",
+    "sklearn",
+    "statsmodels",
+    "strop",
+    "sympy",
+    "sys",
+    "thread",
+    "time",
+    "wx",
+    "xxsubtype",
+    "zipimport",
+    "zlib",
+]
+
+@hookimpl
+def pylsp_settings():
+    # Setup default modules to preload, and rope extension modules
+    return {
+        "plugins": {"preload": {"modules": MODULES}},
+        "rope": {"extensionModules": MODULES},
+    }
+
+@hookimpl
+def pylsp_initialize(config) -> None:
+    for mod_name in config.plugin_settings("preload").get("modules", []):
+        try:
+            __import__(mod_name)
+            log.debug("Preloaded module %s", mod_name)
+        except Exception:
+            # Catch any exception since not only ImportError can be raised here
+            # For example, old versions of NumPy can cause a ValueError.
+            # See spyder-ide/spyder#13985
+            pass
+
+def _get_severity(code):
+    # Are style errors ever really errors?
+    if code[0] == "E" or code[0] == "W":
+        return lsp.DiagnosticSeverity.Warning
+    # If no severity is specified, why wouldn't this be informational only?
+    return lsp.DiagnosticSeverity.Information
+
+class PyCodeStyleDiagnosticReport(pycodestyle.BaseReport):
+    def __init__(self, options) -> None:
+        self.diagnostics = []
+        super().__init__(options=options)
+
+    def error(self, line_number, offset, text, check):
+        code = text[:4]
+        if self._ignore_code(code):
+            return
+
+        # Don't care about expected errors or warnings
+        if code in self.expected:
+            return
+
+        # PyCodeStyle will sometimes give you an error the line after the end of the file
+        #   e.g. no newline at end of file
+        # In that case, the end offset should just be some number ~100
+        # (because why not? There's nothing to underline anyways)
+        err_range = {
+            "start": {"line": line_number - 1, "character": offset},
+            "end": {
+                # FIXME: It's a little naiive to mark until the end of the line, can we not easily do better?
+                "line": line_number - 1,
+                "character": 100
+                if line_number > len(self.lines)
+                else len(self.lines[line_number - 1]),
+            },
+        }
+        diagnostic = {
+            "source": "pycodestyle",
+            "range": err_range,
+            "message": text,
+            "code": code,
+            # Are style errors really ever errors?
+            "severity": _get_severity(code),
+        }
+        if code.startswith("W6"):
+            diagnostic["tags"] = [lsp.DiagnosticTag.Deprecated]
+        self.diagnostics.append(diagnostic)
+
+@hookimpl
+def pylsp_lint(workspace, document):
+    with workspace.report_progress("lint: pycodestyle"):
+        config = workspace._config
+        settings = config.plugin_settings("pycodestyle", document_path=document.path)
+        log.debug("Got pycodestyle settings: %s", settings)
+
+        opts = {
+            "exclude": settings.get("exclude"),
+            "filename": settings.get("filename"),
+            "hang_closing": settings.get("hangClosing"),
+            "ignore": settings.get("ignore"),
+            "max_line_length": settings.get("maxLineLength"),
+            "indent_size": settings.get("indentSize"),
+            "select": settings.get("select"),
+        }
+        kwargs = {k: v for k, v in opts.items() if v}
+        styleguide = pycodestyle.StyleGuide(kwargs)
+
+        # Use LF to lint file because other line endings can give false positives.
+        # See spyder-ide/spyder#19565 for context.
+        source = document.source
+        eol_chars = get_eol_chars(source)
+        if eol_chars in ["\r", "\r\n"]:
+            source = source.replace(eol_chars, "\n")
+            lines = source.splitlines(keepends=True)
+        else:
+            lines = document.lines
+
+        c = pycodestyle.Checker(
+            filename=document.path,
+            lines=lines,
+            options=styleguide.options,
+            report=PyCodeStyleDiagnosticReport(styleguide.options),
+        )
+        c.check_all()
+        diagnostics = c.report.diagnostics
+
+        return diagnostics
+
+@hookimpl
+def pylsp_settings():
+    # Default pydocstyle to disabled
+    return {"plugins": {"pydocstyle": {"enabled": False}}}
+
+DEFAULT_MATCH_RE = pydocstyle.config.ConfigurationParser.DEFAULT_MATCH_RE
+
+DEFAULT_MATCH_DIR_RE = pydocstyle.config.ConfigurationParser.DEFAULT_MATCH_DIR_RE
+
+def _parse_diagnostic(document, error):
+    lineno = error.definition.start - 1
+    line = document.lines[0] if document.lines else ""
+
+    start_character = len(line) - len(line.lstrip())
+    end_character = len(line)
+
+    return {
+        "source": "pydocstyle",
+        "code": error.code,
+        "message": error.message,
+        "severity": lsp.DiagnosticSeverity.Warning,
+        "range": {
+            "start": {"line": lineno, "character": start_character},
+            "end": {"line": lineno, "character": end_character},
+        },
+    }
+
+@contextlib.contextmanager
+def _patch_sys_argv(arguments) -> None:
+    old_args = sys.argv
+
+    # Preserve argv[0] since it's the executable
+    sys.argv = old_args[0:1] + arguments
+
+    try:
+        yield
+    finally:
+        sys.argv = old_args
+
+@hookimpl
+def pylsp_lint(config, workspace, document):
+    with workspace.report_progress("lint: pydocstyle"):
+        settings = config.plugin_settings("pydocstyle", document_path=document.path)
+        log.debug("Got pydocstyle settings: %s", settings)
+
+        # Explicitly passing a path to pydocstyle means it doesn't respect the --match flag, so do it ourselves
+        filename_match_re = re.compile(settings.get("match", DEFAULT_MATCH_RE) + "$")
+        if not filename_match_re.match(os.path.basename(document.path)):
+            return []
+
+        # Likewise with --match-dir
+        dir_match_re = re.compile(settings.get("matchDir", DEFAULT_MATCH_DIR_RE) + "$")
+        if not dir_match_re.match(os.path.basename(os.path.dirname(document.path))):
+            return []
+
+        args = [document.path]
+
+        if settings.get("convention"):
+            args.append("--convention=" + settings["convention"])
+
+            if settings.get("addSelect"):
+                args.append("--add-select=" + ",".join(settings["addSelect"]))
+            if settings.get("addIgnore"):
+                args.append("--add-ignore=" + ",".join(settings["addIgnore"]))
+
+        elif settings.get("select"):
+            args.append("--select=" + ",".join(settings["select"]))
+        elif settings.get("ignore"):
+            args.append("--ignore=" + ",".join(settings["ignore"]))
+
+        log.info("Using pydocstyle args: %s", args)
+
+        conf = pydocstyle.config.ConfigurationParser()
+        with _patch_sys_argv(args):
+            # TODO(gatesn): We can add more pydocstyle args here from our pylsp config
+            conf.parse()
+
+        # Will only yield a single filename, the document path
+        diags = []
+        for (
+            filename,
+            checked_codes,
+            ignore_decorators,
+            property_decorators,
+            ignore_self_only_init,
+        ) in conf.get_files_to_check():
+            errors = pydocstyle.checker.ConventionChecker().check_source(
+                document.source,
+                filename,
+                ignore_decorators=ignore_decorators,
+                property_decorators=property_decorators,
+                ignore_self_only_init=ignore_self_only_init,
+            )
+
+            try:
+                for error in errors:
+                    if error.code not in checked_codes:
+                        continue
+                    diags.append(_parse_diagnostic(document, error))
+            except pydocstyle.parser.ParseError:
+                # In the case we cannot parse the Python file, just continue
+                pass
+
+        log.debug("Got pydocstyle errors: %s", diags)
+        return diags
+
+# Pyflakes messages that should be reported as Errors instead of Warns
+PYFLAKES_ERROR_MESSAGES = (
+    messages.UndefinedName,
+    messages.UndefinedExport,
+    messages.UndefinedLocal,
+    messages.DuplicateArgument,
+    messages.FutureFeatureNotDefined,
+    messages.ReturnOutsideFunction,
+    messages.YieldOutsideFunction,
+    messages.ContinueOutsideLoop,
+    messages.BreakOutsideLoop,
+    messages.TwoStarredExpressions,
+)
+
+class PyflakesDiagnosticReport:
+    def __init__(self, lines) -> None:
+        self.lines = lines
+        self.diagnostics = []
+
+    def unexpectedError(self, _filename, msg) -> None:  # pragma: no cover
+        err_range = {
+            "start": {"line": 0, "character": 0},
+            "end": {"line": 0, "character": 0},
+        }
+        self.diagnostics.append(
+            {
+                "source": "pyflakes",
+                "range": err_range,
+                "message": msg,
+                "severity": lsp.DiagnosticSeverity.Error,
+            }
+        )
+
+    def syntaxError(self, _filename, msg, lineno, offset, text) -> None:
+        # We've seen that lineno and offset can sometimes be None
+        lineno = lineno or 1
+        offset = offset or 0
+        # could be None if the error is due to an invalid encoding
+        # see e.g. https://github.com/python-lsp/python-lsp-server/issues/429
+        text = text or ""
+
+        err_range = {
+            "start": {"line": lineno - 1, "character": offset},
+            "end": {"line": lineno - 1, "character": offset + len(text)},
+        }
+        self.diagnostics.append(
+            {
+                "source": "pyflakes",
+                "range": err_range,
+                "message": msg,
+                "severity": lsp.DiagnosticSeverity.Error,
+            }
+        )
+
+    def flake(self, message) -> None:
+        """Get message like <filename>:<lineno>: <msg>"""
+        err_range = {
+            "start": {"line": message.lineno - 1, "character": message.col},
+            "end": {
+                "line": message.lineno - 1,
+                "character": len(self.lines[message.lineno - 1]),
+            },
+        }
+
+        severity = lsp.DiagnosticSeverity.Warning
+        for message_type in PYFLAKES_ERROR_MESSAGES:
+            if isinstance(message, message_type):
+                severity = lsp.DiagnosticSeverity.Error
+                break
+
+        self.diagnostics.append(
+            {
+                "source": "pyflakes",
+                "range": err_range,
+                "message": message.message % message.message_args,
+                "severity": severity,
+            }
+        )
+
+@hookimpl
+def pylsp_lint(workspace, document):
+    with workspace.report_progress("lint: pyflakes"):
+        reporter = PyflakesDiagnosticReport(document.lines)
+        pyflakes_api.check(
+            document.source.encode("utf-8"), document.path, reporter=reporter
+        )
+        return reporter.diagnostics
+
+@hookimpl
+def pylsp_settings():
+    # Default pylint to disabled because it requires a config
+    # file to be useful.
+    return {
+        "plugins": {
+            "pylint": {
+                "enabled": False,
+                "args": [],
+                # disabled by default as it can slow down the workflow
+                "executable": None,
+            }
+        }
+    }
+
+DEPRECATION_CODES = {
+    "W0402",  # Uses of a deprecated module %r
+    "W1505",  # Using deprecated method %s()
+    "W1511",  # Using deprecated argument %s of method %s()
+    "W1512",  # Using deprecated class %s of module %s
+    "W1513",  # Using deprecated decorator %s()
+}
+
+UNNECESSITY_CODES = {
+    "W0611",  # Unused import %s
+    "W0612",  # Unused variable %r
+    "W0613",  # Unused argument %r
+    "W0614",  # Unused import %s from wildcard import
+    "W1304",  # Unused-format-string-argument
+}
+
+class PylintLinter:
+    last_diags = collections.defaultdict(list)
+
+    @classmethod
+    def lint(cls, document, is_saved, flags=""):
+        """Plugin interface to pylsp linter.
+
+        Args:
+            document: The document to be linted.
+            is_saved: Whether or not the file has been saved to disk.
+            flags: Additional flags to pass to pylint. Not exposed to
+                pylsp_lint, but used for testing.
+
+        Returns:
+            A list of dicts with the following format:
+
+                {
+                    'source': 'pylint',
+                    'range': {
+                        'start': {
+                            'line': start_line,
+                            'character': start_column,
+                        },
+                        'end': {
+                            'line': end_line,
+                            'character': end_column,
+                        },
+                    }
+                    'message': msg,
+                    'severity': lsp.DiagnosticSeverity.*,
+                }
+        """
+        if not is_saved:
+            # Pylint can only be run on files that have been saved to disk.
+            # Rather than return nothing, return the previous list of
+            # diagnostics. If we return an empty list, any diagnostics we'd
+            # previously shown will be cleared until the next save. Instead,
+            # continue showing (possibly stale) diagnostics until the next
+            # save.
+            return cls.last_diags[document.path]
+
+        cmd = [
+            sys.executable,
+            "-c",
+            "import sys; from pylint.lint import Run; Run(sys.argv[1:])",
+            "-f",
+            "json",
+            document.path,
+        ] + (shlex.split(str(flags)) if flags else [])
+        log.debug("Calling pylint with '%s'", " ".join(cmd))
+
+        cwd = document._workspace.root_path
+        if not cwd:
+            cwd = os.path.dirname(__file__)
+
+        with Popen(
+            cmd, stdout=PIPE, stderr=PIPE, cwd=cwd, universal_newlines=True
+        ) as process:
+            json_out, err = process.communicate()
+
+        if err != "":
+            log.error("Error calling pylint: '%s'", err)
+
+        # pylint prints nothing rather than [] when there are no diagnostics.
+        # json.loads will not parse an empty string, so just return.
+        if not json_out.strip():
+            cls.last_diags[document.path] = []
+            return []
+
+        # Pylint's JSON output is a list of objects with the following format.
+        #
+        #     {
+        #         "obj": "main",
+        #         "path": "foo.py",
+        #         "message": "Missing function docstring",
+        #         "message-id": "C0111",
+        #         "symbol": "missing-docstring",
+        #         "column": 0,
+        #         "type": "convention",
+        #         "line": 5,
+        #         "module": "foo"
+        #     }
+        #
+        # The type can be any of:
+        #
+        #  * convention
+        #  * information
+        #  * error
+        #  * fatal
+        #  * refactor
+        #  * warning
+        diagnostics = []
+        for diag in json.loads(json_out):
+            # pylint lines index from 1, pylsp lines index from 0
+            line = diag["line"] - 1
+
+            err_range = {
+                "start": {
+                    "line": line,
+                    # Index columns start from 0
+                    "character": diag["column"],
+                },
+                "end": {
+                    "line": line,
+                    # It's possible that we're linting an empty file. Even an empty
+                    # file might fail linting if it isn't named properly.
+                    "character": len(document.lines[line]) if document.lines else 0,
+                },
+            }
+
+            if diag["type"] == "convention":
+                severity = lsp.DiagnosticSeverity.Information
+            elif diag["type"] == "information":
+                severity = lsp.DiagnosticSeverity.Information
+            elif diag["type"] == "error":
+                severity = lsp.DiagnosticSeverity.Error
+            elif diag["type"] == "fatal":
+                severity = lsp.DiagnosticSeverity.Error
+            elif diag["type"] == "refactor":
+                severity = lsp.DiagnosticSeverity.Hint
+            elif diag["type"] == "warning":
+                severity = lsp.DiagnosticSeverity.Warning
+
+            code = diag["message-id"]
+
+            diagnostic = {
+                "source": "pylint",
+                "range": err_range,
+                "message": "[{}] {}".format(diag["symbol"], diag["message"]),
+                "severity": severity,
+                "code": code,
+            }
+
+            if code in UNNECESSITY_CODES:
+                diagnostic["tags"] = [lsp.DiagnosticTag.Unnecessary]
+            if code in DEPRECATION_CODES:
+                diagnostic["tags"] = [lsp.DiagnosticTag.Deprecated]
+
+            diagnostics.append(diagnostic)
+        cls.last_diags[document.path] = diagnostics
+        return diagnostics
+
+def _build_pylint_flags(settings):
+    """Build arguments for calling pylint."""
+    pylint_args = settings.get("args")
+    if pylint_args is None:
+        return ""
+    return " ".join(pylint_args)
+
+def build_args_stdio(settings):
+    """Build arguments for calling pylint.
+
+    :param settings: client settings
+    :type settings: dict
+
+    :return: arguments to path to pylint
+    :rtype: list
+    """
+    pylint_args = settings.get("args")
+    if pylint_args is None:
+        return []
+    return pylint_args
+
+def _run_pylint_stdio(pylint_executable, document, flags):
+    """Run pylint in popen.
+
+    :param pylint_executable: path to pylint executable
+    :type pylint_executable: string
+    :param document: document to run pylint on
+    :type document: pylsp.workspace.Document
+    :param flags: arguments to path to pylint
+    :type flags: list
+
+    :return: result of calling pylint
+    :rtype: string
+    """
+    log.debug("Calling %s with args: '%s'", pylint_executable, flags)
+    try:
+        cmd = [pylint_executable]
+        cmd.extend(flags)
+        cmd.extend(["--from-stdin", document.path])
+        p = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE)
+    except IOError:
+        log.debug("Can't execute %s. Trying with 'python -m pylint'", pylint_executable)
+        cmd = [sys.executable, "-m", "pylint"]
+        cmd.extend(flags)
+        cmd.extend(["--from-stdin", document.path])
+        p = Popen(cmd, stdin=PIPE, stdout=PIPE, stderr=PIPE)
+    (stdout, stderr) = p.communicate(document.source.encode())
+    if stderr:
+        log.error("Error while running pylint '%s'", stderr.decode())
+    return stdout.decode()
+
+def _parse_pylint_stdio_result(document, stdout):
+    """Parse pylint results.
+
+    :param document: document to run pylint on
+    :type document: pylsp.workspace.Document
+    :param stdout: pylint results to parse
+    :type stdout: string
+
+    :return: linting diagnostics
+    :rtype: list
+    """
+    diagnostics = []
+    lines = stdout.splitlines()
+    for raw_line in lines:
+        parsed_line = re.match(r"(.*):(\d*):(\d*): (\w*): (.*)", raw_line)
+        if not parsed_line:
+            log.debug("Pylint output parser can't parse line '%s'", raw_line)
+            continue
+
+        parsed_line = parsed_line.groups()
+        if len(parsed_line) != 5:
+            log.debug("Pylint output parser can't parse line '%s'", raw_line)
+            continue
+
+        _, line, character, code, msg = parsed_line
+        line = int(line) - 1
+        character = int(character)
+        severity_map = {
+            "C": lsp.DiagnosticSeverity.Information,
+            "E": lsp.DiagnosticSeverity.Error,
+            "F": lsp.DiagnosticSeverity.Error,
+            "I": lsp.DiagnosticSeverity.Information,
+            "R": lsp.DiagnosticSeverity.Hint,
+            "W": lsp.DiagnosticSeverity.Warning,
+        }
+        severity = severity_map[code[0]]
+        diagnostic = {
+            "source": "pylint",
+            "code": code,
+            "range": {
+                "start": {"line": line, "character": character},
+                "end": {
+                    "line": line,
+                    # no way to determine the column
+                    "character": len(document.lines[line]) - 1,
+                },
+            },
+            "message": msg,
+            "severity": severity,
+        }
+        if code in UNNECESSITY_CODES:
+            diagnostic["tags"] = [lsp.DiagnosticTag.Unnecessary]
+        if code in DEPRECATION_CODES:
+            diagnostic["tags"] = [lsp.DiagnosticTag.Deprecated]
+        diagnostics.append(diagnostic)
+
+    return diagnostics
+
+def pylint_lint_stdin(pylint_executable, document, flags):
+    """Run pylint linter from stdin.
+
+    This runs pylint in a subprocess with popen.
+    This allows passing the file from stdin and as a result
+    run pylint on unsaved files. Can slowdown the workflow.
+
+    :param pylint_executable: path to pylint executable
+    :type pylint_executable: string
+    :param document: document to run pylint on
+    :type document: pylsp.workspace.Document
+    :param flags: arguments to path to pylint
+    :type flags: list
+
+    :return: linting diagnostics
+    :rtype: list
+    """
+    pylint_result = _run_pylint_stdio(pylint_executable, document, flags)
+    return _parse_pylint_stdio_result(document, pylint_result)
+
+@hookimpl
+def pylsp_lint(config, workspace, document, is_saved):
+    """Run pylint linter."""
+    with workspace.report_progress("lint: pylint"):
+        settings = config.plugin_settings("pylint")
+        log.debug("Got pylint settings: %s", settings)
+        # pylint >= 2.5.0 is required for working through stdin and only
+        # available with python3
+        if settings.get("executable") and sys.version_info[0] >= 3:
+            flags = build_args_stdio(settings)
+            pylint_executable = settings.get("executable", "pylint")
+            return pylint_lint_stdin(pylint_executable, document, flags)
+        flags = _build_pylint_flags(settings)
+        return PylintLinter.lint(document, is_saved, flags=flags)
+
+@hookimpl
+def pylsp_references(document, position, exclude_declaration):
+    code_position = _utils.position_to_jedi_linecolumn(document, position)
+    usages = document.jedi_script().get_references(**code_position)
+
+    if exclude_declaration:
+        # Filter out if the usage is the actual declaration of the thing
+        usages = [d for d in usages if not d.is_definition()]
+
+    # Filter out builtin modules
+    return [
+        {
+            "uri": uris.uri_with(document.uri, path=str(d.module_path))
+            if d.module_path
+            else document.uri,
+            "range": {
+                "start": {"line": d.line - 1, "character": d.column},
+                "end": {"line": d.line - 1, "character": d.column + len(d.name)},
+            },
+        }
+        for d in usages
+        if not d.in_builtin_module()
+    ]
+
+@hookimpl
+def pylsp_settings() -> Dict[str, Dict[str, Dict[str, Any]]]:
+    # Default rope_completion to disabled
+    return {
+        "plugins": {
+            "rope_autoimport": {
+                "enabled": False,
+                "memory": False,
+                "completions": {
+                    "enabled": True,
+                },
+                "code_actions": {
+                    "enabled": True,
+                },
+            }
+        }
+    }
+
+MAX_RESULTS_COMPLETIONS = 1000
+
+def _should_import_class(word_node: tree.Leaf, expr: tree.BaseNode) -> bool:
+    prev_node = None
+    for node in expr.children:
+        if isinstance(node, tree.Name):
+            if isinstance(prev_node, tree.Operator):
+                if node == word_node and prev_node.value == "(":
+                    return True
+        prev_node = node
+
+    return False
+
+def _handle_argument(node: NodeOrLeaf, word_node: tree.Leaf):
+    if isinstance(node, tree.PythonNode):
+        if node.type == "tfpdef":
+            if node.children[2] == word_node:
+                return True
+        if node.type == "parameters":
+            for parameter in node.children:
+                if _handle_argument(parameter, word_node):
+                    return True
+    return False
+
+def _should_import_function(word_node: tree.Leaf, expr: tree.BaseNode) -> bool:
+    prev_node = None
+    for node in expr.children:
+        if _handle_argument(node, word_node):
+            return True
+        if isinstance(prev_node, tree.Operator):
+            if prev_node.value == "->":
+                if node == word_node:
+                    return True
+        prev_node = node
+    return False
+
+def _handle_first_child(
+    first_child: NodeOrLeaf, expr: tree.BaseNode, word_node: tree.Leaf
+) -> bool:
+    """Check if we suggest imports given the following first child."""
+    if isinstance(first_child, tree.Import):
+        return False
+    if isinstance(first_child, (tree.PythonLeaf, tree.PythonErrorLeaf)):
+        # Check if the first item is a from or import statement even when incomplete
+        if first_child.value in ("import", "from"):
+            return False
+    if isinstance(first_child, tree.Keyword):
+        if first_child.value == "def":
+            return _should_import_function(word_node, expr)
+        if first_child.value == "class":
+            return _should_import_class(word_node, expr)
+    return True
+
+def _should_insert(expr: tree.BaseNode, word_node: tree.Leaf) -> bool:
+    """
+    Check if we should insert the word_node on the given expr.
+
+    Works for both correct and incorrect code. This is because the
+    user is often working on the code as they write it.
+    """
+    if not word_node:
+        return False
+    if len(expr.children) == 0:
+        return True
+    first_child = expr.children[0]
+    if isinstance(first_child, tree.EndMarker):
+        if "#" in first_child.prefix:
+            return False  # Check for single line comment
+    if first_child == word_node:
+        return True  # If the word is the first word then its fine
+    if len(expr.children) > 1:
+        if any(
+            node.type == "operator" and "." in node.value or node.type == "trailer"
+            for node in expr.children
+        ):
+            return False  # Check if we're on a method of a function
+    if isinstance(first_child, (tree.PythonErrorNode, tree.PythonNode)):
+        # The tree will often include error nodes like this to indicate errors
+        # we want to ignore errors since the code is being written
+        return _should_insert(first_child, word_node)
+    return _handle_first_child(first_child, expr, word_node)
+
+_score_pow = 5
+
+_score_max = 10**_score_pow
+
+def _document(import_statement: str) -> str:
+    return """# Auto-Import\n""" + import_statement
+
+def _get_score(
+    source: int, full_statement: str, suggested_name: str, desired_name
+) -> int:
+    import_length = len("import")
+    full_statement_score = len(full_statement) - import_length
+    suggested_name_score = (len(suggested_name) - len(desired_name)) ** 2
+    source_score = 20 * source
+    return suggested_name_score + full_statement_score + source_score
+
+def _sort_import(score: int) -> str:
+    score = max(min(score, (_score_max) - 1), 0)
+    # Since we are using ints, we need to pad them.
+    # We also want to prioritize autoimport behind everything since its the last priority.
+    # The minimum is to prevent score from overflowing the pad
+    return "[z" + str(score).rjust(_score_pow, "0")
+
+def _process_statements(
+    suggestions: List[SearchResult],
+    doc_uri: str,
+    word: str,
+    autoimport: AutoImport,
+    document: Document,
+    feature: str = "completions",
+) -> Generator[Dict[str, Any], None, None]:
+    for suggestion in suggestions:
+        insert_line = autoimport.find_insertion_line(document.source) - 1
+        start = {"line": insert_line, "character": 0}
+        edit_range = {"start": start, "end": start}
+        edit = {"range": edit_range, "newText": suggestion.import_statement + "\n"}
+        score = _get_score(
+            suggestion.source, suggestion.import_statement, suggestion.name, word
+        )
+        if score > _score_max:
+            continue
+        # TODO make this markdown
+        if feature == "completions":
+            yield {
+                "label": suggestion.name,
+                "kind": suggestion.itemkind,
+                "sortText": _sort_import(score),
+                "data": {"doc_uri": doc_uri},
+                "detail": _document(suggestion.import_statement),
+                "additionalTextEdits": [edit],
+            }
+        elif feature == "code_actions":
+            yield {
+                "title": suggestion.import_statement,
+                "kind": "quickfix",
+                "edit": {"changes": {doc_uri: [edit]}},
+                # data is a supported field for codeAction responses
+                # See https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#textDocument_codeAction
+                "data": {"sortText": _sort_import(score)},
+            }
+        else:
+            raise ValueError(f"Unknown feature: {feature}")
+
+def get_names(script: Script) -> Set[str]:
+    """Get all names to ignore from the current file."""
+    raw_names = script.get_names(definitions=True)
+    log.debug(raw_names)
+    return {name.name for name in raw_names}
+
+class AutoimportCache:
+    """Handles the cache creation."""
+
+    def __init__(self) -> None:
+        self.thread = None
+
+    def reload_cache(
+        self,
+        config: Config,
+        workspace: Workspace,
+        files: Optional[List[Document]] = None,
+        single_thread: Optional[bool] = True,
+    ):
+        if self.is_blocked():
+            return
+
+        memory: bool = config.plugin_settings("rope_autoimport").get("memory", False)
+        rope_config = config.settings().get("rope", {})
+        autoimport = workspace._rope_autoimport(rope_config, memory)
+        resources: Optional[List[Resource]] = (
+            None
+            if files is None
+            else [document._rope_resource(rope_config) for document in files]
+        )
+
+        if single_thread:
+            self._reload_cache(workspace, autoimport, resources)
+        else:
+            # Creating the cache may take 10-20s for a environment with 5k python modules. That's
+            # why we decided to move cache creation into its own thread.
+            self.thread = threading.Thread(
+                target=self._reload_cache, args=(workspace, autoimport, resources)
+            )
+            self.thread.start()
+
+    def _reload_cache(
+        self,
+        workspace: Workspace,
+        autoimport: AutoImport,
+        resources: Optional[List[Resource]] = None,
+    ) -> None:
+        task_handle = PylspTaskHandle(workspace)
+        autoimport.generate_cache(task_handle=task_handle, resources=resources)
+        autoimport.generate_modules_cache(task_handle=task_handle)
+
+    def is_blocked(self):
+        return self.thread and self.thread.is_alive()
+
+cache: AutoimportCache = AutoimportCache()
+
+@hookimpl
+def pylsp_completions(
+    config: Config,
+    workspace: Workspace,
+    document: Document,
+    position,
+    ignored_names: Union[Set[str], None],
+):
+    """Get autoimport suggestions."""
+    if (
+        not config.plugin_settings("rope_autoimport")
+        .get("completions", {})
+        .get("enabled", True)
+    ) or cache.is_blocked():
+        return []
+
+    line = document.lines[position["line"]]
+    expr = parso.parse(line)
+    word_node = expr.get_leaf_for_position((1, position["character"]))
+    if not _should_insert(expr, word_node):
+        return []
+    word = word_node.value
+    log.debug(f"autoimport: searching for word: {word}")
+    rope_config = config.settings(document_path=document.path).get("rope", {})
+    ignored_names: Set[str] = ignored_names or get_names(
+        document.jedi_script(use_document_path=True)
+    )
+    autoimport = workspace._rope_autoimport(rope_config)
+    suggestions = list(autoimport.search_full(word, ignored_names=ignored_names))
+    results = sorted(
+        _process_statements(
+            suggestions, document.uri, word, autoimport, document, "completions"
+        ),
+        key=lambda statement: statement["sortText"],
+    )
+    if len(results) > MAX_RESULTS_COMPLETIONS:
+        results = results[:MAX_RESULTS_COMPLETIONS]
+    return results
+
+MAX_RESULTS_CODE_ACTIONS = 5
+
+def get_name_or_module(document, diagnostic) -> str:
+    start = diagnostic["range"]["start"]
+    return (
+        parso.parse(document.lines[start["line"]])
+        .get_leaf_for_position((1, start["character"] + 1))
+        .value
+    )
+
+@hookimpl
+def pylsp_code_actions(
+    config: Config,
+    workspace: Workspace,
+    document: Document,
+    range: Dict,
+    context: Dict,
+) -> List[Dict]:
+    """
+    Provide code actions through rope.
+
+    Parameters
+    ----------
+    config : pylsp.config.config.Config
+        Current config.
+    workspace : pylsp.workspace.Workspace
+        Current workspace.
+    document : pylsp.workspace.Document
+        Document to apply code actions on.
+    range : Dict
+        Range argument given by pylsp. Not used here.
+    context : Dict
+        CodeActionContext given as dict.
+
+    Returns
+    -------
+      List of dicts containing the code actions.
+    """
+    if (
+        not config.plugin_settings("rope_autoimport")
+        .get("code_actions", {})
+        .get("enabled", True)
+    ) or cache.is_blocked():
+        return []
+
+    log.debug(f"textDocument/codeAction: {document} {range} {context}")
+    code_actions = []
+    for diagnostic in context.get("diagnostics", []):
+        if "undefined name" not in diagnostic.get("message", "").lower():
+            continue
+
+        word = get_name_or_module(document, diagnostic)
+        log.debug(f"autoimport: searching for word: {word}")
+        rope_config = config.settings(document_path=document.path).get("rope", {})
+        autoimport = workspace._rope_autoimport(rope_config)
+        suggestions = list(autoimport.search_full(word))
+        log.debug("autoimport: suggestions: %s", suggestions)
+        results = sorted(
+            _process_statements(
+                suggestions,
+                document.uri,
+                word,
+                autoimport,
+                document,
+                "code_actions",
+            ),
+            key=lambda statement: statement["data"]["sortText"],
+        )
+
+        if len(results) > MAX_RESULTS_CODE_ACTIONS:
+            results = results[:MAX_RESULTS_CODE_ACTIONS]
+        code_actions.extend(results)
+
+    return code_actions
+
+@hookimpl
+def pylsp_initialize(config: Config, workspace: Workspace) -> None:
+    """Initialize AutoImport.
+
+    Generates the cache for local and global items.
+    """
+    cache.reload_cache(config, workspace)
+
+@hookimpl
+def pylsp_document_did_open(config: Config, workspace: Workspace) -> None:
+    """Initialize AutoImport.
+
+    Generates the cache for local and global items.
+    """
+    cache.reload_cache(config, workspace)
+
+@hookimpl
+def pylsp_document_did_save(
+    config: Config, workspace: Workspace, document: Document
+) -> None:
+    """Update the names associated with this document."""
+    cache.reload_cache(config, workspace, [document])
+
+@hookimpl
+def pylsp_workspace_configuration_changed(config: Config, workspace: Workspace) -> None:
+    """
+    Initialize autoimport if it has been enabled through a
+    workspace/didChangeConfiguration message from the frontend.
+
+    Generates the cache for local and global items.
+    """
+    if config.plugin_settings("rope_autoimport").get("enabled", False):
+        cache.reload_cache(config, workspace)
+    else:
+        log.debug("autoimport: Skipping cache reload.")
+
+@hookimpl
+def pylsp_settings():
+    # Default rope_completion to disabled
+    return {"plugins": {"rope_completion": {"enabled": False, "eager": False}}}
+
+def _resolve_completion(completion, data, markup_kind):
+    try:
+        doc = _utils.format_docstring(data.get_doc(), markup_kind=markup_kind)
+    except Exception as e:
+        log.debug("Failed to resolve Rope completion: %s", e)
+        doc = ""
+    completion["detail"] = "{0} {1}".format(data.scope or "", data.name)
+    completion["documentation"] = doc
+    return completion
+
+def _sort_text(definition):
+    """Ensure builtins appear at the bottom.
+    Description is of format <type>: <module>.<item>
+    """
+    if definition.name.startswith("_"):
+        # It's a 'hidden' func, put it next last
+        return "z" + definition.name
+    if definition.scope == "builtin":
+        return "y" + definition.name
+
+    # Else put it at the front
+    return "a" + definition.name
+
+def _kind(d):
+    """Return the LSP type"""
+    MAP = {
+        "none": lsp.CompletionItemKind.Value,
+        "type": lsp.CompletionItemKind.Class,
+        "tuple": lsp.CompletionItemKind.Class,
+        "dict": lsp.CompletionItemKind.Class,
+        "dictionary": lsp.CompletionItemKind.Class,
+        "function": lsp.CompletionItemKind.Function,
+        "lambda": lsp.CompletionItemKind.Function,
+        "generator": lsp.CompletionItemKind.Function,
+        "class": lsp.CompletionItemKind.Class,
+        "instance": lsp.CompletionItemKind.Reference,
+        "method": lsp.CompletionItemKind.Method,
+        "builtin": lsp.CompletionItemKind.Class,
+        "builtinfunction": lsp.CompletionItemKind.Function,
+        "module": lsp.CompletionItemKind.Module,
+        "file": lsp.CompletionItemKind.File,
+        "xrange": lsp.CompletionItemKind.Class,
+        "slice": lsp.CompletionItemKind.Class,
+        "traceback": lsp.CompletionItemKind.Class,
+        "frame": lsp.CompletionItemKind.Class,
+        "buffer": lsp.CompletionItemKind.Class,
+        "dictproxy": lsp.CompletionItemKind.Class,
+        "funcdef": lsp.CompletionItemKind.Function,
+        "property": lsp.CompletionItemKind.Property,
+        "import": lsp.CompletionItemKind.Module,
+        "keyword": lsp.CompletionItemKind.Keyword,
+        "constant": lsp.CompletionItemKind.Variable,
+        "variable": lsp.CompletionItemKind.Variable,
+        "value": lsp.CompletionItemKind.Value,
+        "param": lsp.CompletionItemKind.Variable,
+        "statement": lsp.CompletionItemKind.Keyword,
+    }
+
+    return MAP.get(d.type)
+
+@hookimpl
+def pylsp_completions(config, workspace, document, position):
+    settings = config.plugin_settings("rope_completion", document_path=document.path)
+    resolve_eagerly = settings.get("eager", False)
+
+    # Rope is a bit rubbish at completing module imports, so we'll return None
+    word = document.word_at_position(
+        {
+            # The -1 should really be trying to look at the previous word, but that might be quite expensive
+            # So we only skip import completions when the cursor is one space after `import`
+            "line": position["line"],
+            "character": max(position["character"] - 1, 0),
+        }
+    )
+    if word == "import":
+        return None
+
+    offset = document.offset_at_position(position)
+    rope_config = config.settings(document_path=document.path).get("rope", {})
+    rope_project = workspace._rope_project_builder(rope_config)
+    document_rope = document._rope_resource(rope_config)
+
+    completion_capabilities = config.capabilities.get("textDocument", {}).get(
+        "completion", {}
+    )
+    item_capabilities = completion_capabilities.get("completionItem", {})
+    supported_markup_kinds = item_capabilities.get("documentationFormat", ["markdown"])
+    preferred_markup_kind = _utils.choose_markup_kind(supported_markup_kinds)
+
+    try:
+        definitions = code_assist(
+            rope_project, document.source, offset, document_rope, maxfixes=3
+        )
+    except Exception as e:
+        log.debug("Failed to run Rope code assist: %s", e)
+        return []
+
+    definitions = sorted_proposals(definitions)
+    new_definitions = []
+    for d in definitions:
+        item = {
+            "label": d.name,
+            "kind": _kind(d),
+            "sortText": _sort_text(d),
+            "data": {"doc_uri": document.uri},
+        }
+        if resolve_eagerly:
+            item = _resolve_completion(item, d, preferred_markup_kind)
+        new_definitions.append(item)
+
+    # most recently retrieved completion items, used for resolution
+    document.shared_data["LAST_ROPE_COMPLETIONS"] = {
+        # label is the only required property; here it is assumed to be unique
+        completion["label"]: (completion, data)
+        for completion, data in zip(new_definitions, definitions)
+    }
+
+    definitions = new_definitions
+
+    return definitions or None
+
+@hookimpl
+def pylsp_completion_item_resolve(config, completion_item, document):
+    """Resolve formatted completion for given non-resolved completion"""
+    shared_data = document.shared_data["LAST_ROPE_COMPLETIONS"].get(
+        completion_item["label"]
+    )
+
+    completion_capabilities = config.capabilities.get("textDocument", {}).get(
+        "completion", {}
+    )
+    item_capabilities = completion_capabilities.get("completionItem", {})
+    supported_markup_kinds = item_capabilities.get("documentationFormat", ["markdown"])
+    preferred_markup_kind = _utils.choose_markup_kind(supported_markup_kinds)
+
+    if shared_data:
+        completion, data = shared_data
+        return _resolve_completion(completion, data, preferred_markup_kind)
+    return completion_item
+
+SPHINX = re.compile(r"\s*:param\s+(?P<param>\w+):\s*(?P<doc>[^\n]+)")
+
+EPYDOC = re.compile(r"\s*@param\s+(?P<param>\w+):\s*(?P<doc>[^\n]+)")
+
+GOOGLE = re.compile(r"\s*(?P<param>\w+).*:\s*(?P<doc>[^\n]+)")
+
+DOC_REGEX = [SPHINX, EPYDOC, GOOGLE]
+
+def _param_docs(docstring, param_name):
+    for line in docstring.splitlines():
+        for regex in DOC_REGEX:
+            m = regex.match(line)
+            if not m:
+                continue
+            if m.group("param") != param_name:
+                continue
+            return m.group("doc") or ""
+
+@hookimpl
+def pylsp_signature_help(config, document, position):
+    code_position = _utils.position_to_jedi_linecolumn(document, position)
+    signatures = document.jedi_script().get_signatures(**code_position)
+
+    if not signatures:
+        return {"signatures": []}
+
+    signature_capabilities = config.capabilities.get("textDocument", {}).get(
+        "signatureHelp", {}
+    )
+    signature_information_support = signature_capabilities.get(
+        "signatureInformation", {}
+    )
+    supported_markup_kinds = signature_information_support.get(
+        "documentationFormat", ["markdown"]
+    )
+    preferred_markup_kind = _utils.choose_markup_kind(supported_markup_kinds)
+
+    s = signatures[0]
+
+    docstring = s.docstring()
+
+    # Docstring contains one or more lines of signature, followed by empty line, followed by docstring
+    function_sig_lines = (docstring.split("\n\n") or [""])[0].splitlines()
+    function_sig = " ".join([line.strip() for line in function_sig_lines])
+    sig = {
+        "label": function_sig,
+        "documentation": _utils.format_docstring(
+            s.docstring(raw=True), markup_kind=preferred_markup_kind
+        ),
+    }
+
+    # If there are params, add those
+    if s.params:
+        sig["parameters"] = [
+            {
+                "label": p.name,
+                "documentation": _utils.format_docstring(
+                    _param_docs(docstring, p.name), markup_kind=preferred_markup_kind
+                ),
+            }
+            for p in s.params
+        ]
+
+    # We only return a single signature because Python doesn't allow overloading
+    sig_info = {"signatures": [sig], "activeSignature": 0}
+
+    if s.index is not None and s.params:
+        # Then we know which parameter we're looking at
+        sig_info["activeParameter"] = s.index
+
+    return sig_info
+
+def _include_def(definition):
+    return (
+        # Don't tend to include parameters as symbols
+        definition.type != "param"
+        and
+        # Unused vars should also be skipped
+        definition.name != "_"
+        and _kind(definition) is not None
+    )
+
+def _container(definition):
+    try:
+        # Jedi sometimes fails here.
+        parent = definition.parent()
+        # Here we check that a grand-parent exists to avoid declaring symbols
+        # as children of the module.
+        if parent.parent():
+            return parent.name
+    except:
+        return None
+
+    return None
+
+def _range(definition):
+    # This gets us more accurate end position
+    definition = definition._name.tree_name.get_definition()
+    (start_line, start_column) = definition.start_pos
+    (end_line, end_column) = definition.end_pos
+    return {
+        "start": {"line": start_line - 1, "character": start_column},
+        "end": {"line": end_line - 1, "character": end_column},
+    }
+
+def _tuple_range(definition):
+    definition = definition._name.tree_name.get_definition()
+    return (definition.start_pos, definition.end_pos)
+
+_SYMBOL_KIND_MAP = {
+    "none": SymbolKind.Variable,
+    "type": SymbolKind.Class,
+    "tuple": SymbolKind.Class,
+    "dict": SymbolKind.Class,
+    "dictionary": SymbolKind.Class,
+    "function": SymbolKind.Function,
+    "lambda": SymbolKind.Function,
+    "generator": SymbolKind.Function,
+    "class": SymbolKind.Class,
+    "instance": SymbolKind.Class,
+    "method": SymbolKind.Method,
+    "builtin": SymbolKind.Class,
+    "builtinfunction": SymbolKind.Function,
+    "module": SymbolKind.Module,
+    "file": SymbolKind.File,
+    "xrange": SymbolKind.Array,
+    "slice": SymbolKind.Class,
+    "traceback": SymbolKind.Class,
+    "frame": SymbolKind.Class,
+    "buffer": SymbolKind.Array,
+    "dictproxy": SymbolKind.Class,
+    "funcdef": SymbolKind.Function,
+    "property": SymbolKind.Property,
+    "import": SymbolKind.Module,
+    "keyword": SymbolKind.Variable,
+    "constant": SymbolKind.Constant,
+    "variable": SymbolKind.Variable,
+    "value": SymbolKind.Variable,
+    "param": SymbolKind.Variable,
+    "statement": SymbolKind.Variable,
+    "boolean": SymbolKind.Boolean,
+    "int": SymbolKind.Number,
+    "longlean": SymbolKind.Number,
+    "float": SymbolKind.Number,
+    "complex": SymbolKind.Number,
+    "string": SymbolKind.String,
+    "unicode": SymbolKind.String,
+    "list": SymbolKind.Array,
+    "field": SymbolKind.Field,
+}
+
+def _kind(d):
+    """Return the VSCode Symbol Type"""
+    return _SYMBOL_KIND_MAP.get(d.type)
+
+@hookimpl
+def pylsp_document_symbols(config, document):
+    symbols_settings = config.plugin_settings("jedi_symbols")
+    all_scopes = symbols_settings.get("all_scopes", True)
+    add_import_symbols = symbols_settings.get("include_import_symbols", True)
+    definitions = document.jedi_names(all_scopes=all_scopes)
+    symbols = []
+    exclude = set({})
+    redefinitions = {}
+
+    while definitions != []:
+        d = definitions.pop(0)
+
+        # Skip symbols imported from other modules.
+        if not add_import_symbols:
+            # Skip if there's an import in the code the symbol is defined.
+            code = d.get_line_code()
+            if " import " in code or "import " in code:
+                continue
+
+            # Skip imported symbols comparing module names.
+            sym_full_name = d.full_name
+            if sym_full_name is not None:
+                document_dot_path = document.dot_path
+
+                # We assume a symbol is imported from another module to start
+                # with.
+                imported_symbol = True
+
+                # The last element of sym_full_name is the symbol itself, so
+                # we need to discard it to do module comparisons below.
+                if "." in sym_full_name:
+                    sym_module_name = sym_full_name.rpartition(".")[0]
+                else:
+                    sym_module_name = sym_full_name
+
+                # This is necessary to display symbols in init files (the checks
+                # below fail without it).
+                if document_dot_path.endswith("__init__"):
+                    document_dot_path = document_dot_path.rpartition(".")[0]
+
+                # document_dot_path is the module where the symbol is imported,
+                # whereas sym_module_name is the one where it was declared.
+                if document_dot_path in sym_module_name:
+                    # If document_dot_path is in sym_module_name, we can safely assume
+                    # that the symbol was declared in the document.
+                    imported_symbol = False
+                elif sym_module_name.split(".")[0] in document_dot_path.split("."):
+                    # If the first module in sym_module_name is one of the modules in
+                    # document_dot_path, we need to check if sym_module_name starts
+                    # with the modules in document_dot_path.
+                    document_mods = document_dot_path.split(".")
+                    for i in range(1, len(document_mods) + 1):
+                        submod = ".".join(document_mods[-i:])
+                        if sym_module_name.startswith(submod):
+                            imported_symbol = False
+                            break
+
+                # When there's no __init__.py next to a file or in one of its
+                # parents, the checks above fail. However, Jedi has a nice way
+                # to tell if the symbol was declared in the same file: if
+                # sym_module_name starts by __main__.
+                if imported_symbol:
+                    if not sym_module_name.startswith("__main__"):
+                        continue
+            else:
+                # We need to skip symbols if their definition doesn't have `full_name` info, they
+                # are detected as a definition, but their description (e.g. `class Foo`) doesn't
+                # match the code where they're detected by Jedi. This happens for relative imports.
+                if _include_def(d):
+                    if d.description not in d.get_line_code():
+                        continue
+                else:
+                    continue
+
+        if _include_def(d) and Path(document.path) == Path(d.module_path):
+            tuple_range = _tuple_range(d)
+            if tuple_range in exclude:
+                continue
+
+            kind = redefinitions.get(tuple_range, None)
+            if kind is not None:
+                exclude |= {tuple_range}
+
+            if d.type == "statement":
+                if d.description.startswith("self"):
+                    kind = "field"
+
+            symbol = {
+                "name": d.name,
+                "containerName": _container(d),
+                "location": {
+                    "uri": document.uri,
+                    "range": _range(d),
+                },
+                "kind": _kind(d) if kind is None else _SYMBOL_KIND_MAP[kind],
+            }
+            symbols.append(symbol)
+
+            if d.type == "class":
+                try:
+                    defined_names = list(d.defined_names())
+                    for method in defined_names:
+                        if method.type == "function":
+                            redefinitions[_tuple_range(method)] = "method"
+                        elif method.type == "statement":
+                            redefinitions[_tuple_range(method)] = "field"
+                        else:
+                            redefinitions[_tuple_range(method)] = method.type
+                    definitions = list(defined_names) + definitions
+                except Exception:
+                    pass
+    return symbols
+
+def get_style_config(document_path, options=None):
+    # Exclude file if it follows the patterns for that
+    exclude_patterns_from_ignore_file = file_resources.GetExcludePatternsForDir(
+        os.getcwd()
+    )
+    if file_resources.IsIgnored(document_path, exclude_patterns_from_ignore_file):
+        return []
+
+    # Get the default styles as a string
+    # for a preset configuration, i.e. "pep8"
+    style_config = file_resources.GetDefaultStyleForDir(os.path.dirname(document_path))
+    if options is None:
+        return style_config
+
+    # We have options passed from LSP format request
+    # let's pass them to the formatter.
+    # First we want to get a dictionary of the preset style
+    # to pass instead of a string so that we can modify it
+    style_config = style.CreateStyleFromConfig(style_config)
+
+    use_tabs = style_config["USE_TABS"]
+    indent_width = style_config["INDENT_WIDTH"]
+
+    if options.get("tabSize") is not None:
+        indent_width = max(int(options.get("tabSize")), 1)
+
+    if options.get("insertSpaces") is not None:
+        # TODO is it guaranteed to be a boolean, or can it be a string?
+        use_tabs = not options.get("insertSpaces")
+
+        if use_tabs:
+            # Indent width doesn't make sense when using tabs
+            # the specifications state: "Size of a tab in spaces"
+            indent_width = 1
+
+    style_config["USE_TABS"] = use_tabs
+    style_config["INDENT_WIDTH"] = indent_width
+    style_config["CONTINUATION_INDENT_WIDTH"] = indent_width
+
+    for style_option, value in options.items():
+        # Apply arbitrary options passed as formatter options
+        if style_option not in style_config:
+            # ignore if it's not a known yapf config
+            continue
+
+        style_config[style_option] = value
+
+    return style_config
+
+def diff_to_text_edits(diff, eol_chars):
+    # To keep things simple our text edits will be line based.
+    # We will also return the edits uncompacted, meaning a
+    # line replacement will come in as a line remove followed
+    # by a line add instead of a line replace.
+    text_edits = []
+    # keep track of line number since additions
+    # don't include the line number it's being added
+    # to in diffs. lsp is 0-indexed so we'll start with -1
+    prev_line_no = -1
+
+    for change in diff.changes:
+        if change.old and change.new:
+            # old and new are the same line, no change
+            # diffs are 1-indexed
+            prev_line_no = change.old - 1
+        elif change.new:
+            # addition
+            text_edits.append(
+                {
+                    "range": {
+                        "start": {"line": prev_line_no + 1, "character": 0},
+                        "end": {"line": prev_line_no + 1, "character": 0},
+                    },
+                    "newText": change.line + eol_chars,
+                }
+            )
+        elif change.old:
+            # remove
+            lsp_line_no = change.old - 1
+            text_edits.append(
+                {
+                    "range": {
+                        "start": {"line": lsp_line_no, "character": 0},
+                        "end": {
+                            # From LSP spec:
+                            # If you want to specify a range that contains a line
+                            # including the line ending character(s) then use an
+                            # end position denoting the start of the next line.
+                            "line": lsp_line_no + 1,
+                            "character": 0,
+                        },
+                    },
+                    "newText": "",
+                }
+            )
+            prev_line_no = lsp_line_no
+
+    return text_edits
+
+def ensure_eof_new_line(document, eol_chars, text_edits):
+    # diffs don't include EOF newline https://github.com/google/yapf/issues/1008
+    # we'll add it ourselves if our document doesn't already have it and the diff
+    # does not change the last line.
+    if document.source.endswith(eol_chars):
+        return
+
+    lines = document.lines
+    last_line_number = len(lines) - 1
+
+    if text_edits and text_edits[-1]["range"]["start"]["line"] >= last_line_number:
+        return
+
+    text_edits.append(
+        {
+            "range": {
+                "start": {"line": last_line_number, "character": 0},
+                "end": {"line": last_line_number + 1, "character": 0},
+            },
+            "newText": lines[-1] + eol_chars,
+        }
+    )
+
+def _format(document, lines=None, options=None):
+    source = document.source
+    # Yapf doesn't work with CRLF/CR line endings, so we replace them by '\n'
+    # and restore them below when adding new lines
+    eol_chars = get_eol_chars(source)
+    if eol_chars in ["\r", "\r\n"]:
+        source = source.replace(eol_chars, "\n")
+    else:
+        eol_chars = "\n"
+
+    style_config = get_style_config(document_path=document.path, options=options)
+
+    diff_txt, changed = FormatCode(
+        source,
+        lines=lines,
+        filename=document.filename,
+        print_diff=True,
+        style_config=style_config,
+    )
+
+    if not changed:
+        return []
+
+    patch_generator = whatthepatch.parse_patch(diff_txt)
+    diff = next(patch_generator)
+    patch_generator.close()
+
+    text_edits = diff_to_text_edits(diff=diff, eol_chars=eol_chars)
+
+    ensure_eof_new_line(document=document, eol_chars=eol_chars, text_edits=text_edits)
+
+    return text_edits
+
+@hookimpl
+def pylsp_format_document(workspace, document, options):
+    log.info("Formatting document %s with yapf", document)
+    with workspace.report_progress("format: yapf"):
+        return _format(document, options=options)
+
+@hookimpl
+def pylsp_format_range(document, range, options):
+    log.info("Formatting document %s in range %s with yapf", document, range)
+    # First we 'round' the range up/down to full lines only
+    range["start"]["character"] = 0
+    range["end"]["line"] += 1
+    range["end"]["character"] = 0
+
+    # From Yapf docs:
+    # lines: (list of tuples of integers) A list of tuples of lines, [start, end],
+    #   that we want to format. The lines are 1-based indexed. It can be used by
+    #   third-party code (e.g., IDEs) when reformatting a snippet of code rather
+    #   than a whole file.
+
+    # Add 1 for 1-indexing vs LSP's 0-indexing
+    lines = [(range["start"]["line"] + 1, range["end"]["line"] + 1)]
+    return _format(document, lines=lines, options=options)
\ No newline at end of file
diff --git a/pylsp/python_lsp.py b/pylsp/python_lsp.py
index ba41d6a..cad1046 100644
--- a/pylsp/python_lsp.py
+++ b/pylsp/python_lsp.py
@@ -1,6 +1,15 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import log
+from pylsp.pylsp_shared import LINT_DEBOUNCE_S
+from pylsp.pylsp_shared import PARENT_PROCESS_WATCH_INTERVAL
+from pylsp.pylsp_shared import MAX_WORKERS
+from pylsp.pylsp_shared import PYTHON_FILE_EXTENSIONS
+from pylsp.pylsp_shared import CONFIG_FILEs
+from pylsp.pylsp_shared import flatten
+from pylsp.pylsp_shared import merge
+from pylsp.pylsp_shared import PythonLSPServer
 import logging
 import os
 import socketserver
@@ -23,15 +32,6 @@ from ._version import __version__
 from .config import config
 from .workspace import Cell, Document, Notebook, Workspace
 
-log = logging.getLogger(__name__)
-
-
-LINT_DEBOUNCE_S = 0.5  # 500 ms
-PARENT_PROCESS_WATCH_INTERVAL = 10  # 10 s
-MAX_WORKERS = 64
-PYTHON_FILE_EXTENSIONS = (".py", ".pyi")
-CONFIG_FILEs = ("pycodestyle.cfg", "setup.cfg", "tox.ini", ".flake8")
-
 
 class _StreamHandlerWrapper(socketserver.StreamRequestHandler):
     """A wrapper class that is used to construct a custom handler class."""
@@ -156,742 +156,3 @@ def start_ws_lang_server(port, check_parent_process, handler_class) -> None:
                 await asyncio.Future()
 
         asyncio.run(run_server())
-
-
-class PythonLSPServer(MethodDispatcher):
-    """Implementation of the Microsoft VSCode Language Server Protocol
-    https://github.com/Microsoft/language-server-protocol/blob/master/versions/protocol-1-x.md
-    """
-
-    def __init__(
-        self, rx, tx, check_parent_process=False, consumer=None, *, endpoint_cls=None
-    ) -> None:
-        self.workspace = None
-        self.config = None
-        self.root_uri = None
-        self.watching_thread = None
-        self.workspaces = {}
-        self.uri_workspace_mapper = {}
-
-        self._check_parent_process = check_parent_process
-
-        if rx is not None:
-            self._jsonrpc_stream_reader = JsonRpcStreamReader(rx)
-        else:
-            self._jsonrpc_stream_reader = None
-
-        if tx is not None:
-            self._jsonrpc_stream_writer = JsonRpcStreamWriter(tx)
-        else:
-            self._jsonrpc_stream_writer = None
-
-        endpoint_cls = endpoint_cls or Endpoint
-
-        # if consumer is None, it is assumed that the default streams-based approach is being used
-        if consumer is None:
-            self._endpoint = endpoint_cls(
-                self, self._jsonrpc_stream_writer.write, max_workers=MAX_WORKERS
-            )
-        else:
-            self._endpoint = endpoint_cls(self, consumer, max_workers=MAX_WORKERS)
-
-        self._dispatchers = []
-        self._shutdown = False
-
-    def start(self) -> None:
-        """Entry point for the server."""
-        self._jsonrpc_stream_reader.listen(self._endpoint.consume)
-
-    def consume(self, message) -> None:
-        """Entry point for consumer based server. Alternative to stream listeners."""
-        # assuming message will be JSON
-        self._endpoint.consume(message)
-
-    def __getitem__(self, item):
-        """Override getitem to fallback through multiple dispatchers."""
-        if self._shutdown and item != "exit":
-            # exit is the only allowed method during shutdown
-            log.debug("Ignoring non-exit method during shutdown: %s", item)
-            item = "invalid_request_after_shutdown"
-
-        try:
-            return super().__getitem__(item)
-        except KeyError:
-            # Fallback through extra dispatchers
-            for dispatcher in self._dispatchers:
-                try:
-                    return dispatcher[item]
-                except KeyError:
-                    continue
-
-        raise KeyError()
-
-    def m_shutdown(self, **_kwargs) -> None:
-        for workspace in self.workspaces.values():
-            workspace.close()
-        self._shutdown = True
-
-    def m_invalid_request_after_shutdown(self, **_kwargs):
-        return {
-            "error": {
-                "code": lsp.ErrorCodes.InvalidRequest,
-                "message": "Requests after shutdown are not valid",
-            }
-        }
-
-    def m_exit(self, **_kwargs) -> None:
-        self._endpoint.shutdown()
-        if self._jsonrpc_stream_reader is not None:
-            self._jsonrpc_stream_reader.close()
-        if self._jsonrpc_stream_writer is not None:
-            self._jsonrpc_stream_writer.close()
-
-    def _match_uri_to_workspace(self, uri):
-        workspace_uri = _utils.match_uri_to_workspace(uri, self.workspaces)
-        return self.workspaces.get(workspace_uri, self.workspace)
-
-    def _hook(self, hook_name, doc_uri=None, **kwargs):
-        """Calls hook_name and returns a list of results from all registered handlers"""
-        workspace = self._match_uri_to_workspace(doc_uri)
-        doc = workspace.get_document(doc_uri) if doc_uri else None
-        hook_handlers = self.config.plugin_manager.subset_hook_caller(
-            hook_name, self.config.disabled_plugins
-        )
-        return hook_handlers(
-            config=self.config, workspace=workspace, document=doc, **kwargs
-        )
-
-    def capabilities(self):
-        server_capabilities = {
-            "codeActionProvider": True,
-            "codeLensProvider": {
-                "resolveProvider": False,  # We may need to make this configurable
-            },
-            "completionProvider": {
-                "resolveProvider": True,  # We could know everything ahead of time, but this takes time to transfer
-                "triggerCharacters": ["."],
-            },
-            "documentFormattingProvider": True,
-            "documentHighlightProvider": True,
-            "documentRangeFormattingProvider": True,
-            "documentSymbolProvider": True,
-            "definitionProvider": True,
-            "executeCommandProvider": {
-                "commands": flatten(self._hook("pylsp_commands"))
-            },
-            "hoverProvider": True,
-            "referencesProvider": True,
-            "renameProvider": True,
-            "foldingRangeProvider": True,
-            "signatureHelpProvider": {"triggerCharacters": ["(", ",", "="]},
-            "textDocumentSync": {
-                "change": lsp.TextDocumentSyncKind.INCREMENTAL,
-                "save": {
-                    "includeText": True,
-                },
-                "openClose": True,
-            },
-            "notebookDocumentSync": {
-                "notebookSelector": [{"cells": [{"language": "python"}]}]
-            },
-            "workspace": {
-                "workspaceFolders": {"supported": True, "changeNotifications": True}
-            },
-            "experimental": merge(self._hook("pylsp_experimental_capabilities")),
-        }
-        log.info("Server capabilities: %s", server_capabilities)
-        return server_capabilities
-
-    def m_initialize(
-        self,
-        processId=None,
-        rootUri=None,
-        rootPath=None,
-        initializationOptions=None,
-        workspaceFolders=None,
-        **_kwargs,
-    ):
-        log.debug(
-            "Language server initialized with %s %s %s %s",
-            processId,
-            rootUri,
-            rootPath,
-            initializationOptions,
-        )
-        if rootUri is None:
-            rootUri = uris.from_fs_path(rootPath) if rootPath is not None else ""
-
-        self.workspaces.pop(self.root_uri, None)
-        self.root_uri = rootUri
-        self.config = config.Config(
-            rootUri,
-            initializationOptions or {},
-            processId,
-            _kwargs.get("capabilities", {}),
-        )
-        self.workspace = Workspace(rootUri, self._endpoint, self.config)
-        self.workspaces[rootUri] = self.workspace
-        if workspaceFolders:
-            for folder in workspaceFolders:
-                uri = folder["uri"]
-                if uri == rootUri:
-                    # Already created
-                    continue
-                workspace_config = config.Config(
-                    uri,
-                    self.config._init_opts,
-                    self.config._process_id,
-                    self.config._capabilities,
-                )
-                workspace_config.update(self.config._settings)
-                self.workspaces[uri] = Workspace(uri, self._endpoint, workspace_config)
-
-        self._dispatchers = self._hook("pylsp_dispatchers")
-        self._hook("pylsp_initialize")
-
-        if (
-            self._check_parent_process
-            and processId is not None
-            and self.watching_thread is None
-        ):
-
-            def watch_parent_process(pid):
-                # exit when the given pid is not alive
-                if not _utils.is_process_alive(pid):
-                    log.info("parent process %s is not alive, exiting!", pid)
-                    self.m_exit()
-                else:
-                    threading.Timer(
-                        PARENT_PROCESS_WATCH_INTERVAL, watch_parent_process, args=[pid]
-                    ).start()
-
-            self.watching_thread = threading.Thread(
-                target=watch_parent_process, args=(processId,)
-            )
-            self.watching_thread.daemon = True
-            self.watching_thread.start()
-        # Get our capabilities
-        return {
-            "capabilities": self.capabilities(),
-            "serverInfo": {
-                "name": "pylsp",
-                "version": __version__,
-            },
-        }
-
-    def m_initialized(self, **_kwargs) -> None:
-        self._hook("pylsp_initialized")
-
-    def code_actions(self, doc_uri: str, range: Dict, context: Dict):
-        return flatten(
-            self._hook("pylsp_code_actions", doc_uri, range=range, context=context)
-        )
-
-    def code_lens(self, doc_uri):
-        return flatten(self._hook("pylsp_code_lens", doc_uri))
-
-    def completions(self, doc_uri, position):
-        workspace = self._match_uri_to_workspace(doc_uri)
-        document = workspace.get_document(doc_uri)
-        ignored_names = None
-        if isinstance(document, Cell):
-            # We need to get the ignored names from the whole notebook document
-            notebook_document = workspace.get_maybe_document(document.notebook_uri)
-            ignored_names = notebook_document.jedi_names(doc_uri)
-        completions = self._hook(
-            "pylsp_completions", doc_uri, position=position, ignored_names=ignored_names
-        )
-        return {"isIncomplete": False, "items": flatten(completions)}
-
-    def completion_item_resolve(self, completion_item):
-        doc_uri = completion_item.get("data", {}).get("doc_uri", None)
-        return self._hook(
-            "pylsp_completion_item_resolve", doc_uri, completion_item=completion_item
-        )
-
-    def definitions(self, doc_uri, position):
-        return flatten(self._hook("pylsp_definitions", doc_uri, position=position))
-
-    def document_symbols(self, doc_uri):
-        return flatten(self._hook("pylsp_document_symbols", doc_uri))
-
-    def document_did_save(self, doc_uri):
-        return self._hook("pylsp_document_did_save", doc_uri)
-
-    def execute_command(self, command, arguments):
-        return self._hook("pylsp_execute_command", command=command, arguments=arguments)
-
-    def format_document(self, doc_uri, options):
-        return lambda: self._hook("pylsp_format_document", doc_uri, options=options)
-
-    def format_range(self, doc_uri, range, options):
-        return self._hook("pylsp_format_range", doc_uri, range=range, options=options)
-
-    def highlight(self, doc_uri, position):
-        return (
-            flatten(self._hook("pylsp_document_highlight", doc_uri, position=position))
-            or None
-        )
-
-    def hover(self, doc_uri, position):
-        return self._hook("pylsp_hover", doc_uri, position=position) or {"contents": ""}
-
-    @_utils.debounce(LINT_DEBOUNCE_S, keyed_by="doc_uri")
-    def lint(self, doc_uri, is_saved) -> None:
-        # Since we're debounced, the document may no longer be open
-        workspace = self._match_uri_to_workspace(doc_uri)
-        document_object = workspace.documents.get(doc_uri, None)
-        if isinstance(document_object, Document):
-            self._lint_text_document(
-                doc_uri, workspace, is_saved, document_object.version
-            )
-        elif isinstance(document_object, Notebook):
-            self._lint_notebook_document(document_object, workspace)
-
-    def _lint_text_document(
-        self, doc_uri, workspace, is_saved, doc_version=None
-    ) -> None:
-        workspace.publish_diagnostics(
-            doc_uri,
-            flatten(self._hook("pylsp_lint", doc_uri, is_saved=is_saved)),
-            doc_version,
-        )
-
-    def _lint_notebook_document(self, notebook_document, workspace) -> None:
-        """
-        Lint a notebook document.
-
-        This is a bit more complicated than linting a text document, because we need to
-        send the entire notebook document to the pylsp_lint hook, but we need to send
-        the diagnostics back to the client on a per-cell basis.
-        """
-
-        # First, we create a temp TextDocument that represents the whole notebook
-        # contents. We'll use this to send to the pylsp_lint hook.
-        random_uri = str(uuid.uuid4())
-
-        # cell_list helps us map the diagnostics back to the correct cell later.
-        cell_list: List[Dict[str, Any]] = []
-
-        offset = 0
-        total_source = ""
-        for cell in notebook_document.cells:
-            cell_uri = cell["document"]
-            cell_document = workspace.get_cell_document(cell_uri)
-
-            num_lines = cell_document.line_count
-
-            data = {
-                "uri": cell_uri,
-                "line_start": offset,
-                "line_end": offset + num_lines - 1,
-                "source": cell_document.source,
-            }
-
-            cell_list.append(data)
-            if offset == 0:
-                total_source = cell_document.source
-            else:
-                total_source += "\n" + cell_document.source
-
-            offset += num_lines
-
-        workspace.put_document(random_uri, total_source)
-
-        try:
-            document_diagnostics = flatten(
-                self._hook("pylsp_lint", random_uri, is_saved=True)
-            )
-
-            # Now we need to map the diagnostics back to the correct cell and publish them.
-            # Note: this is O(n*m) in the number of cells and diagnostics, respectively.
-            for cell in cell_list:
-                cell_diagnostics = []
-                for diagnostic in document_diagnostics:
-                    start_line = diagnostic["range"]["start"]["line"]
-                    end_line = diagnostic["range"]["end"]["line"]
-
-                    if start_line > cell["line_end"] or end_line < cell["line_start"]:
-                        continue
-                    diagnostic["range"]["start"]["line"] = (
-                        start_line - cell["line_start"]
-                    )
-                    diagnostic["range"]["end"]["line"] = end_line - cell["line_start"]
-                    cell_diagnostics.append(diagnostic)
-
-                workspace.publish_diagnostics(cell["uri"], cell_diagnostics)
-        finally:
-            workspace.rm_document(random_uri)
-
-    def references(self, doc_uri, position, exclude_declaration):
-        return flatten(
-            self._hook(
-                "pylsp_references",
-                doc_uri,
-                position=position,
-                exclude_declaration=exclude_declaration,
-            )
-        )
-
-    def rename(self, doc_uri, position, new_name):
-        return self._hook("pylsp_rename", doc_uri, position=position, new_name=new_name)
-
-    def signature_help(self, doc_uri, position):
-        return self._hook("pylsp_signature_help", doc_uri, position=position)
-
-    def folding(self, doc_uri):
-        return flatten(self._hook("pylsp_folding_range", doc_uri))
-
-    def m_completion_item__resolve(self, **completionItem):
-        return self.completion_item_resolve(completionItem)
-
-    def m_notebook_document__did_open(
-        self, notebookDocument=None, cellTextDocuments=None, **_kwargs
-    ) -> None:
-        workspace = self._match_uri_to_workspace(notebookDocument["uri"])
-        workspace.put_notebook_document(
-            notebookDocument["uri"],
-            notebookDocument["notebookType"],
-            cells=notebookDocument["cells"],
-            version=notebookDocument.get("version"),
-            metadata=notebookDocument.get("metadata"),
-        )
-        for cell in cellTextDocuments or []:
-            workspace.put_cell_document(
-                cell["uri"],
-                notebookDocument["uri"],
-                cell["languageId"],
-                cell["text"],
-                version=cell.get("version"),
-            )
-        self.lint(notebookDocument["uri"], is_saved=True)
-
-    def m_notebook_document__did_close(
-        self, notebookDocument=None, cellTextDocuments=None, **_kwargs
-    ) -> None:
-        workspace = self._match_uri_to_workspace(notebookDocument["uri"])
-        for cell in cellTextDocuments or []:
-            workspace.publish_diagnostics(cell["uri"], [])
-            workspace.rm_document(cell["uri"])
-        workspace.rm_document(notebookDocument["uri"])
-
-    def m_notebook_document__did_change(
-        self, notebookDocument=None, change=None, **_kwargs
-    ) -> None:
-        """
-        Changes to the notebook document.
-
-        This could be one of the following:
-        1. Notebook metadata changed
-        2. Cell(s) added
-        3. Cell(s) deleted
-        4. Cell(s) data changed
-            4.1 Cell metadata changed
-            4.2 Cell source changed
-        """
-        workspace = self._match_uri_to_workspace(notebookDocument["uri"])
-
-        if change.get("metadata"):
-            # Case 1
-            workspace.update_notebook_metadata(
-                notebookDocument["uri"], change.get("metadata")
-            )
-
-        cells = change.get("cells")
-        if cells:
-            # Change to cells
-            structure = cells.get("structure")
-            if structure:
-                # Case 2 or 3
-                notebook_cell_array_change = structure["array"]
-                start = notebook_cell_array_change["start"]
-                cell_delete_count = notebook_cell_array_change["deleteCount"]
-                if cell_delete_count == 0:
-                    # Case 2
-                    # Cell documents
-                    for cell_document in structure["didOpen"]:
-                        workspace.put_cell_document(
-                            cell_document["uri"],
-                            notebookDocument["uri"],
-                            cell_document["languageId"],
-                            cell_document["text"],
-                            cell_document.get("version"),
-                        )
-                    # Cell metadata which is added to Notebook
-                    workspace.add_notebook_cells(
-                        notebookDocument["uri"],
-                        notebook_cell_array_change["cells"],
-                        start,
-                    )
-                else:
-                    # Case 3
-                    # Cell documents
-                    for cell_document in structure["didClose"]:
-                        workspace.rm_document(cell_document["uri"])
-                        workspace.publish_diagnostics(cell_document["uri"], [])
-                    # Cell metadata which is removed from Notebook
-                    workspace.remove_notebook_cells(
-                        notebookDocument["uri"], start, cell_delete_count
-                    )
-
-            data = cells.get("data")
-            if data:
-                # Case 4.1
-                for cell in data:
-                    # update NotebookDocument.cells properties
-                    pass
-
-            text_content = cells.get("textContent")
-            if text_content:
-                # Case 4.2
-                for cell in text_content:
-                    cell_uri = cell["document"]["uri"]
-                    # Even though the protocol says that `changes` is an array, we assume that it's always a single
-                    # element array that contains the last change to the cell source.
-                    workspace.update_document(cell_uri, cell["changes"][0])
-        self.lint(notebookDocument["uri"], is_saved=True)
-
-    def m_text_document__did_close(self, textDocument=None, **_kwargs) -> None:
-        workspace = self._match_uri_to_workspace(textDocument["uri"])
-        workspace.publish_diagnostics(textDocument["uri"], [])
-        workspace.rm_document(textDocument["uri"])
-
-    def m_text_document__did_open(self, textDocument=None, **_kwargs) -> None:
-        workspace = self._match_uri_to_workspace(textDocument["uri"])
-        workspace.put_document(
-            textDocument["uri"],
-            textDocument["text"],
-            version=textDocument.get("version"),
-        )
-        self._hook("pylsp_document_did_open", textDocument["uri"])
-        self.lint(textDocument["uri"], is_saved=True)
-
-    def m_text_document__did_change(
-        self, contentChanges=None, textDocument=None, **_kwargs
-    ) -> None:
-        workspace = self._match_uri_to_workspace(textDocument["uri"])
-        for change in contentChanges:
-            workspace.update_document(
-                textDocument["uri"], change, version=textDocument.get("version")
-            )
-        self.lint(textDocument["uri"], is_saved=False)
-
-    def m_text_document__did_save(self, textDocument=None, **_kwargs) -> None:
-        self.lint(textDocument["uri"], is_saved=True)
-        self.document_did_save(textDocument["uri"])
-
-    def m_text_document__code_action(
-        self, textDocument=None, range=None, context=None, **_kwargs
-    ):
-        return self.code_actions(textDocument["uri"], range, context)
-
-    def m_text_document__code_lens(self, textDocument=None, **_kwargs):
-        return self.code_lens(textDocument["uri"])
-
-    def _cell_document__completion(self, cellDocument, position=None, **_kwargs):
-        workspace = self._match_uri_to_workspace(cellDocument.notebook_uri)
-        notebookDocument = workspace.get_maybe_document(cellDocument.notebook_uri)
-        if notebookDocument is None:
-            raise ValueError("Invalid notebook document")
-
-        cell_data = notebookDocument.cell_data()
-
-        # Concatenate all cells to be a single temporary document
-        total_source = "\n".join(data["source"] for data in cell_data.values())
-        with workspace.temp_document(total_source) as temp_uri:
-            # update position to be the position in the temp document
-            if position is not None:
-                position["line"] += cell_data[cellDocument.uri]["line_start"]
-
-            completions = self.completions(temp_uri, position)
-
-            # Translate temp_uri locations to cell document locations
-            for item in completions.get("items", []):
-                if item.get("data", {}).get("doc_uri") == temp_uri:
-                    item["data"]["doc_uri"] = cellDocument.uri
-
-            return completions
-
-    def m_text_document__completion(self, textDocument=None, position=None, **_kwargs):
-        # textDocument here is just a dict with a uri
-        workspace = self._match_uri_to_workspace(textDocument["uri"])
-        document = workspace.get_document(textDocument["uri"])
-        if isinstance(document, Cell):
-            return self._cell_document__completion(document, position, **_kwargs)
-        return self.completions(textDocument["uri"], position)
-
-    def _cell_document__definition(self, cellDocument, position=None, **_kwargs):
-        workspace = self._match_uri_to_workspace(cellDocument.notebook_uri)
-        notebookDocument = workspace.get_maybe_document(cellDocument.notebook_uri)
-        if notebookDocument is None:
-            raise ValueError("Invalid notebook document")
-
-        cell_data = notebookDocument.cell_data()
-
-        # Concatenate all cells to be a single temporary document
-        total_source = "\n".join(data["source"] for data in cell_data.values())
-        with workspace.temp_document(total_source) as temp_uri:
-            # update position to be the position in the temp document
-            if position is not None:
-                position["line"] += cell_data[cellDocument.uri]["line_start"]
-
-            definitions = self.definitions(temp_uri, position)
-
-            # Translate temp_uri locations to cell document locations
-            for definition in definitions:
-                if definition["uri"] == temp_uri:
-                    # Find the cell the start line is in and adjust the uri and line numbers
-                    for cell_uri, data in cell_data.items():
-                        if (
-                            data["line_start"]
-                            <= definition["range"]["start"]["line"]
-                            <= data["line_end"]
-                        ):
-                            definition["uri"] = cell_uri
-                            definition["range"]["start"]["line"] -= data["line_start"]
-                            definition["range"]["end"]["line"] -= data["line_start"]
-                            break
-
-            return definitions
-
-    def m_text_document__definition(self, textDocument=None, position=None, **_kwargs):
-        # textDocument here is just a dict with a uri
-        workspace = self._match_uri_to_workspace(textDocument["uri"])
-        document = workspace.get_document(textDocument["uri"])
-        if isinstance(document, Cell):
-            return self._cell_document__definition(document, position, **_kwargs)
-        return self.definitions(textDocument["uri"], position)
-
-    def m_text_document__document_highlight(
-        self, textDocument=None, position=None, **_kwargs
-    ):
-        return self.highlight(textDocument["uri"], position)
-
-    def m_text_document__hover(self, textDocument=None, position=None, **_kwargs):
-        return self.hover(textDocument["uri"], position)
-
-    def m_text_document__document_symbol(self, textDocument=None, **_kwargs):
-        return self.document_symbols(textDocument["uri"])
-
-    def m_text_document__formatting(self, textDocument=None, options=None, **_kwargs):
-        return self.format_document(textDocument["uri"], options)
-
-    def m_text_document__rename(
-        self, textDocument=None, position=None, newName=None, **_kwargs
-    ):
-        return self.rename(textDocument["uri"], position, newName)
-
-    def m_text_document__folding_range(self, textDocument=None, **_kwargs):
-        return self.folding(textDocument["uri"])
-
-    def m_text_document__range_formatting(
-        self, textDocument=None, range=None, options=None, **_kwargs
-    ):
-        return self.format_range(textDocument["uri"], range, options)
-
-    def m_text_document__references(
-        self, textDocument=None, position=None, context=None, **_kwargs
-    ):
-        exclude_declaration = not context["includeDeclaration"]
-        return self.references(textDocument["uri"], position, exclude_declaration)
-
-    def m_text_document__signature_help(
-        self, textDocument=None, position=None, **_kwargs
-    ):
-        return self.signature_help(textDocument["uri"], position)
-
-    def m_workspace__did_change_configuration(self, settings=None) -> None:
-        if self.config is not None:
-            self.config.update((settings or {}).get("pylsp", {}))
-        for workspace in self.workspaces.values():
-            workspace.update_config(settings)
-            self._hook("pylsp_workspace_configuration_changed")
-            for doc_uri in workspace.documents:
-                self.lint(doc_uri, is_saved=False)
-
-    def m_workspace__did_change_workspace_folders(self, event=None, **_kwargs):
-        if event is None:
-            return
-        added = event.get("added", [])
-        removed = event.get("removed", [])
-
-        for removed_info in removed:
-            if "uri" in removed_info:
-                removed_uri = removed_info["uri"]
-                self.workspaces.pop(removed_uri, None)
-
-        for added_info in added:
-            if "uri" in added_info:
-                added_uri = added_info["uri"]
-                workspace_config = config.Config(
-                    added_uri,
-                    self.config._init_opts,
-                    self.config._process_id,
-                    self.config._capabilities,
-                )
-                workspace_config.update(self.config._settings)
-                self.workspaces[added_uri] = Workspace(
-                    added_uri, self._endpoint, workspace_config
-                )
-
-        root_workspace_removed = any(
-            removed_info["uri"] == self.root_uri for removed_info in removed
-        )
-        workspace_added = len(added) > 0 and "uri" in added[0]
-        if root_workspace_removed and workspace_added:
-            added_uri = added[0]["uri"]
-            self.root_uri = added_uri
-            new_root_workspace = self.workspaces[added_uri]
-            self.config = new_root_workspace._config
-            self.workspace = new_root_workspace
-        elif root_workspace_removed:
-            # NOTE: Removing the root workspace can only happen when the server
-            # is closed, thus the else condition of this if can never happen.
-            if self.workspaces:
-                log.debug("Root workspace deleted!")
-                available_workspaces = sorted(self.workspaces)
-                first_workspace = available_workspaces[0]
-                new_root_workspace = self.workspaces[first_workspace]
-                self.root_uri = first_workspace
-                self.config = new_root_workspace._config
-                self.workspace = new_root_workspace
-
-        # Migrate documents that are on the root workspace and have a better
-        # match now
-        doc_uris = list(self.workspace._docs.keys())
-        for uri in doc_uris:
-            doc = self.workspace._docs.pop(uri)
-            new_workspace = self._match_uri_to_workspace(uri)
-            new_workspace._docs[uri] = doc
-
-    def m_workspace__did_change_watched_files(self, changes=None, **_kwargs):
-        changed_py_files = set()
-        config_changed = False
-        for d in changes or []:
-            if d["uri"].endswith(PYTHON_FILE_EXTENSIONS):
-                changed_py_files.add(d["uri"])
-            elif d["uri"].endswith(CONFIG_FILEs):
-                config_changed = True
-
-        if config_changed:
-            self.config.settings.cache_clear()
-        elif not changed_py_files:
-            # Only externally changed python files and lint configs may result in changed diagnostics.
-            return
-
-        for workspace in self.workspaces.values():
-            for doc_uri in workspace.documents:
-                # Changes in doc_uri are already handled by m_text_document__did_save
-                if doc_uri not in changed_py_files:
-                    self.lint(doc_uri, is_saved=False)
-
-    def m_workspace__execute_command(self, command=None, arguments=None):
-        return self.execute_command(command, arguments)
-
-
-def flatten(list_of_lists):
-    return [item for lst in list_of_lists for item in lst]
-
-
-def merge(list_of_dicts):
-    return {k: v for dictionary in list_of_dicts for k, v in dictionary.items()}
diff --git a/test/fixtures.py b/test/fixtures.py
index dd10140..094d550 100644
--- a/test/fixtures.py
+++ b/test/fixtures.py
@@ -1,6 +1,10 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import PythonLSPServer
+from pylsp.pylsp_shared import FakeEditorMethodsMixin
+from pylsp.pylsp_shared import FakePythonLSPServer
+from pylsp.pylsp_shared import FakeEndpoint
 import os
 from io import StringIO
 from unittest.mock import MagicMock
@@ -12,7 +16,6 @@ from pylsp_jsonrpc.exceptions import JsonRpcException
 
 from pylsp import uris
 from pylsp.config.config import Config
-from pylsp.python_lsp import PythonLSPServer
 from pylsp.workspace import Document, Workspace
 from test.test_utils import CALL_TIMEOUT_IN_SECONDS, ClientServerPair
 
@@ -24,46 +27,6 @@ def main():
 """
 
 
-class FakeEditorMethodsMixin:
-    """
-    Represents the methods to be added to a dispatcher class when faking an editor.
-    """
-
-    def m_window__work_done_progress__create(self, *_args, **_kwargs):
-        """
-        Fake editor method `window/workDoneProgress/create`.
-
-        related spec:
-        https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/#window_workDoneProgress_create
-        """
-        return None
-
-
-class FakePythonLSPServer(FakeEditorMethodsMixin, PythonLSPServer):
-    pass
-
-
-class FakeEndpoint(Endpoint):
-    """
-    Fake Endpoint representing the editor / LSP client.
-
-    The `dispatcher` dict will be used to synchronously calculate the responses
-    for calls to `.request` and resolve the futures with the value or errors.
-
-    Fake methods in the `dispatcher` should raise `JsonRpcException` for any
-    error.
-    """
-
-    def request(self, method, params=None):
-        request_future = super().request(method, params)
-        try:
-            request_future.set_result(self._dispatcher[method](params))
-        except JsonRpcException as e:
-            request_future.set_exception(e)
-
-        return request_future
-
-
 @pytest.fixture
 def pylsp(tmpdir):
     """Return an initialized python LS"""
@@ -76,28 +39,6 @@ def pylsp(tmpdir):
     return ls
 
 
-@pytest.fixture
-def pylsp_w_workspace_folders(tmpdir):
-    """Return an initialized python LS"""
-    ls = FakePythonLSPServer(StringIO, StringIO, endpoint_cls=FakeEndpoint)
-
-    folder1 = tmpdir.mkdir("folder1")
-    folder2 = tmpdir.mkdir("folder2")
-
-    ls.m_initialize(
-        processId=1,
-        rootUri=uris.from_fs_path(str(folder1)),
-        initializationOptions={},
-        workspaceFolders=[
-            {"uri": uris.from_fs_path(str(folder1)), "name": "folder1"},
-            {"uri": uris.from_fs_path(str(folder2)), "name": "folder2"},
-        ],
-    )
-
-    workspace_folders = [folder1, folder2]
-    return (ls, workspace_folders)
-
-
 @pytest.fixture()
 def consumer():
     return MagicMock()
diff --git a/test/plugins/test_autoimport.py b/test/plugins/test_autoimport.py
index dbad8d0..b27a0d5 100644
--- a/test/plugins/test_autoimport.py
+++ b/test/plugins/test_autoimport.py
@@ -1,5 +1,11 @@
 # Copyright 2022- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import _should_insert
+from pylsp.pylsp_shared import _get_score
+from pylsp.pylsp_shared import get_names
+from pylsp.pylsp_shared import cache
+from pylsp.pylsp_shared import pylsp_completions
+from pylsp.pylsp_shared import get_name_or_module
 from typing import Any, Dict, List
 from unittest.mock import Mock, patch
 
@@ -9,16 +15,6 @@ import pytest
 
 from pylsp import IS_WIN, lsp, uris
 from pylsp.config.config import Config
-from pylsp.plugins.rope_autoimport import (
-    _get_score,
-    _should_insert,
-    cache,
-    get_name_or_module,
-    get_names,
-)
-from pylsp.plugins.rope_autoimport import (
-    pylsp_completions as pylsp_autoimport_completions,
-)
 from pylsp.workspace import Workspace
 from test.test_notebook_document import wait_for_condition
 from test.test_utils import send_initialize_request, send_notebook_did_open
diff --git a/test/plugins/test_autopep8_format.py b/test/plugins/test_autopep8_format.py
index 4966b89..e689bbc 100644
--- a/test/plugins/test_autopep8_format.py
+++ b/test/plugins/test_autopep8_format.py
@@ -1,10 +1,11 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import pylsp_format_document
+from pylsp.pylsp_shared import pylsp_format_range
 import pytest
 
 from pylsp import uris
-from pylsp.plugins.autopep8_format import pylsp_format_document, pylsp_format_range
 from pylsp.workspace import Document
 
 DOC_URI = uris.from_fs_path(__file__)
diff --git a/test/plugins/test_completion.py b/test/plugins/test_completion.py
index d1ca5ef..48c6af7 100644
--- a/test/plugins/test_completion.py
+++ b/test/plugins/test_completion.py
@@ -1,6 +1,8 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import pylsp_completions
+from pylsp.pylsp_shared import pylsp_completion_item_resolve
 import math
 import os
 import sys
@@ -11,11 +13,6 @@ import pytest
 
 from pylsp import lsp, uris
 from pylsp._utils import JEDI_VERSION
-from pylsp.plugins.jedi_completion import (
-    pylsp_completion_item_resolve as pylsp_jedi_completion_item_resolve,
-)
-from pylsp.plugins.jedi_completion import pylsp_completions as pylsp_jedi_completions
-from pylsp.plugins.rope_completion import pylsp_completions as pylsp_rope_completions
 from pylsp.workspace import Document
 
 PY2 = sys.version[0] == "2"
diff --git a/test/plugins/test_definitions.py b/test/plugins/test_definitions.py
index 7923524..efdf5e1 100644
--- a/test/plugins/test_definitions.py
+++ b/test/plugins/test_definitions.py
@@ -1,10 +1,10 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import pylsp_definitions
 import os
 
 from pylsp import uris
-from pylsp.plugins.definition import pylsp_definitions
 from pylsp.workspace import Document
 
 DOC_URI = uris.from_fs_path(__file__)
diff --git a/test/plugins/test_flake8_lint.py b/test/plugins/test_flake8_lint.py
index e7b6b00..046a7b5 100644
--- a/test/plugins/test_flake8_lint.py
+++ b/test/plugins/test_flake8_lint.py
@@ -1,6 +1,7 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import pylsp_lint
 import os
 import tempfile
 from textwrap import dedent
@@ -32,7 +33,7 @@ def temp_document(doc_text, workspace):
 
 def test_flake8_unsaved(workspace) -> None:
     doc = Document("", workspace, DOC)
-    diags = flake8_lint.pylsp_lint(workspace, doc)
+    diags = pylsp_lint(workspace, doc)
     msg = "F841 local variable 'a' is assigned to but never used"
     unused_var = [d for d in diags if d["message"] == msg][0]
 
@@ -47,7 +48,7 @@ def test_flake8_unsaved(workspace) -> None:
 def test_flake8_lint(workspace) -> None:
     name, doc = temp_document(DOC, workspace)
     try:
-        diags = flake8_lint.pylsp_lint(workspace, doc)
+        diags = pylsp_lint(workspace, doc)
         msg = "F841 local variable 'a' is assigned to but never used"
         unused_var = [d for d in diags if d["message"] == msg][0]
 
@@ -91,7 +92,7 @@ def test_flake8_respecting_configuration(workspace) -> None:
         workspace.put_document(made[rel]["uri"], contents)
         made[rel]["document"] = workspace._docs[made[rel]["uri"]]
 
-    diags = flake8_lint.pylsp_lint(workspace, made["src/a.py"]["document"])
+    diags = pylsp_lint(workspace, made["src/a.py"]["document"])
     assert diags == [
         {
             "source": "flake8",
@@ -106,7 +107,7 @@ def test_flake8_respecting_configuration(workspace) -> None:
         },
     ]
 
-    diags = flake8_lint.pylsp_lint(workspace, made["src/b.py"]["document"])
+    diags = pylsp_lint(workspace, made["src/b.py"]["document"])
     assert diags == [
         {
             "source": "flake8",
@@ -129,7 +130,7 @@ def test_flake8_config_param(workspace) -> None:
         flake8_conf = "/tmp/some.cfg"
         workspace._config.update({"plugins": {"flake8": {"config": flake8_conf}}})
         _name, doc = temp_document(DOC, workspace)
-        flake8_lint.pylsp_lint(workspace, doc)
+        pylsp_lint(workspace, doc)
         (call_args,) = popen_mock.call_args[0]
         assert "flake8" in call_args
         assert "--config={}".format(flake8_conf) in call_args
@@ -146,7 +147,7 @@ def test_flake8_executable_param(workspace) -> None:
         )
 
         _name, doc = temp_document(DOC, workspace)
-        flake8_lint.pylsp_lint(workspace, doc)
+        pylsp_lint(workspace, doc)
 
         (call_args,) = popen_mock.call_args[0]
         assert flake8_executable in call_args
@@ -190,7 +191,7 @@ exclude =
         mock_instance.communicate.return_value = [bytes(), bytes()]
 
         doc = workspace.get_document(doc_uri)
-        flake8_lint.pylsp_lint(workspace, doc)
+        pylsp_lint(workspace, doc)
 
     call_args = popen_mock.call_args[0][0]
 
@@ -230,7 +231,7 @@ exclude =
     assert len(flake8_settings["exclude"]) == 2
 
     doc = workspace.get_document(doc_uri)
-    res = flake8_lint.pylsp_lint(workspace, doc)
+    res = pylsp_lint(workspace, doc)
     assert not res
 
     os.unlink(os.path.join(workspace.root_path, "setup.cfg"))
@@ -252,7 +253,7 @@ per-file-ignores = **/__init__.py:F401,E402
     assert len(flake8_settings["perFileIgnores"]) == 2
 
     doc = workspace.get_document(doc_uri)
-    res = flake8_lint.pylsp_lint(workspace, doc)
+    res = pylsp_lint(workspace, doc)
     assert not res
 
     os.unlink(os.path.join(workspace.root_path, "setup.cfg"))
diff --git a/test/plugins/test_folding.py b/test/plugins/test_folding.py
index 1f0d34c..8adcd12 100644
--- a/test/plugins/test_folding.py
+++ b/test/plugins/test_folding.py
@@ -1,11 +1,11 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import pylsp_folding_range
 import sys
 from textwrap import dedent
 
 from pylsp import uris
-from pylsp.plugins.folding import pylsp_folding_range
 from pylsp.workspace import Document
 
 DOC_URI = uris.from_fs_path(__file__)
diff --git a/test/plugins/test_highlight.py b/test/plugins/test_highlight.py
index eb5485b..0846a55 100644
--- a/test/plugins/test_highlight.py
+++ b/test/plugins/test_highlight.py
@@ -1,8 +1,8 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import pylsp_document_highlight
 from pylsp import lsp, uris
-from pylsp.plugins.highlight import pylsp_document_highlight
 from pylsp.workspace import Document
 
 DOC_URI = uris.from_fs_path(__file__)
diff --git a/test/plugins/test_hover.py b/test/plugins/test_hover.py
index 9674b87..c3dc92b 100644
--- a/test/plugins/test_hover.py
+++ b/test/plugins/test_hover.py
@@ -1,10 +1,10 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import pylsp_hover
 import os
 
 from pylsp import uris
-from pylsp.plugins.hover import pylsp_hover
 from pylsp.workspace import Document
 
 DOC_URI = uris.from_fs_path(__file__)
diff --git a/test/plugins/test_jedi_rename.py b/test/plugins/test_jedi_rename.py
index 349274b..1ef8969 100644
--- a/test/plugins/test_jedi_rename.py
+++ b/test/plugins/test_jedi_rename.py
@@ -1,12 +1,12 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import pylsp_rename
 import os
 
 import pytest
 
 from pylsp import uris
-from pylsp.plugins.jedi_rename import pylsp_rename
 from pylsp.workspace import Document
 
 DOC_NAME = "test1.py"
diff --git a/test/plugins/test_mccabe_lint.py b/test/plugins/test_mccabe_lint.py
index f4df0c2..64df075 100644
--- a/test/plugins/test_mccabe_lint.py
+++ b/test/plugins/test_mccabe_lint.py
@@ -1,6 +1,7 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import pylsp_lint
 from pylsp import lsp, uris
 from pylsp.plugins import mccabe_lint
 from pylsp.workspace import Document
@@ -19,7 +20,7 @@ def test_mccabe(config, workspace) -> None:
     try:
         config.update({"plugins": {"mccabe": {"threshold": 1}}})
         doc = Document(DOC_URI, workspace, DOC)
-        diags = mccabe_lint.pylsp_lint(config, workspace, doc)
+        diags = pylsp_lint(config, workspace, doc)
 
         assert all(d["source"] == "mccabe" for d in diags)
 
@@ -36,4 +37,4 @@ def test_mccabe(config, workspace) -> None:
 
 def test_mccabe_syntax_error(config, workspace) -> None:
     doc = Document(DOC_URI, workspace, DOC_SYNTAX_ERR)
-    assert mccabe_lint.pylsp_lint(config, workspace, doc) is None
+    assert pylsp_lint(config, workspace, doc) is None
diff --git a/test/plugins/test_pycodestyle_lint.py b/test/plugins/test_pycodestyle_lint.py
index eea0b7d..9e32e7f 100644
--- a/test/plugins/test_pycodestyle_lint.py
+++ b/test/plugins/test_pycodestyle_lint.py
@@ -1,6 +1,7 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import pylsp_lint
 import os
 
 import pytest
@@ -26,7 +27,7 @@ import json
 
 def test_pycodestyle(workspace) -> None:
     doc = Document(DOC_URI, workspace, DOC)
-    diags = pycodestyle_lint.pylsp_lint(workspace, doc)
+    diags = pylsp_lint(workspace, doc)
 
     assert all(d["source"] == "pycodestyle" for d in diags)
 
@@ -84,7 +85,7 @@ def test_pycodestyle_config(workspace) -> None:
     doc = workspace.get_document(doc_uri)
 
     # Make sure we get a warning for 'indentation contains tabs'
-    diags = pycodestyle_lint.pylsp_lint(workspace, doc)
+    diags = pylsp_lint(workspace, doc)
     assert [d for d in diags if d["code"] == "W191"]
 
     content = {
@@ -101,7 +102,7 @@ def test_pycodestyle_config(workspace) -> None:
         workspace._config.settings.cache_clear()
 
         # And make sure we don't get any warnings
-        diags = pycodestyle_lint.pylsp_lint(workspace, doc)
+        diags = pylsp_lint(workspace, doc)
         assert len([d for d in diags if d["code"] == "W191"]) == (0 if working else 1)
         assert len([d for d in diags if d["code"] == "E201"]) == (0 if working else 1)
         assert [d for d in diags if d["code"] == "W391"]
@@ -111,7 +112,7 @@ def test_pycodestyle_config(workspace) -> None:
     # Make sure we can ignore via the PYLS config as well
     workspace._config.update({"plugins": {"pycodestyle": {"ignore": ["W191", "E201"]}}})
     # And make sure we only get one warning
-    diags = pycodestyle_lint.pylsp_lint(workspace, doc)
+    diags = pylsp_lint(workspace, doc)
     assert not [d for d in diags if d["code"] == "W191"]
     assert not [d for d in diags if d["code"] == "E201"]
     assert [d for d in diags if d["code"] == "W391"]
@@ -130,7 +131,7 @@ def test_line_endings(workspace, newline) -> None:
     doc = Document(DOC_URI, workspace, source)
 
     # Get diagnostics
-    diags = pycodestyle_lint.pylsp_lint(workspace, doc)
+    diags = pylsp_lint(workspace, doc)
 
     # Assert no diagnostics were given
     assert len(diags) == 0
diff --git a/test/plugins/test_pydocstyle_lint.py b/test/plugins/test_pydocstyle_lint.py
index 383aaf1..3efcf68 100644
--- a/test/plugins/test_pydocstyle_lint.py
+++ b/test/plugins/test_pydocstyle_lint.py
@@ -1,6 +1,7 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import pylsp_lint
 import os
 
 from pylsp import lsp, uris
@@ -21,7 +22,7 @@ import json
 
 def test_pydocstyle(config, workspace) -> None:
     doc = Document(DOC_URI, workspace, DOC)
-    diags = pydocstyle_lint.pylsp_lint(config, workspace, doc)
+    diags = pylsp_lint(config, workspace, doc)
 
     assert all(d["source"] == "pydocstyle" for d in diags)
 
@@ -41,19 +42,19 @@ def test_pydocstyle(config, workspace) -> None:
 def test_pydocstyle_test_document(config, workspace) -> None:
     # The default --match argument excludes test_* documents.
     doc = Document(TEST_DOC_URI, workspace, "")
-    diags = pydocstyle_lint.pylsp_lint(config, workspace, doc)
+    diags = pylsp_lint(config, workspace, doc)
     assert not diags
 
 
 def test_pydocstyle_empty_source(config, workspace) -> None:
     doc = Document(DOC_URI, workspace, "")
-    diags = pydocstyle_lint.pylsp_lint(config, workspace, doc)
+    diags = pylsp_lint(config, workspace, doc)
     assert diags[0]["message"] == "D100: Missing docstring in public module"
     assert len(diags) == 1
 
 
 def test_pydocstyle_invalid_source(config, workspace) -> None:
     doc = Document(DOC_URI, workspace, "bad syntax")
-    diags = pydocstyle_lint.pylsp_lint(config, workspace, doc)
+    diags = pylsp_lint(config, workspace, doc)
     # We're unable to parse the file, so can't get any pydocstyle diagnostics
     assert not diags
diff --git a/test/plugins/test_pyflakes_lint.py b/test/plugins/test_pyflakes_lint.py
index 8ab3632..4034a74 100644
--- a/test/plugins/test_pyflakes_lint.py
+++ b/test/plugins/test_pyflakes_lint.py
@@ -1,6 +1,7 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import pylsp_lint
 import sys
 
 from pylsp import lsp, uris
@@ -30,7 +31,7 @@ import sys
 
 def test_pyflakes(workspace) -> None:
     doc = Document(DOC_URI, workspace, DOC)
-    diags = pyflakes_lint.pylsp_lint(workspace, doc)
+    diags = pylsp_lint(workspace, doc)
 
     # One we're expecting is:
     msg = "'sys' imported but unused"
@@ -42,7 +43,7 @@ def test_pyflakes(workspace) -> None:
 
 def test_syntax_error_pyflakes(workspace) -> None:
     doc = Document(DOC_URI, workspace, DOC_SYNTAX_ERR)
-    diag = pyflakes_lint.pylsp_lint(workspace, doc)[0]
+    diag = pylsp_lint(workspace, doc)[0]
 
     if sys.version_info[:2] >= (3, 10):
         assert diag["message"] == "expected ':'"
@@ -54,7 +55,7 @@ def test_syntax_error_pyflakes(workspace) -> None:
 
 def test_undefined_name_pyflakes(workspace) -> None:
     doc = Document(DOC_URI, workspace, DOC_UNDEFINED_NAME_ERR)
-    diag = pyflakes_lint.pylsp_lint(workspace, doc)[0]
+    diag = pylsp_lint(workspace, doc)[0]
 
     assert diag["message"] == "undefined name 'b'"
     assert diag["range"]["start"] == {"line": 0, "character": 4}
@@ -63,7 +64,7 @@ def test_undefined_name_pyflakes(workspace) -> None:
 
 def test_unicode_encoding(workspace) -> None:
     doc = Document(DOC_URI, workspace, DOC_ENCODING)
-    diags = pyflakes_lint.pylsp_lint(workspace, doc)
+    diags = pylsp_lint(workspace, doc)
 
     assert len(diags) == 1
     assert diags[0]["message"] == "'sys' imported but unused"
diff --git a/test/plugins/test_pylint_lint.py b/test/plugins/test_pylint_lint.py
index b4d511d..c38794c 100644
--- a/test/plugins/test_pylint_lint.py
+++ b/test/plugins/test_pylint_lint.py
@@ -2,6 +2,8 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import PylintLinter
+from pylsp.pylsp_shared import pylsp_lint
 import contextlib
 import os
 import tempfile
@@ -43,7 +45,7 @@ def write_temp_doc(document, contents) -> None:
 
 def test_pylint(config, workspace) -> None:
     with temp_document(DOC, workspace) as doc:
-        diags = pylint_lint.pylsp_lint(config, workspace, doc, True)
+        diags = pylsp_lint(config, workspace, doc, True)
 
         msg = "[unused-import] Unused import sys"
         unused_import = [d for d in diags if d["message"] == msg][0]
@@ -54,7 +56,7 @@ def test_pylint(config, workspace) -> None:
 
         # test running pylint in stdin
         config.plugin_settings("pylint")["executable"] = "pylint"
-        diags = pylint_lint.pylsp_lint(config, workspace, doc, True)
+        diags = pylsp_lint(config, workspace, doc, True)
 
         msg = "Unused import sys (unused-import)"
         unused_import = [d for d in diags if d["message"] == msg][0]
@@ -68,7 +70,7 @@ def test_pylint(config, workspace) -> None:
 
 def test_syntax_error_pylint(config, workspace) -> None:
     with temp_document(DOC_SYNTAX_ERR, workspace) as doc:
-        diag = pylint_lint.pylsp_lint(config, workspace, doc, True)[0]
+        diag = pylsp_lint(config, workspace, doc, True)[0]
 
         assert diag["message"].startswith("[syntax-error]")
         assert diag["message"].count("expected ':'") or diag["message"].count(
@@ -81,7 +83,7 @@ def test_syntax_error_pylint(config, workspace) -> None:
 
         # test running pylint in stdin
         config.plugin_settings("pylint")["executable"] = "pylint"
-        diag = pylint_lint.pylsp_lint(config, workspace, doc, True)[0]
+        diag = pylsp_lint(config, workspace, doc, True)[0]
 
         assert diag["message"].count("expected ':'") or diag["message"].count(
             "invalid syntax"
@@ -96,7 +98,7 @@ def test_lint_free_pylint(config, workspace) -> None:
     # match pylint's naming requirements. We should be keeping this file clean
     # though, so it works for a test of an empty lint.
     ws = Workspace(str(Path(__file__).absolute().parents[2]), workspace._endpoint)
-    assert not pylint_lint.pylsp_lint(
+    assert not pylsp_lint(
         config, ws, Document(uris.from_fs_path(__file__), ws), True
     )
 
@@ -114,26 +116,26 @@ def test_lint_caching(workspace) -> None:
     flags = "--disable=invalid-name"
     with temp_document(DOC, workspace) as doc:
         # Start with a file with errors.
-        diags = pylint_lint.PylintLinter.lint(doc, True, flags)
+        diags = PylintLinter.lint(doc, True, flags)
         assert diags
 
         # Fix lint errors and write the changes to disk. Run the linter in the
         # in-memory mode to check the cached diagnostic behavior.
         write_temp_doc(doc, "")
-        assert pylint_lint.PylintLinter.lint(doc, False, flags) == diags
+        assert PylintLinter.lint(doc, False, flags) == diags
 
         # Now check the on-disk behavior.
-        assert not pylint_lint.PylintLinter.lint(doc, True, flags)
+        assert not PylintLinter.lint(doc, True, flags)
 
         # Make sure the cache was properly cleared.
-        assert not pylint_lint.PylintLinter.lint(doc, False, flags)
+        assert not PylintLinter.lint(doc, False, flags)
 
 
 def test_per_file_caching(config, workspace) -> None:
     # Ensure that diagnostics are cached per-file.
     with temp_document(DOC, workspace) as doc:
-        assert pylint_lint.pylsp_lint(config, workspace, doc, True)
+        assert pylsp_lint(config, workspace, doc, True)
 
-    assert not pylint_lint.pylsp_lint(
+    assert not pylsp_lint(
         config, workspace, Document(uris.from_fs_path(__file__), workspace), False
     )
diff --git a/test/plugins/test_references.py b/test/plugins/test_references.py
index f512169..3283612 100644
--- a/test/plugins/test_references.py
+++ b/test/plugins/test_references.py
@@ -1,12 +1,12 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import pylsp_references
 import os
 
 import pytest
 
 from pylsp import uris
-from pylsp.plugins.references import pylsp_references
 from pylsp.workspace import Document
 
 DOC1_NAME = "test1.py"
diff --git a/test/plugins/test_signature.py b/test/plugins/test_signature.py
index 4a0a84e..74ead16 100644
--- a/test/plugins/test_signature.py
+++ b/test/plugins/test_signature.py
@@ -1,6 +1,10 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import SPHINX
+from pylsp.pylsp_shared import EPYDOC
+from pylsp.pylsp_shared import GOOGLE
+from pylsp.pylsp_shared import pylsp_signature_help
 import pytest
 
 from pylsp import uris
@@ -47,7 +51,7 @@ def test_no_signature(workspace) -> None:
     sig_position = {"line": 9, "character": 0}
     doc = Document(DOC_URI, workspace, DOC)
 
-    sigs = signature.pylsp_signature_help(doc._config, doc, sig_position)["signatures"]
+    sigs = pylsp_signature_help(doc._config, doc, sig_position)["signatures"]
     assert not sigs
 
 
@@ -56,7 +60,7 @@ def test_signature(workspace) -> None:
     sig_position = {"line": 10, "character": 5}
     doc = Document(DOC_URI, workspace, DOC)
 
-    sig_info = signature.pylsp_signature_help(doc._config, doc, sig_position)
+    sig_info = pylsp_signature_help(doc._config, doc, sig_position)
 
     sigs = sig_info["signatures"]
     assert len(sigs) == 1
@@ -75,7 +79,7 @@ def test_multi_line_signature(workspace) -> None:
     sig_position = {"line": 17, "character": 5}
     doc = Document(DOC_URI, workspace, MULTI_LINE_DOC)
 
-    sig_info = signature.pylsp_signature_help(doc._config, doc, sig_position)
+    sig_info = pylsp_signature_help(doc._config, doc, sig_position)
 
     sigs = sig_info["signatures"]
     assert len(sigs) == 1
@@ -95,9 +99,9 @@ def test_multi_line_signature(workspace) -> None:
 @pytest.mark.parametrize(
     "regex,doc",
     [
-        (signature.SPHINX, "    :param test: parameter docstring"),
-        (signature.EPYDOC, "    @param test: parameter docstring"),
-        (signature.GOOGLE, "    test (str): parameter docstring"),
+        (SPHINX, "    :param test: parameter docstring"),
+        (EPYDOC, "    @param test: parameter docstring"),
+        (GOOGLE, "    test (str): parameter docstring"),
     ],
 )
 def test_docstring_params(regex, doc) -> None:
diff --git a/test/plugins/test_symbols.py b/test/plugins/test_symbols.py
index c00ab93..0ab1cc0 100644
--- a/test/plugins/test_symbols.py
+++ b/test/plugins/test_symbols.py
@@ -1,6 +1,7 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import pylsp_document_symbols
 import os
 import sys
 
@@ -8,7 +9,6 @@ import pytest
 
 from pylsp import uris
 from pylsp.lsp import SymbolKind
-from pylsp.plugins.symbols import pylsp_document_symbols
 from pylsp.workspace import Document
 
 PY2 = sys.version[0] == "2"
diff --git a/test/plugins/test_yapf_format.py b/test/plugins/test_yapf_format.py
index f69541a..1886d18 100644
--- a/test/plugins/test_yapf_format.py
+++ b/test/plugins/test_yapf_format.py
@@ -1,10 +1,11 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import pylsp_format_document
+from pylsp.pylsp_shared import pylsp_format_range
 import pytest
 
 from pylsp import uris
-from pylsp.plugins.yapf_format import pylsp_format_document, pylsp_format_range
 from pylsp.text_edit import apply_text_edits
 from pylsp.workspace import Document
 
diff --git a/test/test_utils.py b/test/test_utils.py
index 07d04e3..f4f1f64 100644
--- a/test/test_utils.py
+++ b/test/test_utils.py
@@ -1,6 +1,7 @@
 # Copyright 2017-2020 Palantir Technologies, Inc.
 # Copyright 2021- Python Language Server Contributors.
 
+from pylsp.pylsp_shared import PythonLSPServer
 import multiprocessing
 import os
 import sys
@@ -14,7 +15,7 @@ from flaky import flaky
 
 from pylsp import _utils
 from pylsp.lsp import NotebookCellKind
-from pylsp.python_lsp import PythonLSPServer, start_io_lang_server
+from pylsp.python_lsp import start_io_lang_server
 
 CALL_TIMEOUT_IN_SECONDS = 30
 
